{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.seq_coref import data_util\n",
    "from mica_text_coref.coref.seq_coref import data\n",
    "\n",
    "import numpy as np\n",
    "from scorch import scores\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import LongformerTokenizer\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data_util.load_tensors(\n",
    "    (\"/home/sbaruah_usc_edu/mica_text_coref/data/tensors/\"\n",
    "    \"longformer_seq_tensors_512/train\"), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "\n",
    "    def __init__(self, recall, precision) -> None:\n",
    "        self.recall = recall\n",
    "        self.precision = precision\n",
    "        self.f1 = 0 if (self.precision == 0 and self.recall == 0) else (\n",
    "            2 * self.precision * self.recall / (self.precision + self.recall))\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return (f\"P = {100*self.precision:.1f}, R = {100*self.recall:.1f}, \"\n",
    "                f\"F1 = {100*self.f1:.1f}\")\n",
    "\n",
    "class CoreferenceMetric:\n",
    "\n",
    "    def __init__(self, muc: Metric, b3: Metric, ceafe: Metric,\n",
    "     ceafm: Metric, mention: Metric) -> None:\n",
    "        self.muc = muc\n",
    "        self.b3 = b3\n",
    "        self.ceafe = ceafe\n",
    "        self.ceafm = ceafm\n",
    "        self.mention = mention\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        average_f1 = (self.muc.f1 + self.b3.f1 + self.ceafe.f1)/3\n",
    "        desc = (f\"MUC: {self.muc}\\nB3: {self.b3}\\nCEAFe: {self.ceafe}\\n\"\n",
    "                f\"Average F1: {100*average_f1:.1f}\\nMention: {self.mention}\")\n",
    "        return desc\n",
    "\n",
    "def evaluate_clusters_scorch(\n",
    "    groundtruth: dict[int, list[set[data.Mention]]],\n",
    "    predictions: dict[int, list[set[data.Mention]]]) -> CoreferenceMetric:\n",
    "    \"\"\"Evaluates the predictions against the groundtruth annotations using the\n",
    "    unofficial python scorch package.\n",
    "\n",
    "    Args:\n",
    "        groundtruth: A dictionary of list of groundtruth coreference clusters\n",
    "            (set of data.Mention objects) keyed by the doc id.\n",
    "        predictions: A dictionary of list of predicted coreference clusters\n",
    "            (set of data.Mention objects) keyed by the doc id.\n",
    "    \n",
    "    Return:\n",
    "        CoreferenceMetric. This contains scores for MUC, B3, CEAFe, CEAFm, and\n",
    "        mention.\n",
    "    \"\"\"\n",
    "    gold_clusters: list[set[tuple[int, data.Mention]]] = []\n",
    "    pred_clusters: list[set[tuple[int, data.Mention]]] = []\n",
    "    gold_mentions: set[tuple[int, data.Mention]] = set()\n",
    "    pred_mentions: set[tuple[int, data.Mention]] = set()\n",
    "    doc_keys: set[int] = set()\n",
    "\n",
    "    for doc_key, clusters in tqdm.tqdm(groundtruth.items(),\n",
    "        total=len(groundtruth),\n",
    "        desc=\"Collecting groundtruth clusters and mentions\"):\n",
    "        doc_keys.add(doc_key)\n",
    "        for cluster in clusters:\n",
    "            gold_cluster: set[tuple[int, data.Mention]] = set()\n",
    "            for mention in cluster:\n",
    "                gold_cluster.add((doc_key, mention))\n",
    "                gold_mentions.add((doc_key, mention))\n",
    "            gold_clusters.append(gold_cluster)\n",
    "    \n",
    "    for doc_key, clusters in tqdm.tqdm(predictions.items(),\n",
    "        total=len(predictions),\n",
    "        desc=\"Collecting predictions clusters and mentions\"):\n",
    "        if doc_key in doc_keys:\n",
    "            for cluster in clusters:\n",
    "                pred_cluster: set[tuple[int, data.Mention]] = set()\n",
    "                for mention in cluster:\n",
    "                    pred_cluster.add((doc_key, mention))\n",
    "                    pred_mentions.add((doc_key, mention))\n",
    "                pred_clusters.append(pred_cluster)\n",
    "    \n",
    "    print(\"Calculating MUC\")\n",
    "    muc_recall, muc_precision, _ = scores.muc(gold_clusters, pred_clusters)\n",
    "    print(\"Calculating B3\")\n",
    "    b3_recall, b3_precision, _ = scores.b_cubed(gold_clusters, pred_clusters)\n",
    "    print(\"Calculating CEAF-e\")\n",
    "    ceafe_recall, ceafe_precision, _ = scores.ceaf_e(gold_clusters,\n",
    "                                                    pred_clusters)\n",
    "    print(\"Calculating CEAF-m\")\n",
    "    ceafm_recall, ceafm_precision, _ = scores.ceaf_m(gold_clusters,\n",
    "                                                    pred_clusters)\n",
    "    n_common_mentions = len(gold_mentions.intersection(pred_mentions))\n",
    "    mention_recall = 0 if len(gold_mentions) == 0 else n_common_mentions/(\n",
    "        len(gold_mentions))\n",
    "    mention_precision = 0 if len(pred_mentions) == 0 else n_common_mentions/(\n",
    "        len(pred_mentions))\n",
    "\n",
    "    muc = Metric(muc_recall, muc_precision)\n",
    "    b3 = Metric(b3_recall, b3_precision)\n",
    "    ceafe = Metric(ceafe_recall, ceafe_precision)\n",
    "    ceafm = Metric(ceafm_recall, ceafm_precision)\n",
    "    mention_metric = Metric(mention_recall, mention_precision)\n",
    "    scorch_metric = CoreferenceMetric(muc, b3, ceafe, ceafm, mention_metric)\n",
    "    return scorch_metric\n",
    "\n",
    "def convert_tensor_to_cluster(tensor: torch.LongTensor) -> set[data.Mention]:\n",
    "    \"\"\"Find the set of mentions from the annotated tensor\"\"\"\n",
    "    cluster: set[data.Mention] = set()\n",
    "    i = 0\n",
    "    while i < len(tensor):\n",
    "        if tensor[i] == 1:\n",
    "            j = i + 1\n",
    "            while j < len(tensor) and tensor[j] == 2:\n",
    "                j += 1\n",
    "            mention = data.Mention(i, j - 1)\n",
    "            cluster.add(mention)\n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    return cluster\n",
    "\n",
    "def evaluate_tensors_scorch(groundtruth: torch.LongTensor, \n",
    "    predictions: torch.LongTensor, doc_ids: torch.IntTensor,\n",
    "    corpus: data.CorefCorpus | None = None\n",
    "    ) -> CoreferenceMetric | tuple[CoreferenceMetric, CoreferenceMetric]:\n",
    "    \"\"\"Evaluate the predictions against the groundtruth annotations using the\n",
    "    unofficial python scorch library. The groundtruth and predictions are\n",
    "    represented by tensors. If corpus is not None, also predict against the\n",
    "    clusters of the corpus.\n",
    "\n",
    "    Args:\n",
    "        groundtruth: Integer tensor annotated with groundtruth cluster\n",
    "            mentions.\n",
    "        predictions: Integer tensor annotated with predicted cluster mentions.\n",
    "        doc_ids: Integer tensor containing doc ids of the corresponding\n",
    "            groundtruth and predictions tensors.\n",
    "        corpus: Original coreference corpus from which the groundtruth tensors\n",
    "            was created.\n",
    "    \n",
    "    Return:\n",
    "        CoreferenceMetric or tuple of two CoreferenceMetric.\n",
    "    \"\"\"\n",
    "    groundtruth_doc_id_to_clusters: dict[int, list[set[data.Mention]]] = {}\n",
    "    predictions_doc_id_to_clusters: dict[int, list[set[data.Mention]]] = {}\n",
    "\n",
    "    for doc_id, gt_tensor, pred_tensor in tqdm.tqdm(zip(\n",
    "        doc_ids, groundtruth, predictions), total=len(doc_ids),\n",
    "            desc=\"Convert Tensor to Cluster\"):\n",
    "        gt_cluster = convert_tensor_to_cluster(gt_tensor)\n",
    "        pred_cluster = convert_tensor_to_cluster(pred_tensor)\n",
    "        if len(gt_cluster):\n",
    "            if doc_id not in groundtruth_doc_id_to_clusters:\n",
    "                groundtruth_doc_id_to_clusters[doc_id] = []\n",
    "            groundtruth_doc_id_to_clusters[doc_id].append(gt_cluster)\n",
    "        if len(pred_cluster):\n",
    "            if doc_id not in predictions_doc_id_to_clusters:\n",
    "                predictions_doc_id_to_clusters[doc_id] = []\n",
    "            predictions_doc_id_to_clusters[doc_id].append(gt_cluster)\n",
    "    \n",
    "    coref_metric1 = evaluate_clusters_scorch(\n",
    "        groundtruth_doc_id_to_clusters, predictions_doc_id_to_clusters)\n",
    "    \n",
    "    if corpus is not None:\n",
    "        corpus_doc_id_to_clusters: dict[int, list[set[data.Mention]]] = {}\n",
    "        for document in corpus.documents:\n",
    "            doc_id = document.doc_id\n",
    "            if len(document.clusters):\n",
    "                corpus_doc_id_to_clusters[doc_id] = document.clusters\n",
    "        \n",
    "        coref_metric2 = evaluate_clusters_scorch(\n",
    "            corpus_doc_id_to_clusters, predictions_doc_id_to_clusters)\n",
    "        return coref_metric1, coref_metric2\n",
    "    else:\n",
    "        return coref_metric1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(token_ids, mention_ids, label_ids, attn_mask,\n",
    "    global_attn_mask, doc_ids) = train_dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_ids = torch.Size([22601, 512]) torch.int64 cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"label_ids = {label_ids.shape} {label_ids.dtype} {label_ids.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_ids = label_ids.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert Tensor to Cluster: 100%|██████████| 10000/10000 [01:17<00:00, 128.96it/s]\n",
      "Collecting groundtruth clusters and mentions: 100%|██████████| 10000/10000 [00:00<00:00, 171624.09it/s]\n",
      "Collecting predictions clusters and mentions: 100%|██████████| 10000/10000 [00:00<00:00, 11469.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating MUC\n",
      "Calculating B3\n",
      "Calculating CEAF-e\n",
      "Calculating CEAF-m\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "coref_metric = evaluate_tensors_scorch(label_ids[:n], prediction_ids[:n],\n",
    "    doc_ids[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = data.CorefCorpus(\n",
    "    (\"/home/sbaruah_usc_edu/mica_text_coref/data/conll-2012/\"\n",
    "    \"gold/train.english.jsonlines\"), use_ascii_transliteration=True)\n",
    "seq_train_corpus = data_util.remove_overlaps(train_corpus)\n",
    "longformer_seq_train_corpus = data_util.remap_spans_document_level(\n",
    "    seq_train_corpus, tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conll(doc_key_with_part_id: str, sentences: list[list[str]],\n",
    "                    clusters: list[set[data.Mention]]) -> list[str]:\n",
    "    \"\"\"Create conll lines from clusters.\n",
    "\n",
    "    Args:\n",
    "        doc_key_with_part_id: The doc key in the jsonlines file.\n",
    "        sentences: List of sentence. Each sentence is a list of tokens (string).\n",
    "        clusters: List of cluster. Each cluster is a set of data.Mention\n",
    "        objects.\n",
    "    \n",
    "    Returns:\n",
    "        List of lines in conll-format. Each line contains the word and\n",
    "        coreference tag.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"(.+)_([^_]+)$\", doc_key_with_part_id)\n",
    "    doc_key, part_id = match.group(1), match.group(2)\n",
    "    total_n_tokens = sum(len(sentence) for sentence in sentences)\n",
    "    max_token_length = max(len(token) for sentence in sentences\n",
    "                            for token in sentence)\n",
    "    coref_column = [\"-\" for _ in range(total_n_tokens)]\n",
    "    mentions = [(mention.begin, mention.end, i + 1)\n",
    "                for i, cluster in enumerate(clusters) for mention in cluster]\n",
    "    non_unigram_mentions_sorted_by_begin = sorted(\n",
    "        filter(lambda mention: mention[1] - mention[0] > 0, mentions),\n",
    "        key=lambda mention: (mention[0], -mention[1]))\n",
    "    non_unigram_mentions_sorted_by_end = sorted(\n",
    "        filter(lambda mention: mention[1] - mention[0] > 0, mentions),\n",
    "        key=lambda mention: (mention[1], -mention[0]))\n",
    "    unigram_mentions = filter(lambda mention: mention[1] == mention[0],\n",
    "                            mentions)\n",
    "    \n",
    "    for begin, _, cluster_index in non_unigram_mentions_sorted_by_begin:\n",
    "        if coref_column[begin] == \"-\":\n",
    "            coref_column[begin] = \"(\" + str(cluster_index)\n",
    "        else:\n",
    "            coref_column[begin] += \"|(\" + str(cluster_index)\n",
    "    \n",
    "    for begin, _, cluster_index in unigram_mentions:\n",
    "        if coref_column[begin] == \"-\":\n",
    "            coref_column[begin] = \"(\" + str(cluster_index) + \")\"\n",
    "        else:\n",
    "            coref_column[begin] += \"|(\" + str(cluster_index) + \")\"\n",
    "    \n",
    "    for _, end, cluster_index in non_unigram_mentions_sorted_by_end:\n",
    "        if coref_column[end] == \"-\":\n",
    "            coref_column[end] = str(cluster_index) + \")\"\n",
    "        else:\n",
    "            coref_column[end] += \"|\" + str(cluster_index) + \")\"\n",
    "    \n",
    "    conll_lines = [f\"#begin document {doc_key}; part {part_id.zfill(3)}\\n\"]\n",
    "    filler = \"  -\" * 7\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        for j, token in enumerate(sentence):\n",
    "            line = (f\"{doc_key} {part_id} {j:>2} \"\n",
    "                   f\"{token:>{max_token_length}}{filler} {coref_column[i]}\\n\")\n",
    "            conll_lines.append(line)\n",
    "            i += 1\n",
    "        conll_lines.append(\"\\n\")\n",
    "    conll_lines = conll_lines[:-1] + [\"#end document\\n\"]\n",
    "\n",
    "    return conll_lines\n",
    "\n",
    "def evaluate_clusters_official(official_scorer: str,\n",
    "    groundtruth: dict[int, list[set[data.Mention]]],\n",
    "    predictions: dict[int, list[set[data.Mention]]],\n",
    "    doc_id_to_doc_key: dict[int, str],\n",
    "    doc_id_to_sentences: dict[int, list[list[str]]],\n",
    "    verbose=False) -> CoreferenceMetric:\n",
    "    \"\"\"Evaluates the predictions against the groundtruth annotations using\n",
    "    the official conll-2012 perl scorer. This function will throw an error if\n",
    "    any key of the groundtruth dictionary is not present in both the\n",
    "    doc_id_to_doc_key and doc_id_to_sentences dictionaries.\n",
    "\n",
    "    Args:\n",
    "        official_scorer: Path to the official perl script scorer.\n",
    "        groundtruth: A dictionary of list of groundtruth coreference clusters \n",
    "            (set of data.Mention objects) keyed by the doc id.\n",
    "        predictions: A dictionary of list of predicted coreference clusters\n",
    "            (set of data.Mention objects) keyed by the doc id.\n",
    "        doc_id_to_doc_key: A map (dictionary) from doc id to doc key.\n",
    "        doc_id_to_sentences: A map (dictionary) from doc id to list of \n",
    "            sentences. Each sentence is a list of string words.\n",
    "        verbose: set to true for verbose output\n",
    "    \n",
    "    Return:\n",
    "        CoreferenceMetric. This contains scores for MUC, B3, CEAFe, CEAFm, and\n",
    "        mention.\n",
    "    \"\"\"\n",
    "    gold_conll_lines, pred_conll_lines = [], []\n",
    "    \n",
    "    for doc_id, gold_clusters in tqdm.tqdm(groundtruth.items(),\n",
    "        desc=\"Creating conll\", total=len(groundtruth)):\n",
    "        doc_key = doc_id_to_doc_key[doc_id]\n",
    "        sentences = doc_id_to_sentences[doc_id]\n",
    "        pred_clusters = predictions[doc_id] if doc_id in predictions else []\n",
    "        \n",
    "        n_tokens = sum(len(s) for s in sentences)\n",
    "        if gold_clusters:\n",
    "            max_gold_end = max(mention.end for cluster in gold_clusters for mention in cluster)\n",
    "        else:\n",
    "            max_gold_end = -1\n",
    "        if pred_clusters:\n",
    "            max_pred_end = max(mention.end for cluster in pred_clusters for mention in cluster)\n",
    "        else:\n",
    "            max_pred_end = -1\n",
    "        print(doc_id, doc_key, n_tokens, max_gold_end, max_pred_end)\n",
    "        \n",
    "        gold_document_conll_lines = convert_to_conll(\n",
    "            doc_key, sentences, gold_clusters)\n",
    "        pred_document_conll_lines = convert_to_conll(\n",
    "            doc_key, sentences, pred_clusters)\n",
    "        gold_conll_lines.extend(gold_document_conll_lines)\n",
    "        pred_conll_lines.extend(pred_document_conll_lines)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", delete=True) as gold_file, \\\n",
    "        tempfile.NamedTemporaryFile(mode=\"w\", delete=True) as pred_file:\n",
    "        gold_file.writelines(gold_conll_lines)\n",
    "        pred_file.writelines(pred_conll_lines)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Gold file = {gold_file.name}\")\n",
    "            print(f\"Pred file = {pred_file.name}\")\n",
    "\n",
    "        cmd = [official_scorer, \"all\", gold_file.name, pred_file.name,\n",
    "                \"none\"]\n",
    "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        process.wait()\n",
    "        stdout = stdout.decode(\"utf-8\")\n",
    "\n",
    "        if verbose:\n",
    "            if stderr is not None:\n",
    "                print(stderr)\n",
    "            if stdout:\n",
    "                print(\"Official result\")\n",
    "                print(stdout)\n",
    "\n",
    "        matched_tuples = re.findall(\n",
    "            r\"Coreference: Recall: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\s+\"\n",
    "            r\"Precision: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\s+F1:\"\n",
    "            r\" ([0-9.]+)%\", stdout, flags=re.DOTALL)\n",
    "        \n",
    "        muc = Metric(float(matched_tuples[0][0]), float(matched_tuples[0][1]))\n",
    "        b3 = Metric(float(matched_tuples[1][0]), float(matched_tuples[1][1]))\n",
    "        ceafm = Metric(float(matched_tuples[2][0]), float(matched_tuples[2][1]))\n",
    "        ceafe = Metric(float(matched_tuples[3][0]), float(matched_tuples[3][1]))\n",
    "        \n",
    "        mention_match = re.search(\n",
    "            r\"Mentions: Recall: \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\s+Precision:\"\n",
    "            r\" \\([0-9.]+ / [0-9.]+\\) ([0-9.]+)%\\s+F1: ([0-9.]+)%\", stdout,\n",
    "            flags=re.DOTALL)\n",
    "        mention_metric = Metric(float(mention_match.group(1)),\n",
    "                                float(mention_match.group(2)))\n",
    "        official_metric = CoreferenceMetric(muc, b3, ceafe, ceafm,\n",
    "                                            mention_metric)\n",
    "        return official_metric\n",
    "\n",
    "def evaluate_tensors_official(official_scorer: str,\n",
    "    groundtruth: torch.LongTensor, predictions: torch.LongTensor,\n",
    "    doc_ids: torch.IntTensor, corpus: data.CorefCorpus\n",
    "    ) -> tuple[CoreferenceMetric, CoreferenceMetric]:\n",
    "    \"\"\"Evaluate the predictions against the groundtruth annotations using the\n",
    "    official conll-2012 perl scorer. The groundtruth and predictions are\n",
    "    represented by tensors.\n",
    "\n",
    "    Args:\n",
    "        official_scorer: Path to the official perl script scorer.\n",
    "        groundtruth: Integer tensor annotated with groundtruth cluster\n",
    "            mentions.\n",
    "        predictions: Integer tensor annotated with predicted cluster mentions.\n",
    "        doc_ids: Integer tensor containing doc ids of the corresponding\n",
    "            groundtruth and predictions tensors.\n",
    "        corpus: Original coreference corpus from which the groundtruth tensors\n",
    "            was created.\n",
    "    \n",
    "    Return:\n",
    "        Tuple of two CoreferenceMetric objects.\n",
    "    \"\"\"\n",
    "    corpus_doc_id_to_clusters: dict[int, list[set[data.Mention]]] = {}\n",
    "    groundtruth_doc_id_to_clusters: dict[int, list[set[data.Mention]]] = {}\n",
    "    predictions_doc_id_to_clusters: dict[int, list[set[data.Mention]]] = {}\n",
    "    doc_id_to_doc_key: dict[int, str] = corpus.get_doc_id_to_doc_key()\n",
    "    doc_id_to_sentences: dict[int, list[list[str]]] = (\n",
    "        corpus.get_doc_id_to_sentences())\n",
    "\n",
    "    for doc_id, gt_tensor, pred_tensor in tqdm.tqdm(zip(\n",
    "        doc_ids, groundtruth, predictions),\n",
    "        desc=\"Convert tensors to cluster\", total=len(doc_ids)):\n",
    "        gt_cluster = convert_tensor_to_cluster(gt_tensor)\n",
    "        pred_cluster = convert_tensor_to_cluster(pred_tensor)\n",
    "        doc_id = doc_id.item()\n",
    "        if len(gt_cluster):\n",
    "            if doc_id not in groundtruth_doc_id_to_clusters:\n",
    "                groundtruth_doc_id_to_clusters[doc_id] = []\n",
    "            groundtruth_doc_id_to_clusters[doc_id].append(gt_cluster)\n",
    "        if len(pred_cluster):\n",
    "            if doc_id not in predictions_doc_id_to_clusters:\n",
    "                predictions_doc_id_to_clusters[doc_id] = []\n",
    "            predictions_doc_id_to_clusters[doc_id].append(gt_cluster)\n",
    "\n",
    "    for document in corpus.documents:\n",
    "        doc_id = document.doc_id\n",
    "        if len(document.clusters):\n",
    "            corpus_doc_id_to_clusters[doc_id] = document.clusters\n",
    "    \n",
    "    coref_metric1 = evaluate_clusters_official(official_scorer, \n",
    "        groundtruth_doc_id_to_clusters, predictions_doc_id_to_clusters,\n",
    "        doc_id_to_doc_key, doc_id_to_sentences)\n",
    "    # coref_metric2 = evaluate_clusters_official(official_scorer, \n",
    "    #     corpus_doc_id_to_clusters, predictions_doc_id_to_clusters,\n",
    "    #     doc_id_to_doc_key, doc_id_to_sentences)\n",
    "    return coref_metric1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convert tensors to cluster: 100%|██████████| 10000/10000 [01:10<00:00, 141.52it/s]\n",
      "Creating conll:   7%|▋         | 85/1257 [00:00<00:01, 839.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bc/cctv/00/cctv_0001_0 401 346 346\n",
      "1 bc/cctv/00/cctv_0001_1 643 508 508\n",
      "2 bc/cctv/00/cctv_0001_2 1027 510 510\n",
      "3 bc/cctv/00/cctv_0001_3 340 335 335\n",
      "4 bc/cctv/00/cctv_0001_4 564 509 509\n",
      "5 bc/cctv/00/cctv_0001_5 679 499 499\n",
      "6 bc/cctv/00/cctv_0001_6 262 238 238\n",
      "7 bc/cctv/00/cctv_0001_7 942 376 376\n",
      "8 bc/cctv/00/cctv_0001_8 234 228 228\n",
      "9 bc/cctv/00/cctv_0002_0 256 225 225\n",
      "10 bc/cctv/00/cctv_0002_1 106 98 98\n",
      "11 bc/cctv/00/cctv_0002_2 434 420 420\n",
      "12 bc/cctv/00/cctv_0002_3 502 488 488\n",
      "13 bc/cctv/00/cctv_0002_4 362 352 352\n",
      "14 bc/cctv/00/cctv_0002_5 275 252 252\n",
      "15 bc/cctv/00/cctv_0002_6 411 388 388\n",
      "16 bc/cctv/00/cctv_0002_7 342 332 332\n",
      "17 bc/cctv/00/cctv_0002_8 657 500 500\n",
      "18 bc/cctv/00/cctv_0002_9 499 490 490\n",
      "19 bc/cctv/00/cctv_0002_10 405 403 403\n",
      "20 bc/cctv/00/cctv_0002_11 469 450 450\n",
      "21 bc/cctv/00/cctv_0002_12 478 461 461\n",
      "22 bc/cctv/00/cctv_0002_13 242 228 228\n",
      "23 bc/cctv/00/cctv_0002_14 484 470 470\n",
      "24 bc/cctv/00/cctv_0002_15 394 373 373\n",
      "25 bc/cctv/00/cctv_0002_16 326 317 317\n",
      "26 bc/cctv/00/cctv_0002_17 230 228 228\n",
      "27 bc/cctv/00/cctv_0002_18 477 431 431\n",
      "28 bc/cctv/00/cctv_0002_19 309 293 293\n",
      "29 bc/cctv/00/cctv_0002_20 498 187 187\n",
      "30 bc/cctv/00/cctv_0003_0 418 397 397\n",
      "31 bc/cctv/00/cctv_0003_1 400 398 398\n",
      "32 bc/cctv/00/cctv_0003_2 426 384 384\n",
      "33 bc/cctv/00/cctv_0003_3 591 492 492\n",
      "34 bc/cctv/00/cctv_0003_4 444 440 440\n",
      "35 bc/cctv/00/cctv_0003_5 566 509 509\n",
      "36 bc/cctv/00/cctv_0003_6 429 407 407\n",
      "37 bc/cctv/00/cctv_0003_7 349 345 345\n",
      "38 bc/cctv/00/cctv_0003_8 541 441 441\n",
      "39 bc/cctv/00/cctv_0003_9 257 225 225\n",
      "40 bc/cctv/00/cctv_0003_10 532 505 505\n",
      "41 bc/cctv/00/cctv_0003_11 581 380 380\n",
      "42 bc/cctv/00/cctv_0003_12 176 125 125\n",
      "43 bc/cctv/00/cctv_0003_13 423 411 411\n",
      "44 bc/cctv/00/cctv_0003_14 292 233 233\n",
      "45 bc/cctv/00/cctv_0003_15 329 322 322\n",
      "46 bc/cctv/00/cctv_0004_0 68 62 62\n",
      "47 bc/cctv/00/cctv_0004_1 127 119 119\n",
      "48 bc/cctv/00/cctv_0004_2 399 397 397\n",
      "49 bc/cctv/00/cctv_0004_3 395 371 371\n",
      "50 bc/cctv/00/cctv_0004_4 375 366 366\n",
      "51 bc/cctv/00/cctv_0004_5 493 482 482\n",
      "52 bc/cctv/00/cctv_0004_6 444 434 434\n",
      "53 bc/cctv/00/cctv_0004_7 469 464 464\n",
      "54 bc/cctv/00/cctv_0004_8 434 421 421\n",
      "55 bc/cctv/00/cctv_0004_9 533 441 441\n",
      "56 bc/cctv/00/cctv_0004_10 497 490 490\n",
      "57 bc/cctv/00/cctv_0004_11 312 310 310\n",
      "58 bc/cctv/00/cctv_0004_12 490 471 471\n",
      "59 bc/cctv/00/cctv_0004_13 335 319 319\n",
      "60 bc/cctv/00/cctv_0004_14 451 449 449\n",
      "61 bc/cctv/00/cctv_0004_15 536 496 496\n",
      "62 bc/cctv/00/cctv_0004_16 352 344 344\n",
      "63 bc/cctv/00/cctv_0004_17 493 192 192\n",
      "64 bc/cnn/00/cnn_0001_0 132 127 127\n",
      "65 bc/cnn/00/cnn_0001_1 174 165 165\n",
      "66 bc/cnn/00/cnn_0001_2 1046 337 337\n",
      "67 bc/cnn/00/cnn_0001_3 604 489 489\n",
      "68 bc/cnn/00/cnn_0001_4 1097 507 507\n",
      "69 bc/cnn/00/cnn_0001_5 71 68 68\n",
      "70 bc/cnn/00/cnn_0001_6 376 369 369\n",
      "71 bc/cnn/00/cnn_0001_7 585 482 482\n",
      "72 bc/cnn/00/cnn_0001_8 753 489 489\n",
      "74 bc/cnn/00/cnn_0001_10 575 488 488\n",
      "75 bc/cnn/00/cnn_0001_11 670 506 506\n",
      "76 bc/cnn/00/cnn_0001_12 762 493 493\n",
      "77 bc/cnn/00/cnn_0001_13 643 465 465\n",
      "78 bc/cnn/00/cnn_0001_14 118 114 114\n",
      "79 bc/cnn/00/cnn_0001_15 624 460 460\n",
      "80 bc/cnn/00/cnn_0001_16 325 323 323\n",
      "81 bc/cnn/00/cnn_0001_17 634 504 504\n",
      "82 bc/cnn/00/cnn_0002_0 704 490 490\n",
      "83 bc/cnn/00/cnn_0002_1 523 511 511\n",
      "84 bc/cnn/00/cnn_0002_2 540 473 473\n",
      "85 bc/cnn/00/cnn_0002_3 961 499 499\n",
      "86 bc/cnn/00/cnn_0002_4 127 125 125\n",
      "87 bc/cnn/00/cnn_0002_5 332 326 326\n",
      "88 bc/cnn/00/cnn_0002_6 1182 502 502\n",
      "89 bc/cnn/00/cnn_0002_7 85 83 83\n",
      "90 bc/cnn/00/cnn_0002_8 661 474 474\n",
      "91 bc/cnn/00/cnn_0002_9 356 355 355\n",
      "92 bc/cnn/00/cnn_0002_10 561 469 469\n",
      "93 bc/cnn/00/cnn_0002_11 744 504 504\n",
      "94 bc/cnn/00/cnn_0002_12 365 363 363\n",
      "95 bc/cnn/00/cnn_0002_13 193 190 190\n",
      "96 bc/cnn/00/cnn_0002_14 702 433 433\n",
      "97 bc/cnn/00/cnn_0002_15 523 500 500\n",
      "98 bc/cnn/00/cnn_0002_16 1278 460 460\n",
      "99 bc/cnn/00/cnn_0002_17 611 506 506\n",
      "100 bc/cnn/00/cnn_0003_0 820 349 349\n",
      "101 bc/cnn/00/cnn_0003_1 803 478 478\n",
      "102 bc/cnn/00/cnn_0003_2 475 471 471\n",
      "103 bc/cnn/00/cnn_0003_3 574 483 483\n",
      "104 bc/cnn/00/cnn_0003_4 721 498 498\n",
      "105 bc/cnn/00/cnn_0003_5 1061 438 438\n",
      "106 bc/cnn/00/cnn_0003_6 236 205 205\n",
      "107 bc/cnn/00/cnn_0003_7 437 433 433\n",
      "108 bc/cnn/00/cnn_0003_8 647 442 442\n",
      "109 bc/cnn/00/cnn_0003_9 438 433 433\n",
      "110 bc/cnn/00/cnn_0003_10 440 402 402\n",
      "111 bc/cnn/00/cnn_0003_11 433 428 428\n",
      "112 bc/cnn/00/cnn_0003_12 372 370 370\n",
      "113 bc/cnn/00/cnn_0003_13 406 404 404\n",
      "114 bc/cnn/00/cnn_0003_14 156 91 91\n",
      "115 bc/cnn/00/cnn_0004_0 52 43 43\n",
      "116 bc/cnn/00/cnn_0004_1 1223 309 309\n",
      "117 bc/cnn/00/cnn_0004_2 871 505 505\n",
      "118 bc/cnn/00/cnn_0004_3 842 502 502\n",
      "119 bc/cnn/00/cnn_0004_4 977 498 498\n",
      "120 bc/cnn/00/cnn_0004_5 835 449 449\n",
      "121 bc/cnn/00/cnn_0004_6 295 282 282\n",
      "122 bc/cnn/00/cnn_0004_7 250 237 237\n",
      "123 bc/cnn/00/cnn_0004_8 378 377 377\n",
      "124 bc/cnn/00/cnn_0004_9 76 69 69\n",
      "125 bc/cnn/00/cnn_0004_10 228 225 225\n",
      "126 bc/cnn/00/cnn_0004_11 152 134 134\n",
      "127 bc/cnn/00/cnn_0004_12 626 495 495\n",
      "128 bc/cnn/00/cnn_0005_0 118 106 106\n",
      "129 bc/cnn/00/cnn_0005_1 188 186 186\n",
      "130 bc/cnn/00/cnn_0005_2 865 478 478\n",
      "131 bc/cnn/00/cnn_0005_3 685 506 506\n",
      "132 bc/cnn/00/cnn_0005_4 134 121 121\n",
      "133 bc/cnn/00/cnn_0005_5 1017 502 502\n",
      "134 bc/cnn/00/cnn_0005_6 781 490 490\n",
      "135 bc/cnn/00/cnn_0005_7 61 54 54\n",
      "136 bc/cnn/00/cnn_0005_8 173 150 150\n",
      "137 bc/cnn/00/cnn_0005_9 1169 433 433\n",
      "138 bc/cnn/00/cnn_0005_10 767 488 488\n",
      "139 bc/cnn/00/cnn_0005_11 990 490 490\n",
      "140 bc/cnn/00/cnn_0005_12 840 504 504\n",
      "141 bc/cnn/00/cnn_0006_0 110 96 96\n",
      "142 bc/cnn/00/cnn_0006_1 211 209 209\n",
      "143 bc/cnn/00/cnn_0006_2 849 510 510\n",
      "144 bc/cnn/00/cnn_0006_3 630 511 511\n",
      "145 bc/cnn/00/cnn_0006_4 474 463 463\n",
      "146 bc/cnn/00/cnn_0006_5 784 469 469\n",
      "147 bc/cnn/00/cnn_0006_6 151 140 140\n",
      "148 bc/cnn/00/cnn_0006_7 660 487 487\n",
      "149 bc/cnn/00/cnn_0006_8 715 500 500\n",
      "150 bc/cnn/00/cnn_0006_9 923 419 419\n",
      "151 bc/cnn/00/cnn_0006_10 286 281 281\n",
      "152 bc/cnn/00/cnn_0006_11 533 502 502\n",
      "153 bc/cnn/00/cnn_0006_12 641 485 485\n",
      "154 bc/cnn/00/cnn_0006_13 947 465 465\n",
      "155 bc/cnn/00/cnn_0006_14 603 471 471\n",
      "156 bc/cnn/00/cnn_0007_0 364 350 350\n",
      "157 bc/cnn/00/cnn_0007_1 968 457 457\n",
      "158 bc/cnn/00/cnn_0007_2 1071 488 488\n",
      "159 bc/cnn/00/cnn_0007_3 379 373 373\n",
      "160 bc/cnn/00/cnn_0007_4 808 507 507\n",
      "161 bc/cnn/00/cnn_0007_5 674 500 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating conll:  19%|█▉        | 244/1257 [00:00<00:01, 735.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 bc/msnbc/00/msnbc_0001_0 405 400 400\n",
      "163 bc/msnbc/00/msnbc_0001_1 687 501 501\n",
      "164 bc/msnbc/00/msnbc_0001_2 951 505 505\n",
      "165 bc/msnbc/00/msnbc_0001_3 561 508 508\n",
      "166 bc/msnbc/00/msnbc_0001_4 1076 505 505\n",
      "167 bc/msnbc/00/msnbc_0001_5 608 475 475\n",
      "168 bc/msnbc/00/msnbc_0001_6 235 227 227\n",
      "169 bc/msnbc/00/msnbc_0001_7 235 201 201\n",
      "170 bc/msnbc/00/msnbc_0001_8 396 391 391\n",
      "171 bc/msnbc/00/msnbc_0001_9 564 508 508\n",
      "172 bc/msnbc/00/msnbc_0001_10 829 454 454\n",
      "173 bc/msnbc/00/msnbc_0001_11 431 422 422\n",
      "174 bc/msnbc/00/msnbc_0001_12 809 450 450\n",
      "175 bc/msnbc/00/msnbc_0001_13 192 166 166\n",
      "176 bc/msnbc/00/msnbc_0001_14 494 488 488\n",
      "177 bc/msnbc/00/msnbc_0001_15 409 397 397\n",
      "178 bc/msnbc/00/msnbc_0002_0 625 374 374\n",
      "179 bc/msnbc/00/msnbc_0002_1 1085 500 500\n",
      "180 bc/msnbc/00/msnbc_0002_2 589 503 503\n",
      "181 bc/msnbc/00/msnbc_0002_3 886 501 501\n",
      "182 bc/msnbc/00/msnbc_0002_4 320 309 309\n",
      "183 bc/msnbc/00/msnbc_0002_5 453 451 451\n",
      "184 bc/msnbc/00/msnbc_0002_6 390 359 359\n",
      "185 bc/msnbc/00/msnbc_0002_7 822 460 460\n",
      "186 bc/msnbc/00/msnbc_0002_8 578 500 500\n",
      "187 bc/msnbc/00/msnbc_0002_9 218 201 201\n",
      "188 bc/msnbc/00/msnbc_0002_10 210 198 198\n",
      "189 bc/msnbc/00/msnbc_0002_11 362 354 354\n",
      "190 bc/msnbc/00/msnbc_0002_12 837 475 475\n",
      "191 bc/msnbc/00/msnbc_0002_13 858 385 385\n",
      "192 bc/msnbc/00/msnbc_0002_14 110 101 101\n",
      "193 bc/msnbc/00/msnbc_0002_15 229 226 226\n",
      "194 bc/msnbc/00/msnbc_0002_16 955 507 507\n",
      "195 bc/msnbc/00/msnbc_0002_17 61 23 23\n",
      "196 bc/msnbc/00/msnbc_0002_18 599 437 437\n",
      "197 bc/msnbc/00/msnbc_0002_19 315 246 246\n",
      "198 bc/msnbc/00/msnbc_0003_0 673 457 457\n",
      "199 bc/msnbc/00/msnbc_0003_1 774 462 462\n",
      "200 bc/msnbc/00/msnbc_0003_2 808 455 455\n",
      "201 bc/msnbc/00/msnbc_0003_3 388 386 386\n",
      "202 bc/msnbc/00/msnbc_0003_4 89 86 86\n",
      "203 bc/msnbc/00/msnbc_0003_5 635 497 497\n",
      "204 bc/msnbc/00/msnbc_0003_6 763 506 506\n",
      "205 bc/msnbc/00/msnbc_0003_7 841 498 498\n",
      "206 bc/msnbc/00/msnbc_0003_8 951 490 490\n",
      "207 bc/msnbc/00/msnbc_0003_9 379 345 345\n",
      "208 bc/msnbc/00/msnbc_0003_10 533 510 510\n",
      "209 bc/msnbc/00/msnbc_0003_11 448 429 429\n",
      "210 bc/msnbc/00/msnbc_0003_12 1006 505 505\n",
      "211 bc/msnbc/00/msnbc_0003_13 1100 471 471\n",
      "212 bc/msnbc/00/msnbc_0003_14 502 500 500\n",
      "213 bc/msnbc/00/msnbc_0003_15 813 463 463\n",
      "214 bc/msnbc/00/msnbc_0004_0 158 150 150\n",
      "215 bc/msnbc/00/msnbc_0004_1 561 483 483\n",
      "216 bc/msnbc/00/msnbc_0004_2 306 302 302\n",
      "217 bc/msnbc/00/msnbc_0004_3 287 279 279\n",
      "218 bc/msnbc/00/msnbc_0004_4 421 417 417\n",
      "219 bc/msnbc/00/msnbc_0004_5 210 204 204\n",
      "220 bc/msnbc/00/msnbc_0004_6 68 52 52\n",
      "221 bc/msnbc/00/msnbc_0004_7 334 278 278\n",
      "222 bc/msnbc/00/msnbc_0004_8 451 436 436\n",
      "223 bc/msnbc/00/msnbc_0004_9 420 414 414\n",
      "224 bc/msnbc/00/msnbc_0004_10 51 41 41\n",
      "225 bc/msnbc/00/msnbc_0004_11 843 501 501\n",
      "226 bc/msnbc/00/msnbc_0004_12 37 25 25\n",
      "227 bc/msnbc/00/msnbc_0004_13 231 218 218\n",
      "228 bc/msnbc/00/msnbc_0004_14 493 455 455\n",
      "229 bc/msnbc/00/msnbc_0004_15 562 502 502\n",
      "230 bc/msnbc/00/msnbc_0004_16 831 497 497\n",
      "231 bc/msnbc/00/msnbc_0004_17 278 239 239\n",
      "232 bc/msnbc/00/msnbc_0004_18 502 500 500\n",
      "233 bc/msnbc/00/msnbc_0004_19 422 420 420\n",
      "234 bc/msnbc/00/msnbc_0004_20 718 511 511\n",
      "235 bc/msnbc/00/msnbc_0004_21 825 498 498\n",
      "236 bc/msnbc/00/msnbc_0004_22 549 492 492\n",
      "237 bc/msnbc/00/msnbc_0005_0 112 87 87\n",
      "238 bc/msnbc/00/msnbc_0005_1 493 491 491\n",
      "239 bc/msnbc/00/msnbc_0005_2 794 422 422\n",
      "240 bc/msnbc/00/msnbc_0005_3 758 493 493\n",
      "241 bc/msnbc/00/msnbc_0005_4 840 504 504\n",
      "242 bc/msnbc/00/msnbc_0005_5 589 492 492\n",
      "243 bc/msnbc/00/msnbc_0005_6 644 480 480\n",
      "244 bc/msnbc/00/msnbc_0005_7 808 392 392\n",
      "245 bc/msnbc/00/msnbc_0005_8 608 509 509\n",
      "246 bc/msnbc/00/msnbc_0005_9 945 495 495\n",
      "247 bc/msnbc/00/msnbc_0005_10 816 486 486\n",
      "248 bc/msnbc/00/msnbc_0005_11 471 468 468\n",
      "249 bc/msnbc/00/msnbc_0005_12 425 397 397\n",
      "250 bc/msnbc/00/msnbc_0005_13 895 465 465\n",
      "251 bc/msnbc/00/msnbc_0005_14 700 499 499\n",
      "252 bc/msnbc/00/msnbc_0005_15 667 489 489\n",
      "253 bc/msnbc/00/msnbc_0006_0 418 356 356\n",
      "254 bc/msnbc/00/msnbc_0006_1 283 264 264\n",
      "255 bc/msnbc/00/msnbc_0006_2 1548 485 485\n",
      "256 bc/msnbc/00/msnbc_0006_3 1322 435 435\n",
      "257 bc/msnbc/00/msnbc_0006_4 705 505 505\n",
      "258 bc/msnbc/00/msnbc_0006_5 644 502 502\n",
      "259 bc/msnbc/00/msnbc_0006_6 778 507 507\n",
      "260 bc/msnbc/00/msnbc_0006_7 404 402 402\n",
      "261 bc/msnbc/00/msnbc_0006_8 1166 477 477\n",
      "262 bc/msnbc/00/msnbc_0006_9 430 424 424\n",
      "263 bc/msnbc/00/msnbc_0006_10 1006 505 505\n",
      "264 bc/msnbc/00/msnbc_0006_11 652 489 489\n",
      "265 bc/msnbc/00/msnbc_0006_12 524 505 505\n",
      "266 bc/msnbc/00/msnbc_0006_13 449 443 443\n",
      "267 bc/msnbc/00/msnbc_0006_14 730 497 497\n",
      "268 bc/msnbc/00/msnbc_0006_15 588 508 508\n",
      "269 bc/phoenix/00/phoenix_0001_0 874 507 507\n",
      "270 bc/phoenix/00/phoenix_0001_1 431 424 424\n",
      "271 bc/phoenix/00/phoenix_0001_2 759 441 441\n",
      "272 bc/phoenix/00/phoenix_0001_3 440 435 435\n",
      "273 bc/phoenix/00/phoenix_0001_4 742 457 457\n",
      "274 bc/phoenix/00/phoenix_0001_5 541 509 509\n",
      "275 bc/phoenix/00/phoenix_0001_6 920 502 502\n",
      "276 bc/phoenix/00/phoenix_0002_0 743 453 453\n",
      "277 bc/phoenix/00/phoenix_0002_1 483 474 474\n",
      "278 bc/phoenix/00/phoenix_0002_2 1123 450 450\n",
      "279 bc/phoenix/00/phoenix_0002_3 403 386 386\n",
      "280 bc/phoenix/00/phoenix_0002_4 940 510 510\n",
      "281 bc/phoenix/00/phoenix_0002_5 811 460 460\n",
      "282 bc/phoenix/00/phoenix_0002_6 892 457 457\n",
      "283 bn/abc/00/abc_0001_0 490 488 488\n",
      "284 bn/abc/00/abc_0002_0 463 461 461\n",
      "285 bn/abc/00/abc_0003_0 494 492 492\n",
      "286 bn/abc/00/abc_0004_0 480 472 472\n",
      "287 bn/abc/00/abc_0005_0 379 375 375\n",
      "288 bn/abc/00/abc_0006_0 56 45 45\n",
      "289 bn/abc/00/abc_0007_0 405 403 403\n",
      "291 bn/abc/00/abc_0011_0 41 34 34\n",
      "293 bn/abc/00/abc_0013_0 342 336 336\n",
      "294 bn/abc/00/abc_0014_0 460 450 450\n",
      "295 bn/abc/00/abc_0015_0 580 486 486\n",
      "296 bn/abc/00/abc_0016_0 406 401 401\n",
      "297 bn/abc/00/abc_0017_0 428 416 416\n",
      "298 bn/abc/00/abc_0018_0 416 404 404\n",
      "299 bn/abc/00/abc_0021_0 294 290 290\n",
      "300 bn/abc/00/abc_0022_0 349 347 347\n",
      "301 bn/abc/00/abc_0023_0 511 507 507\n",
      "303 bn/abc/00/abc_0025_0 422 417 417\n",
      "304 bn/abc/00/abc_0026_0 415 413 413\n",
      "305 bn/abc/00/abc_0027_0 618 501 501\n",
      "306 bn/abc/00/abc_0028_0 418 416 416\n",
      "307 bn/abc/00/abc_0031_0 58 41 41\n",
      "308 bn/abc/00/abc_0032_0 53 47 47\n",
      "309 bn/abc/00/abc_0033_0 48 45 45\n",
      "310 bn/abc/00/abc_0034_0 349 345 345\n",
      "311 bn/abc/00/abc_0035_0 579 490 490\n",
      "312 bn/abc/00/abc_0036_0 95 83 83\n",
      "313 bn/abc/00/abc_0037_0 43 34 34\n",
      "314 bn/abc/00/abc_0038_0 651 494 494\n",
      "315 bn/abc/00/abc_0041_0 408 371 371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating conll:  42%|████▏     | 531/1257 [00:00<00:00, 1217.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 bn/abc/00/abc_0042_0 340 338 338\n",
      "317 bn/abc/00/abc_0043_0 253 251 251\n",
      "318 bn/abc/00/abc_0044_0 69 54 54\n",
      "319 bn/abc/00/abc_0045_0 68 60 60\n",
      "320 bn/abc/00/abc_0046_0 55 53 53\n",
      "321 bn/abc/00/abc_0047_0 424 420 420\n",
      "322 bn/abc/00/abc_0048_0 407 405 405\n",
      "323 bn/abc/00/abc_0051_0 503 501 501\n",
      "324 bn/abc/00/abc_0052_0 443 441 441\n",
      "325 bn/abc/00/abc_0053_0 304 300 300\n",
      "327 bn/abc/00/abc_0055_0 512 510 510\n",
      "328 bn/abc/00/abc_0056_0 507 505 505\n",
      "329 bn/abc/00/abc_0057_0 78 71 71\n",
      "330 bn/abc/00/abc_0058_0 49 37 37\n",
      "331 bn/abc/00/abc_0061_0 42 27 27\n",
      "332 bn/abc/00/abc_0062_0 397 395 395\n",
      "333 bn/abc/00/abc_0063_0 481 476 476\n",
      "334 bn/abc/00/abc_0064_0 477 470 470\n",
      "335 bn/abc/00/abc_0065_0 279 258 258\n",
      "336 bn/abc/00/abc_0066_0 495 493 493\n",
      "337 bn/abc/00/abc_0067_0 311 307 307\n",
      "338 bn/abc/00/abc_0068_0 74 62 62\n",
      "339 bn/cnn/00/cnn_0001_0 294 289 289\n",
      "340 bn/cnn/00/cnn_0002_0 172 150 150\n",
      "341 bn/cnn/00/cnn_0003_0 107 87 87\n",
      "342 bn/cnn/00/cnn_0004_0 96 92 92\n",
      "343 bn/cnn/00/cnn_0005_0 276 274 274\n",
      "344 bn/cnn/00/cnn_0006_0 57 47 47\n",
      "345 bn/cnn/00/cnn_0007_0 67 41 41\n",
      "346 bn/cnn/00/cnn_0008_0 182 125 125\n",
      "347 bn/cnn/00/cnn_0011_0 102 98 98\n",
      "348 bn/cnn/00/cnn_0012_0 154 146 146\n",
      "349 bn/cnn/00/cnn_0013_0 222 220 220\n",
      "350 bn/cnn/00/cnn_0014_0 94 92 92\n",
      "351 bn/cnn/00/cnn_0015_0 366 354 354\n",
      "352 bn/cnn/00/cnn_0016_0 119 102 102\n",
      "353 bn/cnn/00/cnn_0017_0 72 67 67\n",
      "354 bn/cnn/00/cnn_0018_0 185 167 167\n",
      "355 bn/cnn/00/cnn_0021_0 86 78 78\n",
      "356 bn/cnn/00/cnn_0022_0 115 109 109\n",
      "357 bn/cnn/00/cnn_0023_0 93 76 76\n",
      "358 bn/cnn/00/cnn_0024_0 90 21 21\n",
      "359 bn/cnn/00/cnn_0025_0 627 493 493\n",
      "360 bn/cnn/00/cnn_0026_0 110 101 101\n",
      "361 bn/cnn/00/cnn_0027_0 77 69 69\n",
      "362 bn/cnn/00/cnn_0028_0 136 124 124\n",
      "363 bn/cnn/00/cnn_0031_0 74 51 51\n",
      "364 bn/cnn/00/cnn_0032_0 105 97 97\n",
      "365 bn/cnn/00/cnn_0033_0 167 131 131\n",
      "366 bn/cnn/00/cnn_0034_0 123 48 48\n",
      "367 bn/cnn/00/cnn_0035_0 104 94 94\n",
      "368 bn/cnn/00/cnn_0036_0 194 192 192\n",
      "369 bn/cnn/00/cnn_0037_0 259 236 236\n",
      "370 bn/cnn/00/cnn_0038_0 412 389 389\n",
      "371 bn/cnn/00/cnn_0041_0 291 287 287\n",
      "372 bn/cnn/00/cnn_0042_0 89 80 80\n",
      "373 bn/cnn/00/cnn_0043_0 86 74 74\n",
      "374 bn/cnn/00/cnn_0044_0 143 138 138\n",
      "375 bn/cnn/00/cnn_0045_0 102 89 89\n",
      "376 bn/cnn/00/cnn_0046_0 209 201 201\n",
      "377 bn/cnn/00/cnn_0047_0 73 66 66\n",
      "378 bn/cnn/00/cnn_0048_0 152 146 146\n",
      "379 bn/cnn/00/cnn_0051_0 112 90 90\n",
      "380 bn/cnn/00/cnn_0052_0 73 53 53\n",
      "381 bn/cnn/00/cnn_0053_0 65 23 23\n",
      "382 bn/cnn/00/cnn_0054_0 82 78 78\n",
      "383 bn/cnn/00/cnn_0055_0 75 71 71\n",
      "384 bn/cnn/00/cnn_0056_0 70 51 51\n",
      "385 bn/cnn/00/cnn_0057_0 50 22 22\n",
      "386 bn/cnn/00/cnn_0058_0 380 378 378\n",
      "387 bn/cnn/00/cnn_0061_0 130 124 124\n",
      "388 bn/cnn/00/cnn_0062_0 88 74 74\n",
      "389 bn/cnn/00/cnn_0063_0 476 474 474\n",
      "390 bn/cnn/00/cnn_0064_0 104 101 101\n",
      "391 bn/cnn/00/cnn_0065_0 406 404 404\n",
      "392 bn/cnn/00/cnn_0066_0 202 197 197\n",
      "393 bn/cnn/00/cnn_0067_0 272 260 260\n",
      "394 bn/cnn/00/cnn_0068_0 443 432 432\n",
      "395 bn/cnn/00/cnn_0071_0 68 54 54\n",
      "396 bn/cnn/00/cnn_0072_0 83 73 73\n",
      "397 bn/cnn/00/cnn_0073_0 68 54 54\n",
      "398 bn/cnn/00/cnn_0074_0 136 134 134\n",
      "399 bn/cnn/00/cnn_0075_0 108 82 82\n",
      "400 bn/cnn/00/cnn_0076_0 218 216 216\n",
      "401 bn/cnn/00/cnn_0077_0 119 112 112\n",
      "402 bn/cnn/00/cnn_0078_0 103 94 94\n",
      "403 bn/cnn/00/cnn_0081_0 76 57 57\n",
      "404 bn/cnn/00/cnn_0082_0 97 80 80\n",
      "405 bn/cnn/00/cnn_0083_0 122 107 107\n",
      "406 bn/cnn/00/cnn_0084_0 134 132 132\n",
      "407 bn/cnn/00/cnn_0085_0 73 55 55\n",
      "408 bn/cnn/00/cnn_0086_0 67 52 52\n",
      "409 bn/cnn/00/cnn_0087_0 239 237 237\n",
      "410 bn/cnn/00/cnn_0088_0 120 104 104\n",
      "411 bn/cnn/00/cnn_0091_0 361 351 351\n",
      "412 bn/cnn/00/cnn_0092_0 87 85 85\n",
      "413 bn/cnn/00/cnn_0093_0 57 45 45\n",
      "414 bn/cnn/00/cnn_0094_0 122 95 95\n",
      "415 bn/cnn/00/cnn_0095_0 127 122 122\n",
      "416 bn/cnn/00/cnn_0096_0 307 280 280\n",
      "417 bn/cnn/00/cnn_0097_0 67 62 62\n",
      "418 bn/cnn/00/cnn_0098_0 97 90 90\n",
      "419 bn/cnn/01/cnn_0101_0 60 44 44\n",
      "420 bn/cnn/01/cnn_0102_0 75 41 41\n",
      "421 bn/cnn/01/cnn_0103_0 107 102 102\n",
      "422 bn/cnn/01/cnn_0104_0 89 73 73\n",
      "423 bn/cnn/01/cnn_0105_0 101 80 80\n",
      "424 bn/cnn/01/cnn_0106_0 118 115 115\n",
      "425 bn/cnn/01/cnn_0107_0 75 73 73\n",
      "426 bn/cnn/01/cnn_0108_0 88 69 69\n",
      "427 bn/cnn/01/cnn_0111_0 250 242 242\n",
      "428 bn/cnn/01/cnn_0112_0 112 99 99\n",
      "429 bn/cnn/01/cnn_0113_0 527 500 500\n",
      "430 bn/cnn/01/cnn_0114_0 144 140 140\n",
      "431 bn/cnn/01/cnn_0115_0 95 85 85\n",
      "432 bn/cnn/01/cnn_0116_0 74 68 68\n",
      "433 bn/cnn/01/cnn_0117_0 107 102 102\n",
      "434 bn/cnn/01/cnn_0118_0 89 72 72\n",
      "435 bn/cnn/01/cnn_0121_0 232 225 225\n",
      "436 bn/cnn/01/cnn_0122_0 77 60 60\n",
      "437 bn/cnn/01/cnn_0123_0 129 49 49\n",
      "438 bn/cnn/01/cnn_0124_0 176 163 163\n",
      "439 bn/cnn/01/cnn_0125_0 97 95 95\n",
      "440 bn/cnn/01/cnn_0126_0 86 84 84\n",
      "441 bn/cnn/01/cnn_0127_0 149 134 134\n",
      "442 bn/cnn/01/cnn_0128_0 113 87 87\n",
      "443 bn/cnn/01/cnn_0131_0 101 91 91\n",
      "444 bn/cnn/01/cnn_0132_0 133 129 129\n",
      "445 bn/cnn/01/cnn_0133_0 116 114 114\n",
      "446 bn/cnn/01/cnn_0134_0 186 176 176\n",
      "447 bn/cnn/01/cnn_0135_0 88 86 86\n",
      "448 bn/cnn/01/cnn_0136_0 71 69 69\n",
      "449 bn/cnn/01/cnn_0137_0 87 76 76\n",
      "450 bn/cnn/01/cnn_0138_0 91 89 89\n",
      "451 bn/cnn/01/cnn_0141_0 477 471 471\n",
      "452 bn/cnn/01/cnn_0142_0 87 78 78\n",
      "454 bn/cnn/01/cnn_0144_0 107 103 103\n",
      "455 bn/cnn/01/cnn_0145_0 276 268 268\n",
      "456 bn/cnn/01/cnn_0146_0 76 67 67\n",
      "457 bn/cnn/01/cnn_0147_0 355 351 351\n",
      "458 bn/cnn/01/cnn_0148_0 121 113 113\n",
      "459 bn/cnn/01/cnn_0151_0 111 101 101\n",
      "460 bn/cnn/01/cnn_0152_0 350 342 342\n",
      "461 bn/cnn/01/cnn_0153_0 98 84 84\n",
      "462 bn/cnn/01/cnn_0154_0 160 147 147\n",
      "463 bn/cnn/01/cnn_0155_0 70 63 63\n",
      "464 bn/cnn/01/cnn_0156_0 244 242 242\n",
      "465 bn/cnn/01/cnn_0157_0 91 70 70\n",
      "466 bn/cnn/01/cnn_0158_0 434 428 428\n",
      "467 bn/cnn/01/cnn_0161_0 130 110 110\n",
      "468 bn/cnn/01/cnn_0162_0 108 106 106\n",
      "469 bn/cnn/01/cnn_0163_0 101 99 99\n",
      "470 bn/cnn/01/cnn_0164_0 398 396 396\n",
      "471 bn/cnn/01/cnn_0165_0 77 73 73\n",
      "472 bn/cnn/01/cnn_0166_0 104 102 102\n",
      "473 bn/cnn/01/cnn_0167_0 566 492 492\n",
      "474 bn/cnn/01/cnn_0168_0 116 97 97\n",
      "475 bn/cnn/01/cnn_0171_0 201 193 193\n",
      "476 bn/cnn/01/cnn_0172_0 61 52 52\n",
      "477 bn/cnn/01/cnn_0173_0 98 93 93\n",
      "478 bn/cnn/01/cnn_0174_0 116 110 110\n",
      "479 bn/cnn/01/cnn_0175_0 402 398 398\n",
      "480 bn/cnn/01/cnn_0176_0 78 62 62\n",
      "481 bn/cnn/01/cnn_0177_0 80 62 62\n",
      "482 bn/cnn/01/cnn_0178_0 89 87 87\n",
      "483 bn/cnn/01/cnn_0181_0 107 100 100\n",
      "484 bn/cnn/01/cnn_0182_0 97 76 76\n",
      "485 bn/cnn/01/cnn_0183_0 92 79 79\n",
      "486 bn/cnn/01/cnn_0184_0 127 92 92\n",
      "487 bn/cnn/01/cnn_0185_0 290 285 285\n",
      "488 bn/cnn/01/cnn_0186_0 95 92 92\n",
      "489 bn/cnn/01/cnn_0187_0 305 300 300\n",
      "490 bn/cnn/01/cnn_0188_0 87 75 75\n",
      "491 bn/cnn/01/cnn_0191_0 119 116 116\n",
      "492 bn/cnn/01/cnn_0192_0 125 106 106\n",
      "493 bn/cnn/01/cnn_0193_0 108 106 106\n",
      "494 bn/cnn/01/cnn_0194_0 58 42 42\n",
      "495 bn/cnn/01/cnn_0195_0 112 101 101\n",
      "496 bn/cnn/01/cnn_0196_0 137 120 120\n",
      "497 bn/cnn/01/cnn_0197_0 88 83 83\n",
      "498 bn/cnn/01/cnn_0198_0 453 396 396\n",
      "499 bn/cnn/02/cnn_0201_0 443 421 421\n",
      "500 bn/cnn/02/cnn_0202_0 61 54 54\n",
      "501 bn/cnn/02/cnn_0203_0 60 45 45\n",
      "502 bn/cnn/02/cnn_0204_0 59 32 32\n",
      "503 bn/cnn/02/cnn_0205_0 114 99 99\n",
      "504 bn/cnn/02/cnn_0206_0 106 104 104\n",
      "505 bn/cnn/02/cnn_0207_0 128 116 116\n",
      "506 bn/cnn/02/cnn_0208_0 409 403 403\n",
      "507 bn/cnn/02/cnn_0211_0 130 126 126\n",
      "508 bn/cnn/02/cnn_0212_0 65 55 55\n",
      "509 bn/cnn/02/cnn_0213_0 99 68 68\n",
      "510 bn/cnn/02/cnn_0214_0 66 64 64\n",
      "511 bn/cnn/02/cnn_0215_0 58 43 43\n",
      "512 bn/cnn/02/cnn_0216_0 116 109 109\n",
      "513 bn/cnn/02/cnn_0217_0 121 113 113\n",
      "514 bn/cnn/02/cnn_0218_0 58 37 37\n",
      "515 bn/cnn/02/cnn_0221_0 127 115 115\n",
      "516 bn/cnn/02/cnn_0222_0 110 74 74\n",
      "517 bn/cnn/02/cnn_0223_0 126 98 98\n",
      "518 bn/cnn/02/cnn_0224_0 65 37 37\n",
      "519 bn/cnn/02/cnn_0225_0 70 55 55\n",
      "520 bn/cnn/02/cnn_0226_0 496 492 492\n",
      "521 bn/cnn/02/cnn_0227_0 178 173 173\n",
      "522 bn/cnn/02/cnn_0228_0 111 101 101\n",
      "523 bn/cnn/02/cnn_0231_0 308 304 304\n",
      "524 bn/cnn/02/cnn_0232_0 192 180 180\n",
      "525 bn/cnn/02/cnn_0233_0 109 107 107\n",
      "526 bn/cnn/02/cnn_0234_0 120 118 118\n",
      "527 bn/cnn/02/cnn_0235_0 60 58 58\n",
      "528 bn/cnn/02/cnn_0236_0 59 56 56\n",
      "529 bn/cnn/02/cnn_0237_0 244 241 241\n",
      "530 bn/cnn/02/cnn_0238_0 95 81 81\n",
      "531 bn/cnn/02/cnn_0241_0 338 334 334\n",
      "532 bn/cnn/02/cnn_0242_0 343 341 341\n",
      "533 bn/cnn/02/cnn_0243_0 102 100 100\n",
      "534 bn/cnn/02/cnn_0244_0 255 250 250\n",
      "535 bn/cnn/02/cnn_0245_0 301 290 290\n",
      "536 bn/cnn/02/cnn_0246_0 247 246 246\n",
      "537 bn/cnn/02/cnn_0247_0 342 337 337\n",
      "538 bn/cnn/02/cnn_0248_0 119 109 109\n",
      "539 bn/cnn/02/cnn_0251_0 239 237 237\n",
      "540 bn/cnn/02/cnn_0252_0 260 99 99\n",
      "541 bn/cnn/02/cnn_0253_0 917 496 496\n",
      "542 bn/cnn/02/cnn_0254_0 418 402 402\n",
      "543 bn/cnn/02/cnn_0255_0 414 407 407\n",
      "544 bn/cnn/02/cnn_0256_0 530 473 473\n",
      "545 bn/cnn/02/cnn_0257_0 526 507 507\n",
      "546 bn/cnn/02/cnn_0258_0 172 170 170\n",
      "547 bn/cnn/02/cnn_0261_0 394 386 386\n",
      "548 bn/cnn/02/cnn_0262_0 87 84 84\n",
      "549 bn/cnn/02/cnn_0263_0 244 241 241\n",
      "550 bn/cnn/02/cnn_0264_0 131 118 118\n",
      "551 bn/cnn/02/cnn_0265_0 308 294 294\n",
      "552 bn/cnn/02/cnn_0266_0 283 277 277\n",
      "553 bn/cnn/02/cnn_0267_0 587 500 500\n",
      "554 bn/cnn/02/cnn_0268_0 214 198 198\n",
      "555 bn/cnn/02/cnn_0271_0 70 54 54\n",
      "556 bn/cnn/02/cnn_0272_0 819 469 469\n",
      "557 bn/cnn/02/cnn_0273_0 252 233 233\n",
      "558 bn/cnn/02/cnn_0274_0 463 459 459\n",
      "559 bn/cnn/02/cnn_0275_0 158 156 156\n",
      "560 bn/cnn/02/cnn_0276_0 444 417 417\n",
      "561 bn/cnn/02/cnn_0277_0 209 178 178\n",
      "562 bn/cnn/02/cnn_0278_0 473 457 457\n",
      "563 bn/cnn/02/cnn_0281_0 541 495 495\n",
      "564 bn/cnn/02/cnn_0282_0 67 56 56\n",
      "565 bn/cnn/02/cnn_0283_0 220 213 213\n",
      "566 bn/cnn/02/cnn_0284_0 843 507 507\n",
      "567 bn/cnn/02/cnn_0285_0 925 510 510\n",
      "568 bn/cnn/02/cnn_0286_0 207 196 196\n",
      "569 bn/cnn/02/cnn_0287_0 75 74 74\n",
      "570 bn/cnn/02/cnn_0288_0 732 469 469\n",
      "571 bn/cnn/02/cnn_0291_0 295 290 290\n",
      "572 bn/cnn/02/cnn_0292_0 66 53 53\n",
      "573 bn/cnn/02/cnn_0293_0 228 191 191\n",
      "574 bn/cnn/02/cnn_0294_0 60 12 12\n",
      "575 bn/cnn/02/cnn_0295_0 259 232 232\n",
      "576 bn/cnn/02/cnn_0296_0 65 47 47\n",
      "577 bn/cnn/02/cnn_0297_0 331 329 329\n",
      "578 bn/cnn/02/cnn_0298_0 260 257 257\n",
      "579 bn/cnn/03/cnn_0301_0 251 245 245\n",
      "580 bn/cnn/03/cnn_0302_0 190 174 174\n",
      "581 bn/cnn/03/cnn_0303_0 203 140 140\n",
      "582 bn/cnn/03/cnn_0304_0 378 377 377\n",
      "583 bn/cnn/03/cnn_0305_0 426 394 394\n",
      "584 bn/cnn/03/cnn_0306_0 213 208 208\n",
      "585 bn/cnn/03/cnn_0307_0 444 440 440\n",
      "586 bn/cnn/03/cnn_0308_0 62 60 60\n",
      "587 bn/cnn/03/cnn_0311_0 570 386 386\n",
      "588 bn/cnn/03/cnn_0312_0 66 64 64\n",
      "589 bn/cnn/03/cnn_0313_0 471 469 469\n",
      "590 bn/cnn/03/cnn_0314_0 302 299 299\n",
      "591 bn/cnn/03/cnn_0315_0 185 161 161\n",
      "592 bn/cnn/03/cnn_0316_0 92 86 86\n",
      "593 bn/cnn/03/cnn_0317_0 192 189 189\n",
      "594 bn/cnn/03/cnn_0318_0 68 52 52\n",
      "595 bn/cnn/03/cnn_0321_0 186 179 179\n",
      "596 bn/cnn/03/cnn_0322_0 136 134 134\n",
      "597 bn/cnn/03/cnn_0323_0 307 301 301\n",
      "598 bn/cnn/03/cnn_0324_0 874 491 491\n",
      "599 bn/cnn/03/cnn_0325_0 204 201 201\n",
      "600 bn/cnn/03/cnn_0326_0 352 333 333\n",
      "601 bn/cnn/03/cnn_0327_0 209 205 205\n",
      "602 bn/cnn/03/cnn_0328_0 510 504 504\n",
      "603 bn/cnn/03/cnn_0331_0 280 278 278\n",
      "604 bn/cnn/03/cnn_0332_0 641 487 487\n",
      "605 bn/cnn/03/cnn_0333_0 875 497 497\n",
      "606 bn/cnn/03/cnn_0334_0 555 507 507\n",
      "607 bn/cnn/03/cnn_0335_0 447 443 443\n",
      "608 bn/cnn/03/cnn_0336_0 136 125 125\n",
      "609 bn/cnn/03/cnn_0337_0 340 337 337\n",
      "610 bn/cnn/03/cnn_0338_0 588 492 492\n",
      "611 bn/cnn/03/cnn_0341_0 357 348 348\n",
      "612 bn/cnn/03/cnn_0342_0 194 190 190\n",
      "613 bn/cnn/03/cnn_0343_0 550 499 499\n",
      "614 bn/cnn/03/cnn_0344_0 92 90 90\n",
      "615 bn/cnn/03/cnn_0345_0 108 97 97\n",
      "616 bn/cnn/03/cnn_0346_0 347 345 345\n",
      "617 bn/cnn/03/cnn_0347_0 577 505 505\n",
      "618 bn/cnn/03/cnn_0348_0 372 361 361\n",
      "619 bn/cnn/03/cnn_0351_0 64 41 41\n",
      "620 bn/cnn/03/cnn_0352_0 66 57 57\n",
      "621 bn/cnn/03/cnn_0353_0 197 191 191\n",
      "622 bn/cnn/03/cnn_0354_0 228 218 218\n",
      "623 bn/cnn/03/cnn_0355_0 333 329 329\n",
      "624 bn/cnn/03/cnn_0356_0 184 171 171\n",
      "625 bn/cnn/03/cnn_0357_0 73 36 36\n",
      "626 bn/cnn/03/cnn_0358_0 739 436 436\n",
      "627 bn/cnn/03/cnn_0361_0 222 217 217\n",
      "628 bn/cnn/03/cnn_0362_0 868 508 508\n",
      "629 bn/cnn/03/cnn_0363_0 201 199 199\n",
      "630 bn/cnn/03/cnn_0364_0 303 298 298\n",
      "631 bn/cnn/03/cnn_0365_0 235 223 223\n",
      "632 bn/cnn/03/cnn_0366_0 393 387 387\n",
      "633 bn/cnn/03/cnn_0367_0 57 39 39\n",
      "634 bn/cnn/03/cnn_0368_0 430 427 427\n",
      "635 bn/cnn/03/cnn_0371_0 404 401 401\n",
      "636 bn/cnn/03/cnn_0372_0 189 185 185\n",
      "637 bn/cnn/03/cnn_0373_0 154 142 142\n",
      "638 bn/cnn/03/cnn_0374_0 304 302 302\n",
      "639 bn/cnn/03/cnn_0375_0 756 492 492\n",
      "640 bn/cnn/03/cnn_0376_0 344 341 341\n",
      "641 bn/cnn/03/cnn_0377_0 139 137 137\n",
      "642 bn/cnn/03/cnn_0378_0 59 45 45\n",
      "643 bn/cnn/03/cnn_0381_0 343 338 338\n",
      "644 bn/cnn/03/cnn_0382_0 652 505 505\n",
      "645 bn/cnn/03/cnn_0383_0 606 496 496\n",
      "646 bn/cnn/03/cnn_0384_0 197 190 190\n",
      "647 bn/cnn/03/cnn_0385_0 320 293 293\n",
      "648 bn/cnn/03/cnn_0386_0 477 470 470\n",
      "649 bn/cnn/03/cnn_0387_0 310 305 305\n",
      "650 bn/cnn/03/cnn_0388_0 864 500 500\n",
      "651 bn/cnn/03/cnn_0391_0 71 70 70\n",
      "652 bn/cnn/03/cnn_0392_0 114 113 113\n",
      "653 bn/cnn/03/cnn_0393_0 277 264 264\n",
      "654 bn/cnn/03/cnn_0394_0 228 226 226\n",
      "655 bn/cnn/03/cnn_0395_0 295 280 280\n",
      "656 bn/cnn/03/cnn_0396_0 189 187 187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating conll:  62%|██████▏   | 775/1257 [00:00<00:00, 1020.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657 bn/cnn/03/cnn_0397_0 567 501 501\n",
      "658 bn/cnn/03/cnn_0398_0 98 97 97\n",
      "659 bn/cnn/04/cnn_0401_0 261 243 243\n",
      "660 bn/cnn/04/cnn_0402_0 286 284 284\n",
      "661 bn/cnn/04/cnn_0403_0 70 43 43\n",
      "662 bn/cnn/04/cnn_0404_0 735 507 507\n",
      "663 bn/cnn/04/cnn_0405_0 130 114 114\n",
      "664 bn/cnn/04/cnn_0406_0 283 261 261\n",
      "665 bn/cnn/04/cnn_0407_0 243 238 238\n",
      "666 bn/cnn/04/cnn_0408_0 606 508 508\n",
      "667 bn/cnn/04/cnn_0411_0 1212 469 469\n",
      "668 bn/cnn/04/cnn_0412_0 66 64 64\n",
      "669 bn/cnn/04/cnn_0413_0 60 51 51\n",
      "670 bn/cnn/04/cnn_0414_0 193 191 191\n",
      "671 bn/cnn/04/cnn_0415_0 794 509 509\n",
      "672 bn/cnn/04/cnn_0416_0 244 227 227\n",
      "673 bn/cnn/04/cnn_0417_0 186 175 175\n",
      "674 bn/cnn/04/cnn_0418_0 116 95 95\n",
      "675 bn/cnn/04/cnn_0421_0 63 53 53\n",
      "676 bn/cnn/04/cnn_0422_0 318 303 303\n",
      "677 bn/cnn/04/cnn_0423_0 473 465 465\n",
      "678 bn/cnn/04/cnn_0424_0 846 464 464\n",
      "679 bn/cnn/04/cnn_0425_0 250 247 247\n",
      "680 bn/cnn/04/cnn_0426_0 449 418 418\n",
      "681 bn/cnn/04/cnn_0427_0 293 280 280\n",
      "682 bn/cnn/04/cnn_0428_0 258 256 256\n",
      "683 bn/cnn/04/cnn_0431_0 691 501 501\n",
      "684 bn/cnn/04/cnn_0432_0 473 466 466\n",
      "685 bn/cnn/04/cnn_0433_0 409 403 403\n",
      "686 bn/cnn/04/cnn_0434_0 128 127 127\n",
      "687 bn/cnn/04/cnn_0435_0 89 76 76\n",
      "688 bn/cnn/04/cnn_0436_0 67 51 51\n",
      "689 bn/cnn/04/cnn_0437_0 63 40 40\n",
      "690 bn/mnb/00/mnb_0001_0 112 102 102\n",
      "691 bn/mnb/00/mnb_0002_0 165 148 148\n",
      "692 bn/mnb/00/mnb_0003_0 484 482 482\n",
      "693 bn/mnb/00/mnb_0004_0 353 349 349\n",
      "694 bn/mnb/00/mnb_0005_0 363 361 361\n",
      "695 bn/mnb/00/mnb_0006_0 237 235 235\n",
      "696 bn/mnb/00/mnb_0007_0 364 362 362\n",
      "697 bn/mnb/00/mnb_0008_0 909 506 506\n",
      "698 bn/mnb/00/mnb_0011_0 1016 414 414\n",
      "699 bn/mnb/00/mnb_0012_0 420 418 418\n",
      "700 bn/mnb/00/mnb_0013_0 621 505 505\n",
      "701 bn/mnb/00/mnb_0014_0 287 269 269\n",
      "702 bn/mnb/00/mnb_0015_0 492 490 490\n",
      "703 bn/mnb/00/mnb_0016_0 525 492 492\n",
      "704 bn/mnb/00/mnb_0017_0 372 362 362\n",
      "705 bn/mnb/00/mnb_0018_0 574 507 507\n",
      "706 bn/mnb/00/mnb_0021_0 286 282 282\n",
      "707 bn/mnb/00/mnb_0022_0 457 455 455\n",
      "708 bn/mnb/00/mnb_0023_0 685 502 502\n",
      "709 bn/mnb/00/mnb_0024_0 113 86 86\n",
      "710 bn/mnb/00/mnb_0025_0 519 504 504\n",
      "711 bn/nbc/00/nbc_0001_0 578 501 501\n",
      "712 bn/nbc/00/nbc_0002_0 110 100 100\n",
      "713 bn/nbc/00/nbc_0003_0 331 329 329\n",
      "714 bn/nbc/00/nbc_0004_0 601 511 511\n",
      "715 bn/nbc/00/nbc_0005_0 484 482 482\n",
      "716 bn/nbc/00/nbc_0006_0 423 419 419\n",
      "717 bn/nbc/00/nbc_0007_0 369 363 363\n",
      "718 bn/nbc/00/nbc_0008_0 492 490 490\n",
      "719 bn/nbc/00/nbc_0011_0 306 301 301\n",
      "720 bn/nbc/00/nbc_0012_0 743 497 497\n",
      "721 bn/nbc/00/nbc_0013_0 210 208 208\n",
      "722 bn/nbc/00/nbc_0014_0 369 367 367\n",
      "723 bn/nbc/00/nbc_0015_0 404 402 402\n",
      "724 bn/nbc/00/nbc_0016_0 813 485 485\n",
      "725 bn/nbc/00/nbc_0017_0 99 88 88\n",
      "726 bn/nbc/00/nbc_0018_0 424 422 422\n",
      "727 bn/nbc/00/nbc_0021_0 46 44 44\n",
      "728 bn/nbc/00/nbc_0022_0 53 49 49\n",
      "729 bn/nbc/00/nbc_0023_0 86 82 82\n",
      "730 bn/nbc/00/nbc_0024_0 503 501 501\n",
      "731 bn/nbc/00/nbc_0025_0 294 292 292\n",
      "732 bn/nbc/00/nbc_0026_0 446 444 444\n",
      "733 bn/nbc/00/nbc_0027_0 390 388 388\n",
      "734 bn/nbc/00/nbc_0028_0 404 400 400\n",
      "735 bn/nbc/00/nbc_0031_0 646 433 433\n",
      "736 bn/nbc/00/nbc_0032_0 91 73 73\n",
      "737 bn/nbc/00/nbc_0033_0 503 501 501\n",
      "738 bn/nbc/00/nbc_0034_0 598 455 455\n",
      "739 bn/nbc/00/nbc_0035_0 92 90 90\n",
      "740 bn/nbc/00/nbc_0036_0 77 74 74\n",
      "741 bn/nbc/00/nbc_0037_0 83 81 81\n",
      "743 bn/pri/00/pri_0001_0 210 208 208\n",
      "744 bn/pri/00/pri_0002_0 168 166 166\n",
      "745 bn/pri/00/pri_0003_0 178 171 171\n",
      "746 bn/pri/00/pri_0004_0 174 165 165\n",
      "747 bn/pri/00/pri_0005_0 1048 478 478\n",
      "748 bn/pri/00/pri_0006_0 648 502 502\n",
      "749 bn/pri/00/pri_0007_0 263 243 243\n",
      "750 bn/pri/00/pri_0008_0 95 88 88\n",
      "751 bn/pri/00/pri_0011_0 189 187 187\n",
      "752 bn/pri/00/pri_0012_0 191 189 189\n",
      "753 bn/pri/00/pri_0013_0 181 174 174\n",
      "754 bn/pri/00/pri_0014_0 228 226 226\n",
      "755 bn/pri/00/pri_0015_0 222 218 218\n",
      "756 bn/pri/00/pri_0016_0 1054 476 476\n",
      "757 bn/pri/00/pri_0017_0 315 313 313\n",
      "758 bn/pri/00/pri_0018_0 217 216 216\n",
      "759 bn/pri/00/pri_0021_0 732 497 497\n",
      "760 bn/pri/00/pri_0022_0 173 172 172\n",
      "761 bn/pri/00/pri_0023_0 859 511 511\n",
      "762 bn/pri/00/pri_0024_0 66 57 57\n",
      "763 bn/pri/00/pri_0025_0 307 292 292\n",
      "764 bn/pri/00/pri_0026_0 1183 455 455\n",
      "765 bn/pri/00/pri_0027_0 889 459 459\n",
      "766 bn/pri/00/pri_0028_0 672 479 479\n",
      "767 bn/pri/00/pri_0031_0 201 199 199\n",
      "768 bn/pri/00/pri_0032_0 247 245 245\n",
      "769 bn/pri/00/pri_0033_0 68 65 65\n",
      "770 bn/pri/00/pri_0034_0 243 239 239\n",
      "771 bn/pri/00/pri_0035_0 982 482 482\n",
      "772 bn/pri/00/pri_0036_0 115 108 108\n",
      "773 bn/pri/00/pri_0037_0 497 492 492\n",
      "774 bn/pri/00/pri_0038_0 978 511 511\n",
      "775 bn/pri/00/pri_0041_0 164 157 157\n",
      "776 bn/pri/00/pri_0042_0 140 88 88\n",
      "777 bn/pri/00/pri_0043_0 505 500 500\n",
      "778 bn/pri/00/pri_0044_0 167 153 153\n",
      "779 bn/pri/00/pri_0045_0 662 506 506\n",
      "780 bn/pri/00/pri_0046_0 221 219 219\n",
      "781 bn/pri/00/pri_0047_0 197 195 195\n",
      "782 bn/pri/00/pri_0048_0 1044 386 386\n",
      "783 bn/pri/00/pri_0051_0 185 181 181\n",
      "784 bn/pri/00/pri_0052_0 201 197 197\n",
      "785 bn/pri/00/pri_0053_0 842 488 488\n",
      "786 bn/pri/00/pri_0054_0 81 72 72\n",
      "787 bn/pri/00/pri_0055_0 136 132 132\n",
      "788 bn/pri/00/pri_0056_0 56 34 34\n",
      "789 bn/pri/00/pri_0057_0 109 94 94\n",
      "790 bn/pri/00/pri_0058_0 221 219 219\n",
      "791 bn/pri/00/pri_0061_0 80 67 67\n",
      "792 bn/pri/00/pri_0062_0 245 244 244\n",
      "793 bn/pri/00/pri_0063_0 80 78 78\n",
      "794 bn/pri/00/pri_0064_0 824 483 483\n",
      "795 bn/pri/00/pri_0065_0 189 182 182\n",
      "796 bn/pri/00/pri_0066_0 589 509 509\n",
      "797 bn/pri/00/pri_0067_0 111 88 88\n",
      "798 bn/pri/00/pri_0068_0 74 68 68\n",
      "799 bn/pri/00/pri_0071_0 860 439 439\n",
      "800 bn/pri/00/pri_0072_0 179 172 172\n",
      "801 bn/pri/00/pri_0073_0 546 482 482\n",
      "802 bn/pri/00/pri_0074_0 835 483 483\n",
      "803 bn/pri/00/pri_0075_0 267 246 246\n",
      "804 bn/pri/00/pri_0076_0 932 453 453\n",
      "805 bn/pri/00/pri_0077_0 314 312 312\n",
      "806 bn/pri/00/pri_0078_0 906 450 450\n",
      "807 bn/pri/00/pri_0081_0 337 335 335\n",
      "808 bn/pri/00/pri_0082_0 69 67 67\n",
      "809 bn/pri/00/pri_0083_0 790 487 487\n",
      "810 bn/pri/00/pri_0084_0 840 507 507\n",
      "811 bn/pri/00/pri_0085_0 194 187 187\n",
      "812 bn/pri/00/pri_0086_0 639 481 481\n",
      "813 bn/pri/00/pri_0087_0 82 64 64\n",
      "814 bn/pri/00/pri_0088_0 201 178 178\n",
      "815 bn/pri/00/pri_0091_0 73 46 46\n",
      "816 bn/pri/00/pri_0092_0 65 48 48\n",
      "817 bn/pri/00/pri_0093_0 708 484 484\n",
      "818 bn/pri/00/pri_0094_0 996 412 412\n",
      "819 bn/pri/00/pri_0095_0 83 53 53\n",
      "820 bn/pri/00/pri_0096_0 1062 507 507\n",
      "822 bn/pri/00/pri_0098_0 147 145 145\n",
      "823 bn/pri/01/pri_0101_0 213 207 207\n",
      "824 bn/pri/01/pri_0102_0 221 217 217\n",
      "825 bn/pri/01/pri_0103_0 783 495 495\n",
      "826 bn/pri/01/pri_0104_0 188 187 187\n",
      "827 bn/pri/01/pri_0105_0 918 424 424\n",
      "828 bn/pri/01/pri_0106_0 948 511 511\n",
      "829 bn/pri/01/pri_0107_0 166 164 164\n",
      "830 bn/pri/01/pri_0108_0 327 325 325\n",
      "831 bn/pri/01/pri_0111_0 205 203 203\n",
      "832 bn/pri/01/pri_0112_0 555 480 480\n",
      "833 bn/voa/00/voa_0001_0 107 79 79\n",
      "834 bn/voa/00/voa_0002_0 41 20 20\n",
      "835 bn/voa/00/voa_0003_0 128 126 126\n",
      "836 bn/voa/00/voa_0004_0 31 26 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating conll:  85%|████████▍ | 1068/1257 [00:00<00:00, 1245.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837 bn/voa/00/voa_0005_0 235 231 231\n",
      "839 bn/voa/00/voa_0007_0 201 196 196\n",
      "840 bn/voa/00/voa_0008_0 76 66 66\n",
      "841 bn/voa/00/voa_0011_0 114 100 100\n",
      "842 bn/voa/00/voa_0012_0 402 385 385\n",
      "843 bn/voa/00/voa_0013_0 103 68 68\n",
      "844 bn/voa/00/voa_0014_0 414 409 409\n",
      "845 bn/voa/00/voa_0015_0 133 131 131\n",
      "846 bn/voa/00/voa_0016_0 56 50 50\n",
      "848 bn/voa/00/voa_0018_0 54 52 52\n",
      "849 bn/voa/00/voa_0021_0 814 492 492\n",
      "850 bn/voa/00/voa_0022_0 296 294 294\n",
      "851 bn/voa/00/voa_0023_0 314 312 312\n",
      "852 bn/voa/00/voa_0024_0 458 456 456\n",
      "853 bn/voa/00/voa_0025_0 53 44 44\n",
      "854 bn/voa/00/voa_0026_0 117 106 106\n",
      "855 bn/voa/00/voa_0027_0 160 147 147\n",
      "856 bn/voa/00/voa_0028_0 231 229 229\n",
      "857 bn/voa/00/voa_0031_0 93 91 91\n",
      "858 bn/voa/00/voa_0032_0 311 309 309\n",
      "859 bn/voa/00/voa_0033_0 215 194 194\n",
      "860 bn/voa/00/voa_0034_0 69 32 32\n",
      "861 bn/voa/00/voa_0035_0 207 205 205\n",
      "862 bn/voa/00/voa_0036_0 92 87 87\n",
      "863 bn/voa/00/voa_0037_0 159 150 150\n",
      "864 bn/voa/00/voa_0038_0 348 346 346\n",
      "865 bn/voa/00/voa_0041_0 244 242 242\n",
      "866 bn/voa/00/voa_0042_0 131 102 102\n",
      "868 bn/voa/00/voa_0044_0 63 61 61\n",
      "870 bn/voa/00/voa_0046_0 98 89 89\n",
      "871 bn/voa/00/voa_0047_0 79 68 68\n",
      "872 bn/voa/00/voa_0048_0 308 292 292\n",
      "873 bn/voa/00/voa_0051_0 90 76 76\n",
      "874 bn/voa/00/voa_0052_0 203 201 201\n",
      "875 bn/voa/00/voa_0053_0 104 102 102\n",
      "876 bn/voa/00/voa_0054_0 163 143 143\n",
      "877 bn/voa/00/voa_0055_0 73 66 66\n",
      "878 bn/voa/00/voa_0056_0 209 200 200\n",
      "879 bn/voa/00/voa_0057_0 61 59 59\n",
      "880 bn/voa/00/voa_0058_0 108 106 106\n",
      "881 bn/voa/00/voa_0061_0 774 474 474\n",
      "882 bn/voa/00/voa_0062_0 103 75 75\n",
      "883 bn/voa/00/voa_0063_0 220 218 218\n",
      "884 bn/voa/00/voa_0064_0 112 110 110\n",
      "886 bn/voa/00/voa_0066_0 606 500 500\n",
      "887 bn/voa/00/voa_0067_0 77 68 68\n",
      "888 bn/voa/00/voa_0068_0 80 73 73\n",
      "889 bn/voa/00/voa_0071_0 404 403 403\n",
      "890 bn/voa/00/voa_0072_0 323 322 322\n",
      "891 bn/voa/00/voa_0073_0 259 240 240\n",
      "892 bn/voa/00/voa_0074_0 427 422 422\n",
      "893 bn/voa/00/voa_0075_0 509 508 508\n",
      "894 bn/voa/00/voa_0076_0 155 152 152\n",
      "895 bn/voa/00/voa_0077_0 77 75 75\n",
      "896 bn/voa/00/voa_0078_0 144 125 125\n",
      "897 bn/voa/00/voa_0081_0 620 497 497\n",
      "898 bn/voa/00/voa_0082_0 344 342 342\n",
      "899 bn/voa/00/voa_0083_0 96 77 77\n",
      "900 bn/voa/00/voa_0084_0 117 97 97\n",
      "901 bn/voa/00/voa_0085_0 214 212 212\n",
      "902 bn/voa/00/voa_0086_0 89 50 50\n",
      "903 bn/voa/00/voa_0087_0 85 71 71\n",
      "904 bn/voa/00/voa_0088_0 171 168 168\n",
      "905 bn/voa/00/voa_0091_0 437 435 435\n",
      "906 bn/voa/00/voa_0092_0 122 112 112\n",
      "907 bn/voa/00/voa_0093_0 79 50 50\n",
      "908 bn/voa/00/voa_0094_0 237 236 236\n",
      "909 bn/voa/00/voa_0095_0 134 123 123\n",
      "910 bn/voa/00/voa_0096_0 328 324 324\n",
      "911 bn/voa/00/voa_0097_0 169 161 161\n",
      "912 bn/voa/00/voa_0098_0 90 79 79\n",
      "913 bn/voa/01/voa_0101_0 125 115 115\n",
      "914 bn/voa/01/voa_0102_0 240 238 238\n",
      "915 bn/voa/01/voa_0103_0 47 36 36\n",
      "916 bn/voa/01/voa_0104_0 114 93 93\n",
      "917 bn/voa/01/voa_0105_0 73 71 71\n",
      "918 bn/voa/01/voa_0106_0 71 68 68\n",
      "919 bn/voa/01/voa_0107_0 678 445 445\n",
      "920 bn/voa/01/voa_0108_0 74 61 61\n",
      "921 bn/voa/01/voa_0111_0 81 73 73\n",
      "922 bn/voa/01/voa_0112_0 89 69 69\n",
      "923 bn/voa/01/voa_0113_0 414 412 412\n",
      "924 bn/voa/01/voa_0114_0 422 420 420\n",
      "925 bn/voa/01/voa_0115_0 467 465 465\n",
      "926 bn/voa/01/voa_0116_0 230 228 228\n",
      "927 bn/voa/01/voa_0117_0 337 329 329\n",
      "928 bn/voa/01/voa_0118_0 110 108 108\n",
      "929 bn/voa/01/voa_0121_0 79 74 74\n",
      "930 bn/voa/01/voa_0122_0 218 216 216\n",
      "931 bn/voa/01/voa_0123_0 52 37 37\n",
      "932 bn/voa/01/voa_0124_0 52 46 46\n",
      "933 bn/voa/01/voa_0125_0 36 24 24\n",
      "934 bn/voa/01/voa_0126_0 97 95 95\n",
      "935 bn/voa/01/voa_0127_0 103 101 101\n",
      "936 bn/voa/01/voa_0128_0 83 75 75\n",
      "937 bn/voa/01/voa_0131_0 762 503 503\n",
      "938 bn/voa/01/voa_0132_0 837 459 459\n",
      "939 bn/voa/01/voa_0133_0 109 98 98\n",
      "940 bn/voa/01/voa_0134_0 90 80 80\n",
      "941 bn/voa/01/voa_0135_0 93 60 60\n",
      "942 bn/voa/01/voa_0136_0 245 243 243\n",
      "943 bn/voa/01/voa_0137_0 123 105 105\n",
      "944 bn/voa/01/voa_0138_0 112 110 110\n",
      "945 bn/voa/01/voa_0141_0 101 83 83\n",
      "946 bn/voa/01/voa_0142_0 232 221 221\n",
      "947 bn/voa/01/voa_0143_0 274 267 267\n",
      "948 bn/voa/01/voa_0144_0 68 66 66\n",
      "950 bn/voa/01/voa_0146_0 378 376 376\n",
      "951 bn/voa/01/voa_0147_0 188 177 177\n",
      "952 bn/voa/01/voa_0148_0 298 296 296\n",
      "953 bn/voa/01/voa_0151_0 93 67 67\n",
      "954 bn/voa/01/voa_0152_0 125 97 97\n",
      "955 bn/voa/01/voa_0153_0 133 112 112\n",
      "956 bn/voa/01/voa_0154_0 342 340 340\n",
      "957 bn/voa/01/voa_0155_0 74 62 62\n",
      "958 bn/voa/01/voa_0156_0 81 76 76\n",
      "959 bn/voa/01/voa_0157_0 47 18 18\n",
      "961 bn/voa/01/voa_0161_0 54 45 45\n",
      "962 bn/voa/01/voa_0162_0 44 29 29\n",
      "963 bn/voa/01/voa_0163_0 592 496 496\n",
      "964 bn/voa/01/voa_0164_0 86 70 70\n",
      "965 bn/voa/01/voa_0165_0 183 175 175\n",
      "966 bn/voa/01/voa_0166_0 45 19 19\n",
      "967 bn/voa/01/voa_0167_0 211 163 163\n",
      "968 bn/voa/01/voa_0168_0 85 64 64\n",
      "969 bn/voa/01/voa_0171_0 153 112 112\n",
      "970 bn/voa/01/voa_0172_0 44 25 25\n",
      "971 bn/voa/01/voa_0173_0 59 57 57\n",
      "972 bn/voa/01/voa_0174_0 95 91 91\n",
      "973 bn/voa/01/voa_0175_0 83 70 70\n",
      "974 bn/voa/01/voa_0176_0 215 207 207\n",
      "975 bn/voa/01/voa_0177_0 177 167 167\n",
      "976 bn/voa/01/voa_0178_0 197 195 195\n",
      "977 bn/voa/01/voa_0181_0 591 501 501\n",
      "978 bn/voa/01/voa_0182_0 45 28 28\n",
      "979 bn/voa/01/voa_0183_0 254 252 252\n",
      "980 bn/voa/01/voa_0184_0 93 91 91\n",
      "981 bn/voa/01/voa_0185_0 47 28 28\n",
      "982 bn/voa/01/voa_0186_0 49 41 41\n",
      "983 bn/voa/01/voa_0187_0 43 41 41\n",
      "984 bn/voa/01/voa_0188_0 52 50 50\n",
      "985 bn/voa/01/voa_0191_0 276 249 249\n",
      "986 bn/voa/01/voa_0192_0 136 118 118\n",
      "987 bn/voa/01/voa_0193_0 121 119 119\n",
      "988 bn/voa/01/voa_0194_0 123 104 104\n",
      "989 bn/voa/01/voa_0195_0 119 117 117\n",
      "990 bn/voa/01/voa_0196_0 159 146 146\n",
      "991 bn/voa/01/voa_0197_0 115 113 113\n",
      "992 bn/voa/01/voa_0198_0 55 53 53\n",
      "993 bn/voa/02/voa_0201_0 119 46 46\n",
      "994 bn/voa/02/voa_0202_0 70 68 68\n",
      "995 bn/voa/02/voa_0203_0 45 40 40\n",
      "996 bn/voa/02/voa_0204_0 78 76 76\n",
      "997 bn/voa/02/voa_0205_0 98 77 77\n",
      "998 bn/voa/02/voa_0206_0 103 74 74\n",
      "999 bn/voa/02/voa_0207_0 166 159 159\n",
      "1000 bn/voa/02/voa_0208_0 84 58 58\n",
      "1001 bn/voa/02/voa_0211_0 60 25 25\n",
      "1002 bn/voa/02/voa_0212_0 452 430 430\n",
      "1003 bn/voa/02/voa_0213_0 47 43 43\n",
      "1004 bn/voa/02/voa_0214_0 71 69 69\n",
      "1005 bn/voa/02/voa_0215_0 99 88 88\n",
      "1006 bn/voa/02/voa_0216_0 71 69 69\n",
      "1007 bn/voa/02/voa_0217_0 59 57 57\n",
      "1008 bn/voa/02/voa_0218_0 58 50 50\n",
      "1009 bn/voa/02/voa_0221_0 94 73 73\n",
      "1010 bn/voa/02/voa_0222_0 95 90 90\n",
      "1012 bn/voa/02/voa_0224_0 83 79 79\n",
      "1013 bn/voa/02/voa_0225_0 80 70 70\n",
      "1014 bn/voa/02/voa_0226_0 63 54 54\n",
      "1015 bn/voa/02/voa_0227_0 96 74 74\n",
      "1016 bn/voa/02/voa_0228_0 53 39 39\n",
      "1017 bn/voa/02/voa_0231_0 512 510 510\n",
      "1018 bn/voa/02/voa_0232_0 270 268 268\n",
      "1019 bn/voa/02/voa_0233_0 69 67 67\n",
      "1020 bn/voa/02/voa_0234_0 71 58 58\n",
      "1021 bn/voa/02/voa_0235_0 437 435 435\n",
      "1022 bn/voa/02/voa_0236_0 53 46 46\n",
      "1023 bn/voa/02/voa_0237_0 276 274 274\n",
      "1024 bn/voa/02/voa_0238_0 102 95 95\n",
      "1025 bn/voa/02/voa_0241_0 389 387 387\n",
      "1026 bn/voa/02/voa_0242_0 203 201 201\n",
      "1027 bn/voa/02/voa_0243_0 61 53 53\n",
      "1028 bn/voa/02/voa_0244_0 198 189 189\n",
      "1029 bn/voa/02/voa_0245_0 398 396 396\n",
      "1030 bn/voa/02/voa_0246_0 397 395 395\n",
      "1031 bn/voa/02/voa_0247_0 50 39 39\n",
      "1032 bn/voa/02/voa_0248_0 44 7 7\n",
      "1033 bn/voa/02/voa_0251_0 79 76 76\n",
      "1034 bn/voa/02/voa_0252_0 219 217 217\n",
      "1035 bn/voa/02/voa_0253_0 373 364 364\n",
      "1036 bn/voa/02/voa_0254_0 108 106 106\n",
      "1037 bn/voa/02/voa_0255_0 114 108 108\n",
      "1038 bn/voa/02/voa_0256_0 72 67 67\n",
      "1039 bn/voa/02/voa_0257_0 163 161 161\n",
      "1040 bn/voa/02/voa_0258_0 32 24 24\n",
      "1041 bn/voa/02/voa_0261_0 105 103 103\n",
      "1042 bn/voa/02/voa_0262_0 215 213 213\n",
      "1043 bn/voa/02/voa_0263_0 244 242 242\n",
      "1044 bn/voa/02/voa_0264_0 315 313 313\n",
      "1045 bn/voa/02/voa_0265_0 104 96 96\n",
      "1046 mz/sinorama/10/ectb_1001_0 396 394 394\n",
      "1047 mz/sinorama/10/ectb_1001_1 586 511 511\n",
      "1048 mz/sinorama/10/ectb_1001_2 446 440 440\n",
      "1049 mz/sinorama/10/ectb_1001_3 724 473 473\n",
      "1050 mz/sinorama/10/ectb_1001_4 399 397 397\n",
      "1051 mz/sinorama/10/ectb_1001_5 351 349 349\n",
      "1052 mz/sinorama/10/ectb_1001_6 395 393 393\n",
      "1053 mz/sinorama/10/ectb_1001_7 413 411 411\n",
      "1054 mz/sinorama/10/ectb_1001_8 264 262 262\n",
      "1055 mz/sinorama/10/ectb_1001_9 451 448 448\n",
      "1056 mz/sinorama/10/ectb_1001_10 513 511 511\n",
      "1057 mz/sinorama/10/ectb_1001_11 298 271 271\n",
      "1058 mz/sinorama/10/ectb_1002_0 221 219 219\n",
      "1059 mz/sinorama/10/ectb_1002_1 377 375 375\n",
      "1060 mz/sinorama/10/ectb_1002_2 600 493 493\n",
      "1061 mz/sinorama/10/ectb_1003_0 395 195 195\n",
      "1062 mz/sinorama/10/ectb_1003_1 701 418 418\n",
      "1063 mz/sinorama/10/ectb_1003_2 548 451 451\n",
      "1064 mz/sinorama/10/ectb_1003_3 621 469 469\n",
      "1065 mz/sinorama/10/ectb_1003_4 692 472 472\n",
      "1066 mz/sinorama/10/ectb_1003_5 676 492 492\n",
      "1067 mz/sinorama/10/ectb_1003_6 577 474 474\n",
      "1068 mz/sinorama/10/ectb_1003_7 703 483 483\n",
      "1069 mz/sinorama/10/ectb_1003_8 179 173 173\n",
      "1070 mz/sinorama/10/ectb_1003_9 237 235 235\n",
      "1071 mz/sinorama/10/ectb_1003_10 392 390 390\n",
      "1072 mz/sinorama/10/ectb_1004_0 835 425 425\n",
      "1073 mz/sinorama/10/ectb_1004_1 487 465 465\n",
      "1074 mz/sinorama/10/ectb_1004_2 290 276 276\n",
      "1075 mz/sinorama/10/ectb_1005_0 283 267 267\n",
      "1076 mz/sinorama/10/ectb_1005_1 553 442 442\n",
      "1077 mz/sinorama/10/ectb_1005_2 373 369 369\n",
      "1078 mz/sinorama/10/ectb_1005_3 516 413 413\n",
      "1079 mz/sinorama/10/ectb_1005_4 305 298 298\n",
      "1080 mz/sinorama/10/ectb_1005_5 358 335 335\n",
      "1081 mz/sinorama/10/ectb_1006_0 202 193 193\n",
      "1082 mz/sinorama/10/ectb_1006_1 251 249 249\n",
      "1083 mz/sinorama/10/ectb_1006_2 462 416 416\n",
      "1084 mz/sinorama/10/ectb_1006_3 418 409 409\n",
      "1085 mz/sinorama/10/ectb_1006_4 400 379 379\n",
      "1086 mz/sinorama/10/ectb_1006_5 391 383 383\n",
      "1087 mz/sinorama/10/ectb_1006_6 620 487 487\n",
      "1088 mz/sinorama/10/ectb_1006_7 653 502 502\n",
      "1089 mz/sinorama/10/ectb_1006_8 441 435 435\n",
      "1090 mz/sinorama/10/ectb_1006_9 307 296 296\n",
      "1091 mz/sinorama/10/ectb_1006_10 374 367 367\n",
      "1092 mz/sinorama/10/ectb_1006_11 330 295 295\n",
      "1093 mz/sinorama/10/ectb_1006_12 574 500 500\n",
      "1094 mz/sinorama/10/ectb_1007_0 131 117 117\n",
      "1095 mz/sinorama/10/ectb_1007_1 556 433 433\n",
      "1096 mz/sinorama/10/ectb_1007_2 337 316 316\n",
      "1097 mz/sinorama/10/ectb_1007_3 622 444 444\n",
      "1098 mz/sinorama/10/ectb_1007_4 456 453 453\n",
      "1099 mz/sinorama/10/ectb_1007_5 452 450 450\n",
      "1100 mz/sinorama/10/ectb_1007_6 939 475 475\n",
      "1101 mz/sinorama/10/ectb_1007_7 858 503 503\n",
      "1102 mz/sinorama/10/ectb_1007_8 585 473 473\n",
      "1103 mz/sinorama/10/ectb_1008_0 148 97 97\n",
      "1104 mz/sinorama/10/ectb_1008_1 158 132 132\n",
      "1105 mz/sinorama/10/ectb_1008_2 300 265 265\n",
      "1106 mz/sinorama/10/ectb_1008_3 176 156 156\n",
      "1107 mz/sinorama/10/ectb_1008_4 210 178 178\n",
      "1108 mz/sinorama/10/ectb_1008_5 188 131 131\n",
      "1109 mz/sinorama/10/ectb_1008_6 324 180 180\n",
      "1110 mz/sinorama/10/ectb_1008_7 297 118 118\n",
      "1111 mz/sinorama/10/ectb_1011_0 469 465 465\n",
      "1112 mz/sinorama/10/ectb_1011_1 409 406 406\n",
      "1113 mz/sinorama/10/ectb_1011_2 503 496 496\n",
      "1114 mz/sinorama/10/ectb_1011_3 637 344 344\n",
      "1115 mz/sinorama/10/ectb_1011_4 692 506 506\n",
      "1116 mz/sinorama/10/ectb_1011_5 709 506 506\n",
      "1117 mz/sinorama/10/ectb_1011_6 598 499 499\n",
      "1118 mz/sinorama/10/ectb_1011_7 422 412 412\n",
      "1119 mz/sinorama/10/ectb_1011_8 396 383 383\n",
      "1120 mz/sinorama/10/ectb_1012_0 469 457 457\n",
      "1121 mz/sinorama/10/ectb_1012_1 741 440 440\n",
      "1122 mz/sinorama/10/ectb_1012_2 485 444 444\n",
      "1123 mz/sinorama/10/ectb_1012_3 305 289 289\n",
      "1124 mz/sinorama/10/ectb_1012_4 396 378 378\n",
      "1125 mz/sinorama/10/ectb_1012_5 372 343 343\n",
      "1126 mz/sinorama/10/ectb_1012_6 372 343 343\n",
      "1127 mz/sinorama/10/ectb_1012_7 290 287 287\n",
      "1128 mz/sinorama/10/ectb_1012_8 403 381 381\n",
      "1129 mz/sinorama/10/ectb_1013_0 601 495 495\n",
      "1130 mz/sinorama/10/ectb_1013_1 266 238 238\n",
      "1131 mz/sinorama/10/ectb_1013_2 518 510 510\n",
      "1132 mz/sinorama/10/ectb_1013_3 708 349 349\n",
      "1133 mz/sinorama/10/ectb_1013_4 603 473 473\n",
      "1134 mz/sinorama/10/ectb_1013_5 515 488 488\n",
      "1135 mz/sinorama/10/ectb_1013_6 628 404 404\n",
      "1136 mz/sinorama/10/ectb_1013_7 325 299 299\n",
      "1137 mz/sinorama/10/ectb_1013_8 464 462 462\n",
      "1138 mz/sinorama/10/ectb_1013_9 541 470 470\n",
      "1139 mz/sinorama/10/ectb_1014_0 433 426 426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating conll: 100%|██████████| 1257/1257 [00:01<00:00, 1004.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140 mz/sinorama/10/ectb_1014_1 449 430 430\n",
      "1141 mz/sinorama/10/ectb_1014_2 436 432 432\n",
      "1142 mz/sinorama/10/ectb_1014_3 361 359 359\n",
      "1143 mz/sinorama/10/ectb_1014_4 492 462 462\n",
      "1144 mz/sinorama/10/ectb_1014_5 447 444 444\n",
      "1145 mz/sinorama/10/ectb_1014_6 368 357 357\n",
      "1146 mz/sinorama/10/ectb_1014_7 493 476 476\n",
      "1147 mz/sinorama/10/ectb_1014_8 772 337 337\n",
      "1148 mz/sinorama/10/ectb_1014_9 585 433 433\n",
      "1149 mz/sinorama/10/ectb_1015_0 336 334 334\n",
      "1150 mz/sinorama/10/ectb_1015_1 459 446 446\n",
      "1151 mz/sinorama/10/ectb_1015_2 269 257 257\n",
      "1152 mz/sinorama/10/ectb_1016_0 523 481 481\n",
      "1153 mz/sinorama/10/ectb_1016_1 642 473 473\n",
      "1154 mz/sinorama/10/ectb_1016_2 715 373 373\n",
      "1155 mz/sinorama/10/ectb_1016_3 511 501 501\n",
      "1156 mz/sinorama/10/ectb_1016_4 396 289 289\n",
      "1157 mz/sinorama/10/ectb_1016_5 484 461 461\n",
      "1158 mz/sinorama/10/ectb_1016_6 711 491 491\n",
      "1159 mz/sinorama/10/ectb_1016_7 506 503 503\n",
      "1160 mz/sinorama/10/ectb_1016_8 193 177 177\n",
      "1161 mz/sinorama/10/ectb_1017_0 396 394 394\n",
      "1162 mz/sinorama/10/ectb_1017_1 289 269 269\n",
      "1163 mz/sinorama/10/ectb_1017_2 542 489 489\n",
      "1164 mz/sinorama/10/ectb_1017_3 389 372 372\n",
      "1165 mz/sinorama/10/ectb_1017_4 271 260 260\n",
      "1166 mz/sinorama/10/ectb_1017_5 394 370 370\n",
      "1167 mz/sinorama/10/ectb_1017_6 533 501 501\n",
      "1168 mz/sinorama/10/ectb_1017_7 528 474 474\n",
      "1169 mz/sinorama/10/ectb_1017_8 404 402 402\n",
      "1170 mz/sinorama/10/ectb_1017_9 362 357 357\n",
      "1171 mz/sinorama/10/ectb_1018_0 235 231 231\n",
      "1172 mz/sinorama/10/ectb_1018_1 570 475 475\n",
      "1173 mz/sinorama/10/ectb_1018_2 533 462 462\n",
      "1174 mz/sinorama/10/ectb_1018_3 380 368 368\n",
      "1175 mz/sinorama/10/ectb_1018_4 623 496 496\n",
      "1176 mz/sinorama/10/ectb_1018_5 720 509 509\n",
      "1177 mz/sinorama/10/ectb_1018_6 629 422 422\n",
      "1178 mz/sinorama/10/ectb_1018_7 546 501 501\n",
      "1179 mz/sinorama/10/ectb_1018_8 620 503 503\n",
      "1180 mz/sinorama/10/ectb_1018_9 560 449 449\n",
      "1181 mz/sinorama/10/ectb_1021_0 484 469 469\n",
      "1182 mz/sinorama/10/ectb_1021_1 304 298 298\n",
      "1183 mz/sinorama/10/ectb_1021_2 596 505 505\n",
      "1184 mz/sinorama/10/ectb_1021_3 320 306 306\n",
      "1185 mz/sinorama/10/ectb_1021_4 302 277 277\n",
      "1186 mz/sinorama/10/ectb_1021_5 746 502 502\n",
      "1187 mz/sinorama/10/ectb_1021_6 486 473 473\n",
      "1188 mz/sinorama/10/ectb_1021_7 441 436 436\n",
      "1189 mz/sinorama/10/ectb_1021_8 432 430 430\n",
      "1190 mz/sinorama/10/ectb_1021_9 379 377 377\n",
      "1191 mz/sinorama/10/ectb_1021_10 336 333 333\n",
      "1192 mz/sinorama/10/ectb_1022_0 330 315 315\n",
      "1193 mz/sinorama/10/ectb_1022_1 587 497 497\n",
      "1194 mz/sinorama/10/ectb_1022_2 711 500 500\n",
      "1195 mz/sinorama/10/ectb_1022_3 643 494 494\n",
      "1196 mz/sinorama/10/ectb_1022_4 508 499 499\n",
      "1197 mz/sinorama/10/ectb_1022_5 628 500 500\n",
      "1198 mz/sinorama/10/ectb_1023_0 687 493 493\n",
      "1199 mz/sinorama/10/ectb_1023_1 281 278 278\n",
      "1200 mz/sinorama/10/ectb_1023_2 474 472 472\n",
      "1201 mz/sinorama/10/ectb_1024_0 476 471 471\n",
      "1202 mz/sinorama/10/ectb_1024_1 523 506 506\n",
      "1203 mz/sinorama/10/ectb_1024_2 299 234 234\n",
      "1204 mz/sinorama/10/ectb_1025_0 302 300 300\n",
      "1205 mz/sinorama/10/ectb_1025_1 628 305 305\n",
      "1206 mz/sinorama/10/ectb_1025_2 425 411 411\n",
      "1207 mz/sinorama/10/ectb_1025_3 236 234 234\n",
      "1208 mz/sinorama/10/ectb_1025_4 178 176 176\n",
      "1209 mz/sinorama/10/ectb_1026_0 605 435 435\n",
      "1210 mz/sinorama/10/ectb_1026_1 611 505 505\n",
      "1211 mz/sinorama/10/ectb_1026_2 397 335 335\n",
      "1212 mz/sinorama/10/ectb_1026_3 578 477 477\n",
      "1213 mz/sinorama/10/ectb_1026_4 464 459 459\n",
      "1214 mz/sinorama/10/ectb_1026_5 680 454 454\n",
      "1215 mz/sinorama/10/ectb_1026_6 393 383 383\n",
      "1216 mz/sinorama/10/ectb_1026_7 428 410 410\n",
      "1217 mz/sinorama/10/ectb_1026_8 201 182 182\n",
      "1218 mz/sinorama/10/ectb_1027_0 362 360 360\n",
      "1219 mz/sinorama/10/ectb_1027_1 412 404 404\n",
      "1220 mz/sinorama/10/ectb_1027_2 469 461 461\n",
      "1221 mz/sinorama/10/ectb_1028_0 383 378 378\n",
      "1222 mz/sinorama/10/ectb_1028_1 602 470 470\n",
      "1223 mz/sinorama/10/ectb_1028_2 409 393 393\n",
      "1224 mz/sinorama/10/ectb_1028_3 562 486 486\n",
      "1225 mz/sinorama/10/ectb_1028_4 312 157 157\n",
      "1226 mz/sinorama/10/ectb_1028_5 665 472 472\n",
      "1227 mz/sinorama/10/ectb_1028_6 357 335 335\n",
      "1228 mz/sinorama/10/ectb_1028_7 370 361 361\n",
      "1229 mz/sinorama/10/ectb_1028_8 785 489 489\n",
      "1230 mz/sinorama/10/ectb_1028_9 576 401 401\n",
      "1231 mz/sinorama/10/ectb_1031_0 525 491 491\n",
      "1232 mz/sinorama/10/ectb_1031_1 566 507 507\n",
      "1233 mz/sinorama/10/ectb_1031_2 237 235 235\n",
      "1234 mz/sinorama/10/ectb_1031_3 369 354 354\n",
      "1235 mz/sinorama/10/ectb_1031_4 441 435 435\n",
      "1236 mz/sinorama/10/ectb_1031_5 644 482 482\n",
      "1237 mz/sinorama/10/ectb_1031_6 610 509 509\n",
      "1238 mz/sinorama/10/ectb_1031_7 715 466 466\n",
      "1239 mz/sinorama/10/ectb_1031_8 425 393 393\n",
      "1240 mz/sinorama/10/ectb_1032_0 342 339 339\n",
      "1241 mz/sinorama/10/ectb_1032_1 554 506 506\n",
      "1242 mz/sinorama/10/ectb_1032_2 307 297 297\n",
      "1243 mz/sinorama/10/ectb_1033_0 165 126 126\n",
      "1244 mz/sinorama/10/ectb_1033_1 554 424 424\n",
      "1245 mz/sinorama/10/ectb_1033_2 338 306 306\n",
      "1246 mz/sinorama/10/ectb_1033_3 285 281 281\n",
      "1247 mz/sinorama/10/ectb_1034_0 731 291 291\n",
      "1248 mz/sinorama/10/ectb_1034_1 404 402 402\n",
      "1249 mz/sinorama/10/ectb_1034_2 276 274 274\n",
      "1250 mz/sinorama/10/ectb_1035_0 240 238 238\n",
      "1251 mz/sinorama/10/ectb_1035_1 318 260 260\n",
      "1252 mz/sinorama/10/ectb_1035_2 258 229 229\n",
      "1253 mz/sinorama/10/ectb_1035_3 132 97 97\n",
      "1254 mz/sinorama/10/ectb_1035_4 304 235 235\n",
      "1255 mz/sinorama/10/ectb_1035_5 331 317 317\n",
      "1256 mz/sinorama/10/ectb_1035_6 140 138 138\n",
      "1257 mz/sinorama/10/ectb_1035_7 200 171 171\n",
      "1258 mz/sinorama/10/ectb_1035_8 214 182 182\n",
      "1259 mz/sinorama/10/ectb_1035_9 284 248 248\n",
      "1260 mz/sinorama/10/ectb_1035_10 205 186 186\n",
      "1261 mz/sinorama/10/ectb_1035_11 231 154 154\n",
      "1262 mz/sinorama/10/ectb_1035_12 406 400 400\n",
      "1263 mz/sinorama/10/ectb_1035_13 396 382 382\n",
      "1264 mz/sinorama/10/ectb_1036_0 628 493 493\n",
      "1265 mz/sinorama/10/ectb_1036_1 580 511 511\n",
      "1266 mz/sinorama/10/ectb_1036_2 474 422 422\n",
      "1267 mz/sinorama/10/ectb_1036_3 480 476 476\n",
      "1268 mz/sinorama/10/ectb_1036_4 505 503 503\n",
      "1269 mz/sinorama/10/ectb_1036_5 584 503 503\n",
      "1270 mz/sinorama/10/ectb_1036_6 510 506 506\n",
      "1271 mz/sinorama/10/ectb_1036_7 479 458 458\n",
      "1272 mz/sinorama/10/ectb_1036_8 490 485 485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MUC: P = 10000.0, R = 10000.0, F1 = 10000.0\n",
       "B3: P = 10000.0, R = 10000.0, F1 = 10000.0\n",
       "CEAFe: P = 10000.0, R = 10000.0, F1 = 10000.0\n",
       "Average F1: 10000.0\n",
       "Mention: P = 10000.0, R = 10000.0, F1 = 10000.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "evaluate_tensors_official(\n",
    "    (\"/home/sbaruah_usc_edu/mica_text_coref/coref/seq_coref/scorer/\"\n",
    "        \"v8.01/scorer.pl\"), label_ids[:n], prediction_ids[:n],\n",
    "    doc_ids[:n], longformer_seq_train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2,\n",
       "        2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(token_ids[3] != 1).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mica_text_coref.coref.seq_coref.data.CorefDocument object at 0x7f4f438548b0>\n"
     ]
    }
   ],
   "source": [
    "print(longformer_seq_train_corpus.documents[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(42,49), (51,51)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_tensor_to_cluster(prediction_ids[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([38, 39, 40, 41, 42, 43, 44, 45]),)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(doc_ids == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(token_ids[38] == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformer_seq_train_corpus.documents[3].doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc/cctv/00/cctv_0001_3'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longformer_seq_train_corpus.documents[3].doc_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 On\n",
      "1 Ġthe\n",
      "2 Ġafternoon\n",
      "3 Ġof\n",
      "4 ĠAugust\n",
      "5 Ġ22\n",
      "6 Ġ,\n",
      "7 ĠPeng\n",
      "8 ĠDe\n",
      "9 hu\n",
      "10 ai\n",
      "11 Ġwas\n",
      "12 Ġlistening\n",
      "13 Ġto\n",
      "14 Ġthe\n",
      "15 Ġcombat\n",
      "16 Ġoperation\n",
      "17 Ġdirector\n",
      "18 Ġreport\n",
      "19 Ġon\n",
      "20 Ġbattle\n",
      "21 Ġdevelopments\n",
      "22 Ġat\n",
      "23 ĠEighth\n",
      "24 ĠRoute\n",
      "25 ĠArmy\n",
      "26 Ġoperational\n",
      "27 Ġheadquarters\n",
      "28 Ġ.\n",
      "29 ĠWhen\n",
      "30 Ġasked\n",
      "31 Ġabout\n",
      "32 Ġthe\n",
      "33 Ġactual\n",
      "34 Ġcombat\n",
      "35 Ġstrength\n",
      "36 Ġof\n",
      "37 Ġthe\n",
      "38 ĠEighth\n",
      "39 ĠRoute\n",
      "40 ĠArmy\n",
      "41 Ġ,\n",
      "42 Ġthe\n",
      "43 Ġcombat\n",
      "44 Ġoperation\n",
      "45 Ġdirector\n",
      "46 Ġreported\n",
      "47 Ġ:\n",
      "48 ĠThere\n",
      "49 Ġare\n",
      "50 Ġ30\n",
      "51 Ġreg\n",
      "52 iments\n",
      "53 Ġalong\n",
      "54 Ġthe\n",
      "55 ĠZheng\n",
      "56 t\n",
      "57 ai\n",
      "58 ĠLine\n",
      "59 Ġ,\n",
      "60 Ġ15\n",
      "61 Ġreg\n",
      "62 iments\n",
      "63 Ġalong\n",
      "64 Ġthe\n",
      "65 ĠLug\n",
      "66 ou\n",
      "67 ĠBridge\n",
      "68 Ġ-\n",
      "69 ĠHand\n",
      "70 an\n",
      "71 Ġsection\n",
      "72 Ġof\n",
      "73 Ġthe\n",
      "74 ĠPing\n",
      "75 han\n",
      "76 Ġline\n",
      "77 Ġ,\n",
      "78 Ġ12\n",
      "79 Ġreg\n",
      "80 iments\n",
      "81 Ġalong\n",
      "82 Ġthe\n",
      "83 ĠDat\n",
      "84 ong\n",
      "85 Ġ-\n",
      "86 ĠHong\n",
      "87 d\n",
      "88 ong\n",
      "89 Ġsection\n",
      "90 Ġin\n",
      "91 ĠTon\n",
      "92 gh\n",
      "93 u\n",
      "94 ĠCounty\n",
      "95 Ġ,\n",
      "96 Ġand\n",
      "97 Ġfour\n",
      "98 Ġreg\n",
      "99 iments\n",
      "100 Ġalong\n",
      "101 Ġthe\n",
      "102 ĠTian\n",
      "103 jin\n",
      "104 Ġ-\n",
      "105 ĠDe\n",
      "106 zhou\n",
      "107 Ġsection\n",
      "108 Ġof\n",
      "109 Ġthe\n",
      "110 ĠJin\n",
      "111 pu\n",
      "112 ĠLine\n",
      "113 Ġ,\n",
      "114 Ġwith\n",
      "115 Ġa\n",
      "116 Ġtotal\n",
      "117 Ġof\n",
      "118 Ġ105\n",
      "119 Ġreg\n",
      "120 iments\n",
      "121 Ġentering\n",
      "122 Ġthe\n",
      "123 Ġcampaign\n",
      "124 Ġ.\n",
      "125 ĠWith\n",
      "126 Ġa\n",
      "127 Ġwave\n",
      "128 Ġof\n",
      "129 Ġhis\n",
      "130 Ġhand\n",
      "131 Ġ,\n",
      "132 ĠPeng\n",
      "133 ĠDe\n",
      "134 hu\n",
      "135 ai\n",
      "136 Ġsaid\n",
      "137 Ġthat\n",
      "138 Ġdespite\n",
      "139 Ġbeing\n",
      "140 Ġover\n",
      "141 Ġ100\n",
      "142 Ġreg\n",
      "143 iments\n",
      "144 Ġ,\n",
      "145 Ġlet\n",
      "146 Ġ'\n",
      "147 s\n",
      "148 Ġcall\n",
      "149 Ġthis\n",
      "150 Ġcampaign\n",
      "151 Ġthe\n",
      "152 ĠHundred\n",
      "153 ĠReg\n",
      "154 iments\n",
      "155 ĠOffensive\n",
      "156 Ġ.\n",
      "157 ĠThat\n",
      "158 Ġafternoon\n",
      "159 Ġ,\n",
      "160 ĠPeng\n",
      "161 ĠDe\n",
      "162 hu\n",
      "163 ai\n",
      "164 Ġfirst\n",
      "165 Ġused\n",
      "166 Ġthe\n",
      "167 Ġname\n",
      "168 ĠHundred\n",
      "169 ĠReg\n",
      "170 iments\n",
      "171 ĠOffensive\n",
      "172 Ġin\n",
      "173 Ġte\n",
      "174 leg\n",
      "175 rams\n",
      "176 Ġsent\n",
      "177 Ġto\n",
      "178 Ġvarious\n",
      "179 Ġmilitary\n",
      "180 Ġcorps\n",
      "181 Ġand\n",
      "182 Ġthe\n",
      "183 ĠCentral\n",
      "184 ĠMilitary\n",
      "185 ĠCommission\n",
      "186 Ġ.\n",
      "187 ĠA\n",
      "188 Ġtotal\n",
      "189 Ġof\n",
      "190 Ġ105\n",
      "191 Ġreg\n",
      "192 iments\n",
      "193 Ġparticipated\n",
      "194 Ġin\n",
      "195 Ġthe\n",
      "196 ĠHundred\n",
      "197 ĠReg\n",
      "198 iments\n",
      "199 ĠOffensive\n",
      "200 Ġ.\n",
      "201 ĠWell\n",
      "202 Ġ,\n",
      "203 Ġit\n",
      "204 Ġwas\n",
      "205 Ġa\n",
      "206 Ġforce\n",
      "207 Ġof\n",
      "208 Ġmore\n",
      "209 Ġthan\n",
      "210 Ġ200\n",
      "211 ,\n",
      "212 000\n",
      "213 Ġsoldiers\n",
      "214 Ġ.\n",
      "215 ĠIn\n",
      "216 Ġaddition\n",
      "217 Ġto\n",
      "218 Ġthat\n",
      "219 Ġ,\n",
      "220 Ġer\n",
      "221 Ġ,\n",
      "222 Ġno\n",
      "223 Ġless\n",
      "224 Ġthan\n",
      "225 Ġ400\n",
      "226 ,\n",
      "227 000\n",
      "228 Ġmilit\n",
      "229 iam\n",
      "230 en\n",
      "231 Ġ,\n",
      "232 Ġgu\n",
      "233 err\n",
      "234 illas\n",
      "235 Ġ,\n",
      "236 Ġand\n",
      "237 Ġcivilians\n",
      "238 Ġ,\n",
      "239 Ġer\n",
      "240 Ġ,\n",
      "241 Ġvoluntarily\n",
      "242 Ġparticipated\n",
      "243 Ġin\n",
      "244 Ġthe\n",
      "245 Ġoffensive\n",
      "246 Ġ.\n",
      "247 ĠSo\n",
      "248 Ġ,\n",
      "249 Ġthere\n",
      "250 Ġwere\n",
      "251 Ġprobably\n",
      "252 Ġ600\n",
      "253 ,\n",
      "254 000\n",
      "255 Ġ-\n",
      "256 Ġplus\n",
      "257 Ġsoldiers\n",
      "258 Ġand\n",
      "259 Ġcivilians\n",
      "260 Ġ.\n",
      "261 ĠThe\n",
      "262 ĠHundred\n",
      "263 ĠReg\n",
      "264 iments\n",
      "265 ĠOffensive\n",
      "266 Ġdealt\n",
      "267 Ġa\n",
      "268 Ġdevastating\n",
      "269 Ġblow\n",
      "270 Ġto\n",
      "271 Ġthe\n",
      "272 Ġenemy\n",
      "273 Ġat\n",
      "274 ĠShi\n",
      "275 '\n",
      "276 na\n",
      "277 o\n",
      "278 ĠMountain\n",
      "279 Ġ.\n",
      "280 ĠOur\n",
      "281 Ġtroops\n",
      "282 Ġwere\n",
      "283 Ġstrong\n",
      "284 Ġ,\n",
      "285 Ġtroops\n",
      "286 Ġwere\n",
      "287 Ġstrong\n",
      "288 Ġ.\n",
      "289 ĠEr\n",
      "290 Ġ,\n",
      "291 Ġand\n",
      "292 Ġthose\n",
      "293 Ġin\n",
      "294 Ġcharge\n",
      "295 Ġof\n",
      "296 Ġlogistics\n",
      "297 Ġand\n",
      "298 Ġsupport\n",
      "299 Ġwere\n",
      "300 Ġable\n",
      "301 Ġto\n",
      "302 Ġfollow\n",
      "303 Ġin\n",
      "304 Ġtime\n",
      "305 Ġ.\n",
      "306 ĠEr\n",
      "307 Ġ,\n",
      "308 Ġ,\n",
      "309 Ġer\n",
      "310 Ġ,\n",
      "311 Ġthe\n",
      "312 Ġanti\n",
      "313 -\n",
      "314 Japanese\n",
      "315 Ġv\n",
      "316 anguard\n",
      "317 Ġteam\n",
      "318 Ġwas\n",
      "319 Ġcomposed\n",
      "320 Ġof\n",
      "321 Ġall\n",
      "322 Ġyoung\n",
      "323 Ġpeople\n",
      "324 Ġin\n",
      "325 Ġtheir\n",
      "326 Ġ20\n",
      "327 s\n",
      "328 Ġ;\n",
      "329 Ġall\n",
      "330 Ġyoung\n",
      "331 Ġpeople\n",
      "332 Ġ,\n",
      "333 Ġall\n",
      "334 Ġin\n",
      "335 Ġtheir\n",
      "336 Ġ,\n",
      "337 Ġ20\n",
      "338 s\n",
      "339 Ġ.\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sentence in longformer_seq_train_corpus.get_doc_id_to_sentences()[3]:\n",
    "    for token in sentence:\n",
    "        print(i, token)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
