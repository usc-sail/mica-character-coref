{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.seq_coref import data\n",
    "from mica_text_coref.coref.seq_coref import data_util\n",
    "from mica_text_coref.coref.seq_coref import print_document\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import tqdm\n",
    "from transformers import BertTokenizer, RobertaTokenizer, LongformerTokenizer\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = data.CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                               \"conll-2012/gold/test.english.jsonlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "longformer_tokenizer = LongformerTokenizer.from_pretrained(\n",
    "    \"allenai/longformer-base-4096\")\n",
    "tokenizers = [bert_tokenizer, roberta_tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens\n",
      "[['Of', 'all', 'the', 'ethnic', 'tensions', 'in', 'America', ',', 'which', 'is', 'the', 'most', 'troublesome', 'right', 'now', '?'], ['A', 'good', 'bet', 'would', 'be', 'the', 'tension', 'between', 'blacks', 'and', 'Jews', 'in', 'New', 'York', 'City', '.']]\n",
      "Concatenated Sentence\n",
      "['Of all the ethnic tensions in America , which is the most troublesome right now ?', 'A good bet would be the tension between blacks and Jews in New York City .']\n",
      "Concatenated Text\n",
      "Of all the ethnic tensions in America , which is the most troublesome right now ? A good bet would be the tension between blacks and Jews in New York City .\n",
      "\n",
      "bert-base-cased(Concatenated Sentence)\n",
      "[['[CLS]', 'Of', 'all', 'the', 'ethnic', 'tensions', 'in', 'America', ',', 'which', 'is', 'the', 'most', 'troubles', '##ome', 'right', 'now', '?', '[SEP]'], ['[CLS]', 'A', 'good', 'bet', 'would', 'be', 'the', 'tension', 'between', 'blacks', 'and', 'Jews', 'in', 'New', 'York', 'City', '.', '[SEP]']]\n",
      "[[101, 2096, 1155, 1103, 5237, 14696, 1107, 1738, 117, 1134, 1110, 1103, 1211, 16115, 6758, 1268, 1208, 136, 102], [101, 138, 1363, 7023, 1156, 1129, 1103, 6646, 1206, 14892, 1105, 4384, 1107, 1203, 1365, 1392, 119, 102]]\n",
      "bert-base-cased(Concatenated Text)\n",
      "['[CLS]', 'Of', 'all', 'the', 'ethnic', 'tensions', 'in', 'America', ',', 'which', 'is', 'the', 'most', 'troubles', '##ome', 'right', 'now', '?', 'A', 'good', 'bet', 'would', 'be', 'the', 'tension', 'between', 'blacks', 'and', 'Jews', 'in', 'New', 'York', 'City', '.', '[SEP]']\n",
      "[101, 2096, 1155, 1103, 5237, 14696, 1107, 1738, 117, 1134, 1110, 1103, 1211, 16115, 6758, 1268, 1208, 136, 138, 1363, 7023, 1156, 1129, 1103, 6646, 1206, 14892, 1105, 4384, 1107, 1203, 1365, 1392, 119, 102]\n",
      "\n",
      "roberta-base(Concatenated Sentence)\n",
      "[['<s>', 'Of', 'Ġall', 'Ġthe', 'Ġethnic', 'Ġtensions', 'Ġin', 'ĠAmerica', 'Ġ,', 'Ġwhich', 'Ġis', 'Ġthe', 'Ġmost', 'Ġtroublesome', 'Ġright', 'Ġnow', 'Ġ?', '</s>'], ['<s>', 'A', 'Ġgood', 'Ġbet', 'Ġwould', 'Ġbe', 'Ġthe', 'Ġtension', 'Ġbetween', 'Ġblacks', 'Ġand', 'ĠJews', 'Ġin', 'ĠNew', 'ĠYork', 'ĠCity', 'Ġ.', '</s>']]\n",
      "[[0, 10643, 70, 5, 7289, 5734, 11, 730, 2156, 61, 16, 5, 144, 34056, 235, 122, 17487, 2], [0, 250, 205, 5673, 74, 28, 5, 8556, 227, 20465, 8, 8609, 11, 188, 469, 412, 479, 2]]\n",
      "roberta-base(Concatenated Text)\n",
      "['<s>', 'Of', 'Ġall', 'Ġthe', 'Ġethnic', 'Ġtensions', 'Ġin', 'ĠAmerica', 'Ġ,', 'Ġwhich', 'Ġis', 'Ġthe', 'Ġmost', 'Ġtroublesome', 'Ġright', 'Ġnow', 'Ġ?', 'ĠA', 'Ġgood', 'Ġbet', 'Ġwould', 'Ġbe', 'Ġthe', 'Ġtension', 'Ġbetween', 'Ġblacks', 'Ġand', 'ĠJews', 'Ġin', 'ĠNew', 'ĠYork', 'ĠCity', 'Ġ.', '</s>']\n",
      "[0, 10643, 70, 5, 7289, 5734, 11, 730, 2156, 61, 16, 5, 144, 34056, 235, 122, 17487, 83, 205, 5673, 74, 28, 5, 8556, 227, 20465, 8, 8609, 11, 188, 469, 412, 479, 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document in random.sample(test_corpus.documents, 1):\n",
    "    sentences = document.sentences[:2]\n",
    "    concatenated_sentences = [\" \".join(sentence) for sentence in sentences]\n",
    "    concatenated_text = \" \".join([token for sentence in sentences\n",
    "                                        for token in sentence])\n",
    "    print(f\"Original Tokens\\n{sentences}\")\n",
    "    print(f\"Concatenated Sentence\\n{concatenated_sentences}\")\n",
    "    print(f\"Concatenated Text\\n{concatenated_text}\\n\")\n",
    "\n",
    "    for tokenizer in tokenizers:\n",
    "        concat_sent_input_ids = tokenizer(concatenated_sentences)[\"input_ids\"]\n",
    "        concat_sent_tokens = [tokenizer.convert_ids_to_tokens(input_ids)\n",
    "                                for input_ids in concat_sent_input_ids]\n",
    "        concat_text_input_ids = tokenizer(concatenated_text)[\"input_ids\"]\n",
    "        concat_text_tokens = tokenizer.convert_ids_to_tokens(\n",
    "            concat_text_input_ids)\n",
    "        tokenizer_name = tokenizer.name_or_path\n",
    "\n",
    "        print(f\"{tokenizer_name}(Concatenated Sentence)\")\n",
    "        print(concat_sent_tokens)\n",
    "        print(concat_sent_input_ids)\n",
    "\n",
    "        print(f\"{tokenizer_name}(Concatenated Text)\")\n",
    "        print(concat_text_tokens)\n",
    "        print(concat_text_input_ids)\n",
    "        print()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_spans_document_level(\n",
    "    corpus: data.CorefCorpus, tokenize_fn: Callable[[str], list[str]]) -> (\n",
    "        data.CorefCorpus):\n",
    "    \"\"\"Apply tokenize function at the document level.\"\"\"\n",
    "    new_corpus = data.CorefCorpus()\n",
    "\n",
    "    for document in tqdm.tqdm(corpus.documents):\n",
    "        new_document = data.CorefDocument()\n",
    "        words = [word for sentence in document.sentences for word in sentence]\n",
    "        text = \" \".join(words)\n",
    "        tokens = tokenize_fn(text)\n",
    "        word_characters = \"\".join(words)\n",
    "        token_characters = \"\".join(tokens)\n",
    "        word_begin_to_word_character = np.zeros(len(words), dtype=int)\n",
    "        word_end_to_word_character = np.zeros(len(words), dtype=int)\n",
    "        word_character_to_token_character = np.zeros(len(word_characters),\n",
    "                                                    dtype=int)\n",
    "        token_character_to_token_index = np.zeros(len(token_characters),\n",
    "                                                dtype=int)\n",
    "        \n",
    "        c = 0\n",
    "        for i, word in enumerate(words):\n",
    "            word_begin_to_word_character[i] = c\n",
    "            word_end_to_word_character[i] = c + len(word) - 1\n",
    "            c += len(word)\n",
    "        \n",
    "        i, j = 0, 0\n",
    "        while i < len(word_characters) and j < len(token_characters):\n",
    "            if word_characters[i] == token_characters[j]:\n",
    "                word_character_to_token_character[i] = j\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        \n",
    "        c = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            token_character_to_token_index[c: c + len(token)] = i\n",
    "            c += len(token)\n",
    "\n",
    "        def map_begin(word_begin: int) -> int:\n",
    "            return token_character_to_token_index[\n",
    "                    word_character_to_token_character[\n",
    "                        word_begin_to_word_character[word_begin]]]\n",
    "\n",
    "        def map_end(word_end: int) -> int:\n",
    "            return token_character_to_token_index[\n",
    "                    word_character_to_token_character[\n",
    "                        word_end_to_word_character[word_end]]]\n",
    "        \n",
    "        for cluster in document.clusters:\n",
    "            new_cluster: set[data.Mention] = set()\n",
    "            for mention in cluster:\n",
    "                new_begin = map_begin(mention.begin)\n",
    "                new_end = map_end(mention.end)\n",
    "                new_mention = data.Mention(new_begin, new_end)\n",
    "                new_cluster.add(new_mention)\n",
    "            new_document.clusters.append(new_cluster)\n",
    "\n",
    "        for mention, ner_tag in document.named_entities.items():\n",
    "            new_begin = map_begin(mention.begin)\n",
    "            new_end = map_end(mention.end)\n",
    "            new_mention = data.Mention(new_begin, new_end)\n",
    "            new_document.named_entities[new_mention] = ner_tag\n",
    "        \n",
    "        for mention, constituency_tag in document.constituents.items():\n",
    "            new_begin = map_begin(mention.begin)\n",
    "            new_end = map_end(mention.end)\n",
    "            new_mention = data.Mention(new_begin, new_end)\n",
    "            new_document.constituents[new_mention] = constituency_tag\n",
    "        \n",
    "        new_sentences = []\n",
    "        new_speakers = []\n",
    "        i, j = 0, 0\n",
    "        for sentence, speakers in zip(document.sentences, document.speakers):\n",
    "            n_words = len(sentence)\n",
    "            end = map_end(i + n_words - 1)\n",
    "            new_sentence = tokens[j: end + 1]\n",
    "            i += n_words\n",
    "            j = end + 1\n",
    "            new_sentences.append(new_sentence)\n",
    "            new_speakers.append([speakers[0] for _ in range(len(new_sentence))])\n",
    "        new_document.sentences = new_sentences\n",
    "        new_document.speakers = new_speakers\n",
    "\n",
    "        new_document.doc_key = document.doc_key\n",
    "        new_corpus.documents.append(new_document)\n",
    "    \n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [00:02<00:00, 155.47it/s]\n"
     ]
    }
   ],
   "source": [
    "roberta_corpus = remap_spans_document_level(test_corpus, \n",
    "                                            roberta_tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(test_corpus.documents) - 1)\n",
    "original_document = test_corpus.documents[i]\n",
    "roberta_document = roberta_corpus.documents[i]\n",
    "\n",
    "original_desc = print_document.pretty_format_coref_document(original_document)\n",
    "with open(\"/home/sbaruah_usc_edu/mica_text_coref/data/temp/original_document.txt\", \"w\") as fw:\n",
    "    fw.write(original_desc)\n",
    "\n",
    "roberta_desc = print_document.pretty_format_coref_document(roberta_document)\n",
    "with open(\"/home/sbaruah_usc_edu/mica_text_coref/data/temp/roberta_document.txt\", \"w\") as fw:\n",
    "    fw.write(roberta_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
