{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.seq_coref import data\n",
    "from mica_text_coref.coref.seq_coref import data_util\n",
    "from mica_text_coref.coref.seq_coref import representatives\n",
    "from mica_text_coref.coref.seq_coref import tensorize\n",
    "from mica_text_coref.coref.seq_coref import print_document\n",
    "\n",
    "from keras_preprocessing import sequence\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data as tdata\n",
    "import tqdm\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from typing import Callable\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorefLongformerModel(nn.Module):\n",
    "    \"\"\"Coreference Resolution Model for English using the Longformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_large: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        model_size = \"large\" if use_large else \"base\"\n",
    "        self.tokenizer: LongformerTokenizer = (\n",
    "            LongformerTokenizer.from_pretrained(\n",
    "                f\"allenai/longformer-{model_size}-4096\"))\n",
    "        self.longformer: LongformerModel = LongformerModel.from_pretrained(\n",
    "            f\"allenai/longformer-{model_size}-4096\")\n",
    "\n",
    "        self.longformer_hidden_size: int = self.longformer.config.hidden_size\n",
    "        self.n_labels = 3\n",
    "        self.label_embedding_size = 10\n",
    "        self.label_embedding = nn.Embedding(self.n_labels,\n",
    "                                            self.label_embedding_size)\n",
    "        self.gru_hidden_size = self.longformer_hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            self.longformer_hidden_size + self.label_embedding_size,\n",
    "            self.gru_hidden_size, bidirectional=True)\n",
    "        self.token_classifier = nn.Linear(self.gru_hidden_size, self.n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = CorefLongformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensors(\n",
    "    corpus: data.CorefCorpus, \n",
    "    representative_mentions: list[list[data.Mention]],\n",
    "    coref_longformer_model: CorefLongformerModel) -> (\n",
    "        tdata.TensorDataset):\n",
    "    \"\"\"Create Tensor Dataset from coreference corpus and representative mentions\n",
    "     for each document's cluster. The number of representative\n",
    "    mentions should equal the number of clusters. Make sure that you remap\n",
    "    spans of the document before passing it here.\n",
    "\n",
    "    Args:\n",
    "        corpus: Coreference corpus.\n",
    "        representative_mentions: List of list of data.Mention objects.\n",
    "        coref_longformer_model: Coreference Longformer Model.\n",
    "    \n",
    "    Returns:\n",
    "        A tensor pytorch dataset. It contains the following tensors: \n",
    "            1. token ids: LongTensor\n",
    "            2. mention ids: IntTensor\n",
    "            3. label ids: IntTensor\n",
    "            4. attention mask: FloatTensor\n",
    "            5. global attention mask: FloatTensor\n",
    "            6. doc ids: IntTensor\n",
    "    \"\"\"\n",
    "    assert len(corpus.documents) == len(representative_mentions), (\n",
    "        \"Number of documents should equal the number of representative mention\"\n",
    "        \" lists\")\n",
    "    for document, mentions in zip(corpus.documents, representative_mentions):\n",
    "        assert len(document.clusters) == len(mentions), f\"Number of clusters\"\n",
    "        \" should equal the number of representative mentions,\"\n",
    "        \" doc key = {document.doc_key}\"\n",
    "\n",
    "    max_sequence_length = 0\n",
    "    for document in corpus.documents:\n",
    "        n_tokens = sum(len(sentence) for sentence in document.sentences)\n",
    "        max_sequence_length = max(n_tokens, max_sequence_length)\n",
    "    max_sequence_length = min(\n",
    "        coref_longformer_model.longformer.config.max_position_embeddings, \n",
    "        max_sequence_length)\n",
    "    tokenizer = coref_longformer_model.tokenizer\n",
    "\n",
    "    token_ids_list: list[list[int]] = []\n",
    "    mention_ids_list: list[list[int]] = []\n",
    "    label_ids_list: list[list[int]] = []\n",
    "    attn_mask_list: list[list[int]] = []\n",
    "    global_attn_mask_list: list[list[int]] = []\n",
    "    doc_ids: list[int] = []\n",
    "\n",
    "    for i, document in enumerate(corpus.documents):\n",
    "        tokens = [token for sentence in document.sentences \n",
    "                        for token in sentence]\n",
    "        token_ids: list[int] = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attn_mask: list[int] = [1 for _ in range(len(tokens))]\n",
    "        doc_id = document.doc_id\n",
    "        \n",
    "        for j, cluster in enumerate(document.clusters):\n",
    "            sorted_cluster = sorted(cluster)\n",
    "            \n",
    "            if sorted_cluster[-1].end < max_sequence_length:\n",
    "                mention = representative_mentions[i][j]\n",
    "                mention_ids = [0 for _ in range(len(tokens))]\n",
    "                mention_ids[mention.begin] = 1\n",
    "                for k in range(mention.begin + 1, mention.end + 1):\n",
    "                    mention_ids[k] = 2\n",
    "                label_ids = [0 for _ in range(len(tokens))]\n",
    "                global_attn_mask = [0 for _ in range(len(tokens))]\n",
    "                for k in range(mention.begin, mention.end + 1):\n",
    "                    global_attn_mask[k] = 1\n",
    "                \n",
    "                for mention in sorted_cluster:\n",
    "                    label_ids[mention.begin] = 1\n",
    "                    for k in range(mention.begin + 1, mention.end + 1):\n",
    "                        label_ids[k] = 2\n",
    "                \n",
    "                token_ids_list.append(token_ids)\n",
    "                mention_ids_list.append(mention_ids)\n",
    "                label_ids_list.append(label_ids)\n",
    "                attn_mask_list.append(attn_mask)\n",
    "                global_attn_mask_list.append(global_attn_mask)\n",
    "                doc_ids.append(doc_id)\n",
    "    \n",
    "    token_ids_pt = torch.LongTensor(sequence.pad_sequences(token_ids_list, \n",
    "        maxlen=max_sequence_length, dtype=int, padding=\"post\", \n",
    "        truncating=\"post\", value=tokenizer.pad_token_id))\n",
    "    mention_ids_pt = torch.IntTensor(sequence.pad_sequences(mention_ids_list,\n",
    "        maxlen=max_sequence_length, dtype=int, padding=\"post\",\n",
    "        truncating=\"post\", value=0))\n",
    "    label_ids_pt = torch.IntTensor(sequence.pad_sequences(label_ids_list,\n",
    "        maxlen=max_sequence_length, dtype=int, padding=\"post\", \n",
    "        truncating=\"post\", value=0))\n",
    "    attn_mask_pt = torch.FloatTensor(sequence.pad_sequences(attn_mask_list, \n",
    "        maxlen=max_sequence_length, dtype=float, padding=\"post\",\n",
    "        truncating=\"post\", value=0.))\n",
    "    global_attn_mask_pt = torch.FloatTensor(sequence.pad_sequences(\n",
    "        global_attn_mask_list, \n",
    "        maxlen=max_sequence_length, dtype=float, padding=\"post\",\n",
    "        truncating=\"post\", value=0.))\n",
    "    doc_ids_pt = torch.IntTensor(doc_ids)\n",
    "\n",
    "    dataset = tdata.TensorDataset(token_ids_pt, mention_ids_pt, label_ids_pt, \n",
    "                                attn_mask_pt, global_attn_mask_pt, doc_ids_pt)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(word_characters: str, token_characters: str, \n",
    "        ignore_token_characters: list[str] = []) -> list[int]:\n",
    "    \"\"\"Find the longest common subsequence between the word_characters and\n",
    "    token_characters strings.\n",
    "    \"\"\"\n",
    "    lcs_len = np.zeros((len(word_characters), len(token_characters)), dtype=int)\n",
    "    lcs_dir = np.zeros((len(word_characters), len(token_characters)), dtype=int)\n",
    "\n",
    "    for i in tqdm.trange(len(word_characters)):\n",
    "        for j in range(len(token_characters)):\n",
    "            equal = word_characters[i] == token_characters[j] and (\n",
    "                token_characters[j] not in ignore_token_characters)\n",
    "            if i == 0 and j == 0:\n",
    "                if equal:\n",
    "                    lcs_len[i, j] = 1\n",
    "                    lcs_dir[i, j] = 1\n",
    "                else:\n",
    "                    lcs_len[i, j] = 0\n",
    "                    lcs_dir[i, j] = 0\n",
    "            elif i == 0:\n",
    "                if equal:\n",
    "                    lcs_len[i, j] = 1\n",
    "                    lcs_dir[i, j] = 1\n",
    "                else:\n",
    "                    lcs_len[i, j] = lcs_len[i, j - 1]\n",
    "                    lcs_dir[i, j] = 2\n",
    "            elif j == 0:\n",
    "                if equal:\n",
    "                    lcs_len[i, j] = 1\n",
    "                    lcs_dir[i, j] = 1\n",
    "                else:\n",
    "                    lcs_len[i, j] = lcs_len[i - 1, j]\n",
    "                    lcs_dir[i, j] = 3\n",
    "            else:\n",
    "                if equal:\n",
    "                    lcs_len[i, j] = 1 + lcs_len[i - 1, j - 1]\n",
    "                    lcs_dir[i, j] = 1\n",
    "                else:\n",
    "                    if lcs_len[i, j - 1] > lcs_len[i - 1, j]:\n",
    "                        lcs_len[i, j] = lcs_len[i, j - 1]\n",
    "                        lcs_dir[i, j] = 2\n",
    "                    else:\n",
    "                        lcs_len[i, j] = lcs_len[i - 1, j]\n",
    "                        lcs_dir[i, j] = 3\n",
    "    \n",
    "    word_character_to_token_character = np.zeros(len(word_characters), \n",
    "                                                dtype=int)\n",
    "    i = len(word_characters) - 1\n",
    "    j = len(token_characters) - 1\n",
    "    while i >= 0 and j >= 0:\n",
    "        if lcs_dir[i, j] == 1:\n",
    "            word_character_to_token_character[i] = j\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif lcs_dir[i, j] == 2:\n",
    "            j -= 1\n",
    "        elif lcs_dir[i, j] == 3:\n",
    "            i -= 1\n",
    "        else:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "    return word_character_to_token_character.tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_mapping(word_characters: str, token_characters: str, \n",
    "        ignore_token_characters: list[str] = []) -> list[int]:\n",
    "    \"\"\"Find the mapping between the word_characters and token_characters strings.\n",
    "    \"\"\"\n",
    "    word_character_to_token_character = np.zeros(len(word_characters),\n",
    "                                                dtype=int)\n",
    "    i, j = 0, 0\n",
    "    while i < len(word_characters) and j < len(token_characters):\n",
    "        equal = word_characters[i] == token_characters[j] and (\n",
    "                token_characters[j] not in ignore_token_characters)\n",
    "        if equal:\n",
    "            word_character_to_token_character[i] = j\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return word_character_to_token_character.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_spans(\n",
    "    document: data.CorefDocument, tokenize_fn: Callable[[str], list[str]], \n",
    "    verbose=False) -> data.CorefDocument:\n",
    "    \"\"\"Apply tokenize function at the document level by concatenating all\n",
    "    words in the documents together. Then adjust the indices of the mentions\n",
    "    in the coreference clusters, and the named entity and constituencies\n",
    "    dictionaries. Prefer this over remap_spans_at_word_level if you are using\n",
    "    transformer-based tokenizers.\n",
    "    \"\"\"\n",
    "    new_document = data.CorefDocument()\n",
    "    words = [word for sentence in document.sentences for word in sentence]\n",
    "    text = \" \".join(words)\n",
    "    tokens = tokenize_fn(text)\n",
    "    word_characters = \"\".join(words)\n",
    "    token_characters = \"\".join(tokens)\n",
    "    word_begin_to_word_character = np.zeros(len(words), dtype=int)\n",
    "    word_end_to_word_character = np.zeros(len(words), dtype=int)\n",
    "    token_character_to_token_index = np.zeros(len(token_characters),\n",
    "                                            dtype=int)\n",
    "\n",
    "    c = 0\n",
    "    for i, word in enumerate(words):\n",
    "        word_begin_to_word_character[i] = c\n",
    "        word_end_to_word_character[i] = c + len(word) - 1\n",
    "        c += len(word)\n",
    "\n",
    "    word_character_to_token_character = naive_mapping(\n",
    "        word_characters, token_characters, ignore_token_characters=[\"Ġ\"])\n",
    "\n",
    "    c = 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_character_to_token_index[c: c + len(token)] = i\n",
    "        c += len(token)\n",
    "\n",
    "    def map_begin(word_begin: int) -> int:\n",
    "        return token_character_to_token_index[\n",
    "                word_character_to_token_character[\n",
    "                    word_begin_to_word_character[word_begin]]]\n",
    "\n",
    "    def map_end(word_end: int) -> int:\n",
    "        return token_character_to_token_index[\n",
    "                word_character_to_token_character[\n",
    "                    word_end_to_word_character[word_end]]]\n",
    "\n",
    "    for cluster in document.clusters:\n",
    "        new_cluster: set[data.Mention] = set()\n",
    "        for mention in cluster:\n",
    "            new_begin = map_begin(mention.begin)\n",
    "            new_end = map_end(mention.end)\n",
    "            new_mention = data.Mention(new_begin, new_end)\n",
    "            new_cluster.add(new_mention)\n",
    "        new_document.clusters.append(new_cluster)\n",
    "\n",
    "    for mention, ner_tag in document.named_entities.items():\n",
    "        new_begin = map_begin(mention.begin)\n",
    "        new_end = map_end(mention.end)\n",
    "        new_mention = data.Mention(new_begin, new_end)\n",
    "        new_document.named_entities[new_mention] = ner_tag\n",
    "\n",
    "    for mention, constituency_tag in document.constituents.items():\n",
    "        new_begin = map_begin(mention.begin)\n",
    "        new_end = map_end(mention.end)\n",
    "        new_mention = data.Mention(new_begin, new_end)\n",
    "        new_document.constituents[new_mention] = constituency_tag\n",
    "\n",
    "    new_sentences = []\n",
    "    new_speakers = []\n",
    "    i, j = 0, 0\n",
    "    for sentence, speakers in zip(document.sentences, document.speakers):\n",
    "        n_words = len(sentence)\n",
    "        end = map_end(i + n_words - 1)\n",
    "        new_sentence = tokens[j: end + 1]\n",
    "        i += n_words\n",
    "        j = end + 1\n",
    "        new_sentences.append(new_sentence)\n",
    "        new_speakers.append([speakers[0] for _ in range(len(new_sentence))])\n",
    "    new_document.sentences = new_sentences\n",
    "    new_document.speakers = new_speakers\n",
    "\n",
    "    new_document.doc_key = document.doc_key\n",
    "    new_document.doc_id = document.doc_id\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Number of words = {len(words)}\")\n",
    "        print(words)\n",
    "        print()\n",
    "        print(f\"Number of tokens = {len(tokens)}\")\n",
    "        print(tokens)\n",
    "        print()\n",
    "        print(f\"Word characters: {word_characters}\")\n",
    "        print()\n",
    "        print(f\"Token characters: {token_characters}\")\n",
    "        print()\n",
    "        print(\"Word begin to word character:\")\n",
    "        print(word_begin_to_word_character)\n",
    "        print()\n",
    "        print(\"Word end to word character:\")\n",
    "        print(word_end_to_word_character)\n",
    "        print()\n",
    "        print(\"Word character to token character:\")\n",
    "        print(word_character_to_token_character.tolist())\n",
    "        print()\n",
    "        print(\"Token character to token index\")\n",
    "        print(token_character_to_token_index.tolist())\n",
    "    \n",
    "    for i in range(len(word_character_to_token_character) - 1):\n",
    "        assert word_character_to_token_character[i] <= (\n",
    "                word_character_to_token_character[i + 1])\n",
    "\n",
    "    return new_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '(www.mcoa.cn)' instead of '（www.mcoa.cn）'\n",
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '-LRB-c-RRB-?Io]o?' instead of '-LRB-c-RRB-?Ìö]o?'\n",
      "Using 'UDI'' instead of 'ÛDÌ’'\n",
      "Using 'Io]' instead of 'Ìö]'\n",
      "Using 'Io...-LRB-c-RRB-x.goIo]' instead of 'Ìò...-LRB-c-RRB-x˙goÌö]'\n",
      "Using 'iAnyway' instead of 'ِAnyway'\n",
      "Using 'vis-a-vis' instead of 'vis-à-vis'\n",
      "Using 'Y=' instead of '￥'\n",
      "Using ':' instead of '：'\n",
      "Using 'enosnail' instead of 'eのsnail'\n",
      "Using '*Lingtai' instead of '＊Lingtai'\n",
      "Using 'Shanyin*' instead of 'Shanyin＊'\n",
      "Using '#' instead of '□'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '-' instead of '→'\n",
      "Using '-' instead of '→'\n",
      "Using '-' instead of '→'\n",
      "Using '-' instead of '→'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '[(' instead of '【'\n",
      "Using ')]' instead of '】'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '(Yi)' instead of '（一）'\n",
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using 'Y=28' instead of '￥28'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '*' instead of '·'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '[(' instead of '【'\n",
      "Using ':)]' instead of ':】'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '.' instead of '・'\n",
      "Using '*The' instead of '●The'\n",
      "Using '*Stamp' instead of '●Stamp'\n",
      "Using '*' instead of '●'\n",
      "Using '*' instead of '●'\n",
      "Using '*' instead of '●'\n",
      "Using '#' instead of '■'\n",
      "Using '#' instead of '■'\n",
      "Using '#Buy' instead of '■Buy'\n",
      "Using '#Controlling' instead of '■Controlling'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 348/348 [00:01<00:00, 185.29it/s]\n",
      "100%|██████████| 343/343 [00:01<00:00, 250.13it/s]\n",
      "100%|██████████| 2802/2802 [00:12<00:00, 231.04it/s]\n"
     ]
    }
   ],
   "source": [
    "test_corpus = data.CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                          \"conll-2012/gold/test.english.jsonlines\")\n",
    "dev_corpus = data.CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                          \"conll-2012/gold/dev.english.jsonlines\")\n",
    "train_corpus = data.CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                          \"conll-2012/gold/train.english.jsonlines\")\n",
    "seq_test_corpus = data_util.remove_overlaps(test_corpus)\n",
    "seq_dev_corpus = data_util.remove_overlaps(dev_corpus)\n",
    "seq_train_corpus = data_util.remove_overlaps(train_corpus)\n",
    "seq_corpus = seq_test_corpus + seq_dev_corpus + seq_train_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = []\n",
    "longformer_seq_corpus = data.CorefCorpus()\n",
    "for document in seq_corpus.documents:\n",
    "    try:\n",
    "        longformer_document = remap_spans(document, model.tokenizer.tokenize)\n",
    "        longformer_seq_corpus.documents.append(longformer_document)\n",
    "    except AssertionError as e:\n",
    "        doc_ids.append(document.doc_id)\n",
    "        print(document.doc_id, document.doc_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id in doc_ids:\n",
    "    document = seq_corpus.documents[doc_id]\n",
    "    words = [word for sentence in document.sentences for word in sentence]\n",
    "    text = \" \".join(words)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    word_characters = \"\".join(words)\n",
    "    token_characters = \"\".join(tokens)\n",
    "    word_character_to_token_character = lcs(word_characters, token_characters, \n",
    "                                            ignore_token_characters=[\"Ġ\"])\n",
    "    word_characters_not_matched = [word_characters[i] \n",
    "                                   for i in range(len(word_characters)) \n",
    "                                   if i > 0 \n",
    "                                   and \n",
    "                                   word_character_to_token_character[i - 1] > 0 \n",
    "                                   and \n",
    "                                   word_character_to_token_character[i] == 0\n",
    "                                   ]\n",
    "    print(f\"doc_id = {doc_id}, doc_key = {document.doc_key}\")\n",
    "    print(\"word characters not matched:\")\n",
    "    print(word_characters_not_matched)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in seq_corpus.documents:\n",
    "    for sentence in document.sentences:\n",
    "        for word in sentence:\n",
    "            whitespace_removed_word = re.sub(r\"\\s\", \"\", word)\n",
    "            if len(whitespace_removed_word) == 0:\n",
    "                print(f\"doc_id = {document.doc_id}, \"\n",
    "                       \"doc_key = {document.doc_key}, whitespace word = {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_mentions: list[list[data.Mention]] = []\n",
    "\n",
    "for document in longformer_seq_corpus.documents:\n",
    "    document_representative_mentions: list[data.Mention] = []\n",
    "    for cluster in document.clusters:\n",
    "        mention = representatives.representative_mention(cluster, document)\n",
    "        document_representative_mentions.append(mention)\n",
    "    representative_mentions.append(document_representative_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_tensors(longformer_seq_corpus, representative_mentions, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43733, 4098])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
