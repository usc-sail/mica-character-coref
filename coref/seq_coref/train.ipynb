{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.seq_coref import coref_longformer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils import data as tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(directory) -> tdata.TensorDataset:\n",
    "    token_ids = torch.load(os.path.join(directory, \"tokens.pt\"))\n",
    "    mention_ids = torch.load(os.path.join(directory, \"mentions.pt\"))\n",
    "    label_ids = torch.load(os.path.join(directory, \"labels.pt\"))\n",
    "    attn_mask = torch.load(os.path.join(directory, \"attn.pt\"))\n",
    "    global_attn_mask = torch.load(os.path.join(directory, \"global_attn.pt\"))\n",
    "    doc_ids = torch.load(os.path.join(directory, \"docs.pt\"))\n",
    "    dataset = tdata.TensorDataset(token_ids, mention_ids, label_ids,\n",
    "                                    attn_mask, global_attn_mask, doc_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_tensors(\"/home/sbaruah_usc_edu/mica_text_coref/data/tensors/longformer_seq_tensors/train\")\n",
    "test_dataset = load_tensors(\"/home/sbaruah_usc_edu/mica_text_coref/data/tensors/longformer_seq_tensors/test\")\n",
    "dev_dataset = load_tensors(\"/home/sbaruah_usc_edu/mica_text_coref/data/tensors/longformer_seq_tensors/dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.Size([34785, 4098])\n",
      "torch.int32 torch.Size([34785, 4098])\n",
      "torch.int32 torch.Size([34785, 4098])\n",
      "torch.float32 torch.Size([34785, 4098])\n",
      "torch.float32 torch.Size([34785, 4098])\n",
      "torch.int32 torch.Size([34785])\n"
     ]
    }
   ],
   "source": [
    "for tensor in train_dataset.tensors:\n",
    "    print(tensor.dtype, tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136 batches\n"
     ]
    }
   ],
   "source": [
    "dataloader = tdata.DataLoader(train_dataset, batch_size=256, shuffle=True, \n",
    "                              drop_last=False)\n",
    "print(f\"{len(dataloader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = coref_longformer.CorefLongformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    (batch_token_ids, batch_mention_ids, batch_label_ids, batch_attn_mask, \n",
    "        batch_global_attn_mask, batch_doc_ids) = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sbaruah_usc_edu/mica_text_coref/coref/seq_coref/train.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsailnlp.us-west1-b.sail-ccmi/home/sbaruah_usc_edu/mica_text_coref/coref/seq_coref/train.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m longformer_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlongformer(batch_token_ids, batch_attn_mask, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsailnlp.us-west1-b.sail-ccmi/home/sbaruah_usc_edu/mica_text_coref/coref/seq_coref/train.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                      batch_global_attn_mask)\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py:1704\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[39m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \u001b[39m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m extended_attention_mask: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)[\n\u001b[1;32m   1701\u001b[0m     :, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :\n\u001b[1;32m   1702\u001b[0m ]\n\u001b[0;32m-> 1704\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1705\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids, position_ids\u001b[39m=\u001b[39;49mposition_ids, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids, inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds\n\u001b[1;32m   1706\u001b[0m )\n\u001b[1;32m   1708\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1709\u001b[0m     embedding_output,\n\u001b[1;32m   1710\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1715\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m   1716\u001b[0m )\n\u001b[1;32m   1717\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py:474\u001b[0m, in \u001b[0;36mLongformerEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m    471\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    475\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)\n\u001b[1;32m    476\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/coref/lib/python3.10/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "longformer_output = model.longformer(batch_token_ids, batch_attn_mask, \n",
    "                                     batch_global_attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "document = \"\"\"Entertainment media, available in rich variety and diverse forms ranging from traditional feature films, television shows, and theatrical plays to contemporary digital shorts and streaming content, can profoundly impact audience perceptions, beliefs, attitudes, and behavior.\n",
    "Media narratives aim to inform and engage us with stories about the culture, lives, and experiences of different communities of people, including reflecting societal ideas and trends.\n",
    "They shed light on various social, economic and political issues, educating and creating awareness on different aspects of life.\"\"\"\n",
    "long_document = \" \".join(document for _ in range(50))\n",
    "tokens = model.tokenizer.tokenize(long_document)[:4096]\n",
    "token_ids = model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "token_ids = torch.LongTensor(token_ids)\n",
    "attention_mask = torch.FloatTensor([1. for _ in range(len(tokens))])\n",
    "global_attention_mask = torch.FloatTensor([0. for _ in range(len(tokens))])\n",
    "batch_token_ids = token_ids.unsqueeze(0)\n",
    "batch_attention_mask = attention_mask.unsqueeze(0)\n",
    "batch_global_attention_mask = global_attention_mask.unsqueeze(0)\n",
    "output = model.longformer(batch_token_ids, batch_attention_mask, \n",
    "                            batch_global_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
