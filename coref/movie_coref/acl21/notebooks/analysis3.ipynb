{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "from tqdm import tqdm, trange\n",
    "import re\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from textdistance import lcsseq\n",
    "from collections import Counter\n",
    "import multiset\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read script, parsed_script and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bourne_script = open(\"annotated-data/bourne.script.txt\").read()\n",
    "bourne_parsed_script = open(\"annotated-data/bourne.script_parsed.txt\").read().strip()\n",
    "bourne_annotations = pd.read_csv(\"annotated-data/bourne.coref.csv\", index_col = None)\n",
    "\n",
    "basterds_script = open(\"annotated-data/basterds.script.txt\").read()\n",
    "basterds_parsed_script = open(\"annotated-data/basterds.script_parsed.txt\").read().strip()\n",
    "basterds_annotations = pd.read_csv(\"annotated-data/basterds.coref.csv\", index_col = None)\n",
    "\n",
    "shawshank_script = open(\"annotated-data/shawshank.script.txt\").read()\n",
    "shawshank_parsed_script = open(\"annotated-data/shawshank.script_parsed.txt\").read().strip()\n",
    "shawshank_annotations = pd.read_csv(\"annotated-data/shawshank.coref.csv\", index_col = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-18 11:12:42 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-11-18 11:12:42 INFO: Use device: gpu\n",
      "2020-11-18 11:12:42 INFO: Loading: tokenize\n",
      "2020-11-18 11:12:46 INFO: Loading: pos\n",
      "2020-11-18 11:12:47 INFO: Done loading processors!\n",
      "2020-11-18 11:12:47 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-11-18 11:12:47 INFO: Use device: gpu\n",
      "2020-11-18 11:12:47 INFO: Loading: tokenize\n",
      "2020-11-18 11:12:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(processors=\"tokenize,pos\")\n",
    "nlp2 = stanza.Pipeline(processors=\"tokenize\", tokenize_no_ssplit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tokens(parsed_script):\n",
    "    lines = parsed_script.split(\"\\n\")\n",
    "    tags = []\n",
    "    texts = []\n",
    "    docs = []\n",
    "    flattened_tokens = []\n",
    "    flattened_token_sizes = []\n",
    "    flattened_indices = []\n",
    "\n",
    "    for i, line in tqdm(enumerate(lines), total=len(lines)):\n",
    "        tag = line[0]\n",
    "        text = line[2:].strip()\n",
    "        doc = nlp(text).to_dict()\n",
    "\n",
    "        tags.append(tag)\n",
    "        texts.append(text)\n",
    "        docs.append(doc)\n",
    "\n",
    "        for j, sent in enumerate(doc):\n",
    "            for k, token in enumerate(sent):\n",
    "                flattened_tokens.append(token[\"text\"])\n",
    "                flattened_token_sizes.append(len(re.sub(\"\\s\", \"\", token[\"text\"])))\n",
    "                flattened_indices.append((i, j, k))\n",
    "    \n",
    "    return {\"tag\": tags, \"text\": texts, \"doc\": docs, \"token\": flattened_tokens, \"index\": flattened_indices, \"size\": flattened_token_sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 649/649 [00:23<00:00, 28.20it/s]\n",
      "100%|██████████| 591/591 [00:20<00:00, 28.45it/s]\n",
      "100%|██████████| 525/525 [00:18<00:00, 27.97it/s]\n"
     ]
    }
   ],
   "source": [
    "bourne_info = flatten_tokens(bourne_parsed_script)\n",
    "basterds_info = flatten_tokens(basterds_parsed_script)\n",
    "shawshank_info = flatten_tokens(shawshank_parsed_script)\n",
    "\n",
    "bourne_info[\"coref\"] = bourne_annotations\n",
    "bourne_info[\"script\"] = bourne_script\n",
    "\n",
    "basterds_info[\"coref\"] = basterds_annotations\n",
    "basterds_info[\"script\"] = basterds_script\n",
    "\n",
    "shawshank_info[\"coref\"] = shawshank_annotations\n",
    "shawshank_info[\"script\"] = shawshank_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "bourne_info[\"name\"] = \"bourne\"\n",
    "basterds_info[\"name\"] = \"basterds\"\n",
    "shawshank_info[\"name\"] = \"shawshank\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find mention span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lcsi(A, B):\n",
    "    a = [\"\"] + A\n",
    "    b = [\"\"] + B\n",
    "    lcsl = np.zeros((len(a), len(b)), dtype=int)\n",
    "    lcsi = np.full((len(a), len(b)), set())\n",
    "\n",
    "    for i in range(1, len(a)):\n",
    "        for j in range(1, len(b)):\n",
    "            if a[i] == b[j]:\n",
    "                lcsl[i,j] = lcsl[i-1,j-1] + 1\n",
    "                lcsi[i,j] = lcsi[i-1,j-1].union({(i-1,j-1)})\n",
    "            else:\n",
    "                k, l = i-1, j-1\n",
    "                if lcsl[i-1,j] > lcsl[k,l]:\n",
    "                    k, l = i-1, j\n",
    "                if lcsl[i,j-1] > lcsl[k,l]:\n",
    "                    k, l = i, j-1\n",
    "                lcsl[i,j] = lcsl[k,l]\n",
    "                lcsi[i,j] = lcsi[k,l]\n",
    "                \n",
    "    return lcsi[len(a)-1,len(b)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mention_span(begin, end, info):\n",
    "    script = info[\"script\"]\n",
    "    qi = begin - 1\n",
    "    qj = end\n",
    "    \n",
    "    n_left_context_tokens = 0\n",
    "    n_left_context_chars = 0\n",
    "    n_right_context_tokens = 0\n",
    "    n_right_context_chars = 0\n",
    "    \n",
    "    while qi >= 0:\n",
    "        while qi >= 0 and re.match(\"\\s\", script[qi]):\n",
    "            qi -= 1\n",
    "        while qi >= 0 and re.match(\"\\S\", script[qi]):\n",
    "            qi -= 1\n",
    "            n_left_context_chars += 1\n",
    "        n_left_context_tokens += 1\n",
    "        if n_left_context_tokens == 5 or n_left_context_chars >= 50:\n",
    "            break\n",
    "    qi += 1\n",
    "    \n",
    "    tot = len(script)\n",
    "    while qj < tot:\n",
    "        while qj < tot and re.match(\"\\s\", script[qj]):\n",
    "            qj += 1\n",
    "        while qj < tot and re.match(\"\\S\", script[qj]):\n",
    "            qj += 1\n",
    "            n_right_context_chars += 1\n",
    "        n_right_context_tokens += 1\n",
    "        if n_right_context_tokens == 5 or n_right_context_chars >= 50:\n",
    "            break\n",
    "    \n",
    "    left_context = script[qi: begin].strip()\n",
    "    right_context = script[end: qj].strip()\n",
    "    mention = script[begin:end]\n",
    "    \n",
    "    lc_doc = nlp2(left_context)\n",
    "    lc_tokens = [token.text for token in lc_doc.iter_words()]\n",
    "    rc_doc = nlp2(right_context)\n",
    "    rc_tokens = [token.text for token in rc_doc.iter_words()]\n",
    "    mention_doc = nlp2(mention)\n",
    "    mention_tokens = [token.text for token in mention_doc.iter_words()]\n",
    "    \n",
    "    lc_set = multiset.Multiset(lc_tokens)\n",
    "    rc_set = multiset.Multiset(rc_tokens)\n",
    "    mention_set = multiset.Multiset(mention_tokens)\n",
    "    \n",
    "    h = len(lc_tokens) + len(mention_tokens) + len(rc_tokens)\n",
    "    tokens = info[\"token\"]\n",
    "    indices = info[\"index\"]\n",
    "    ind_match_arr = []\n",
    "    \n",
    "    for i in range(len(tokens) - h + 1):\n",
    "        if tokens[i: i + h] == lc_tokens + mention_tokens + rc_tokens:\n",
    "            return i + len(lc_tokens), i + len(lc_tokens) + len(mention_tokens)\n",
    "        else:\n",
    "            token_set = multiset.Multiset(tokens[i: i + h])\n",
    "            nc1 = len(token_set.intersection(mention_set))\n",
    "            token_set.difference_update(mention_set)\n",
    "            nc2 = len(token_set.intersection(lc_set))\n",
    "            token_set.difference_update(lc_set)\n",
    "            nc3 = len(token_set.intersection(rc_set))\n",
    "            if nc1 and nc2 and nc3:\n",
    "                ind_match_arr.append((i, nc1 + nc2 + nc3))\n",
    "\n",
    "    ind_match_arr = sorted(ind_match_arr, key = lambda item: item[1], reverse = True)\n",
    "\n",
    "    print_lines = []\n",
    "    \n",
    "    for ind, match in ind_match_arr[:20]:\n",
    "        if match == 0:\n",
    "            break\n",
    "        lcsi = find_lcsi(tokens[ind: ind + h], lc_tokens + mention_tokens + rc_tokens)\n",
    "        alignment = []\n",
    "        for i, j in lcsi:\n",
    "            if j >= len(lc_tokens) and j < len(lc_tokens) + len(mention_tokens):\n",
    "                alignment.append((i,j))\n",
    "        alignment = sorted(alignment)\n",
    "        matched_tokens = [tokens[ind + i] for i, _ in alignment]\n",
    "        print_lines.append(f\"{match:2d} {len(lcsi):2d} {len(alignment):2d} {matched_tokens} {tokens[ind: ind + h]}\")\n",
    "        if matched_tokens == mention_tokens:\n",
    "            return ind + alignment[0][0], ind + alignment[-1][0] + 1\n",
    "    \n",
    "    return False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mention_spans(info):\n",
    "    n_matched = 0\n",
    "    begin_span = []\n",
    "    end_span = []\n",
    "    \n",
    "    for ri, row in tqdm(info[\"coref\"].iterrows(), total = len(info[\"coref\"])):\n",
    "        i, j = find_mention_span(row[\"begin\"], row[\"end\"], info)\n",
    "        if i:\n",
    "            begin_span.append(i)\n",
    "            end_span.append(j)\n",
    "            n_matched += 1\n",
    "        else:\n",
    "            begin_span.append(None)\n",
    "            end_span.append(None)\n",
    "    \n",
    "    info[\"coref\"][\"begin_span\"] = begin_span\n",
    "    info[\"coref\"][\"end_span\"] = end_span\n",
    "    print(f\"{n_matched}/{len(info['coref'])} matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 911/911 [03:02<00:00,  5.00it/s]\n",
      "  0%|          | 0/1005 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "869/911 matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1005/1005 [03:06<00:00,  5.40it/s]\n",
      "  0%|          | 1/887 [00:00<02:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968/1005 matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887/887 [02:33<00:00,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "873/887 matched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "find_mention_spans(bourne_info)\n",
    "find_mention_spans(basterds_info)\n",
    "find_mention_spans(shawshank_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc_spans(info):\n",
    "    n_no_overlap = 0\n",
    "    n_overlap_element = 0\n",
    "    n_overlap_sentence = 0\n",
    "    n = 0\n",
    "    element = np.full(len(info[\"coref\"]), np.nan)\n",
    "    sent = np.full(len(info[\"coref\"]), np.nan)\n",
    "    word_begin = np.full(len(info[\"coref\"]), np.nan)\n",
    "    word_end = np.full(len(info[\"coref\"]), np.nan)\n",
    "\n",
    "    for ind, row in info[\"coref\"].iterrows():\n",
    "        element_indices, sentence_indices, word_indices = [], [], []\n",
    "        if pd.notna(row[\"begin_span\"]):\n",
    "            for i in range(int(row[\"begin_span\"]), int(row[\"end_span\"])):\n",
    "                x, y, z = info[\"index\"][i]\n",
    "                element_indices.append(x)\n",
    "                sentence_indices.append(y)\n",
    "                word_indices.append(z)\n",
    "            if len(set(element_indices)) > 1:\n",
    "                n_overlap_element += 1\n",
    "            if len(set(element_indices)) > 1 or len(set(sentence_indices)) > 1:\n",
    "                n_overlap_sentence += 1\n",
    "            else:\n",
    "                n_no_overlap += 1\n",
    "                element[ind] = element_indices[0]\n",
    "                sent[ind] = sentence_indices[0]\n",
    "                word_begin[ind] = word_indices[0]\n",
    "                word_end[ind] = word_indices[-1]\n",
    "            n += 1\n",
    "    info[\"coref\"][\"element\"] = element\n",
    "    info[\"coref\"][\"sent\"] = sent\n",
    "    info[\"coref\"][\"word_begin\"] = word_begin\n",
    "    info[\"coref\"][\"word_end\"] = word_end\n",
    "\n",
    "    print(f\"{n_overlap_element}/{n} overlap elements\")\n",
    "    print(f\"{n_overlap_sentence}/{n} overlap sents\")\n",
    "    print(f\"{n_no_overlap}/{n} no overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/869 overlap elements\n",
      "4/869 overlap sents\n",
      "865/869 no overlap\n"
     ]
    }
   ],
   "source": [
    "find_doc_spans(bourne_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/968 overlap elements\n",
      "79/968 overlap sents\n",
      "889/968 no overlap\n"
     ]
    }
   ],
   "source": [
    "find_doc_spans(basterds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/873 overlap elements\n",
      "1/873 overlap sents\n",
      "872/873 no overlap\n"
     ]
    }
   ],
   "source": [
    "find_doc_spans(shawshank_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check non-nested structures within sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info in [bourne_info, basterds_info, shawshank_info]:\n",
    "    for (element, sent), df in info[\"coref\"].groupby([\"element\",\"sent\"]):\n",
    "        spans = [(int(row[\"word_begin\"]), int(row[\"word_end\"])) for _, row in df.iterrows()]\n",
    "        spans = sorted(spans)\n",
    "        for i in range(len(spans)):\n",
    "            for j in range(i + 1, len(spans)):\n",
    "                x, y = spans[i]\n",
    "                a, b = spans[j]\n",
    "                if x < a and a <= y and y < b:\n",
    "                    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CoNLL-U Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_conll(info, conll_file):\n",
    "    index_to_mention_start_labels = {}\n",
    "    index_to_mention_end_labels = {}\n",
    "\n",
    "    for _, row in info[\"coref\"].iterrows():\n",
    "        if pd.notna(row[\"element\"]):\n",
    "            begin_index = int(row[\"begin_span\"])\n",
    "            end_index = int(row[\"end_span\"]) - 1\n",
    "            mention_label = int(row[\"entityNum\"])\n",
    "\n",
    "            if begin_index not in index_to_mention_start_labels:\n",
    "                index_to_mention_start_labels[begin_index] = set()\n",
    "            if end_index not in index_to_mention_end_labels:\n",
    "                index_to_mention_end_labels[end_index] = set()\n",
    "\n",
    "            index_to_mention_start_labels[begin_index].add(mention_label)\n",
    "            index_to_mention_end_labels[end_index].add(mention_label)\n",
    "\n",
    "    ind = 0\n",
    "    for i, element in enumerate(info[\"doc\"]):\n",
    "        for j, sent in enumerate(element):\n",
    "            for k, word in enumerate(sent):\n",
    "                info[\"doc\"][i][j][k][\"coref\"] = \"-\"\n",
    "                text_labels = []\n",
    "                start_labels = set()\n",
    "                end_labels = set()\n",
    "                if ind in index_to_mention_start_labels:\n",
    "                    start_labels = index_to_mention_start_labels[ind]\n",
    "                if ind in index_to_mention_end_labels:\n",
    "                    end_labels = index_to_mention_end_labels[ind]\n",
    "                common_labels = start_labels.intersection(end_labels)\n",
    "                start_labels.difference_update(common_labels)\n",
    "                end_labels.difference_update(common_labels)\n",
    "\n",
    "                for label in common_labels:\n",
    "                    text_labels.append(f\"({label})\")\n",
    "                for label in start_labels:\n",
    "                    text_labels.append(f\"({label}\")\n",
    "                for label in end_labels:\n",
    "                    text_labels.append(f\"{label})\")\n",
    "                text = \"|\".join(text_labels)\n",
    "                if text:\n",
    "                    info[\"doc\"][i][j][k][\"coref\"] = text\n",
    "                ind += 1\n",
    "            \n",
    "    lines = [f\"#begin document ({info['name']}); part 0\"]\n",
    "    ind = 0\n",
    "    for i, element in enumerate(info[\"doc\"]):\n",
    "        for j, sent in enumerate(element):\n",
    "            for k, word in enumerate(sent):\n",
    "                lines.append(f\"{info['tag'][i]}\\t{j:2d}\\t{k:2d}\\t{word['text']:>20s}\\t{word['xpos']:5s}\\t-\\t-\\t-\\t-\\t-\\t-\\t{word['coref']}\")\n",
    "            lines.append(\"\")\n",
    "    lines.append(\"#end document\")\n",
    "    conll = \"\\n\".join(lines)\n",
    "    open(conll_file, \"w\").write(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_conll(bourne_info, \"annotated-data/bourne.conll\")\n",
    "convert_to_conll(basterds_info, \"annotated-data/basterds.conll\")\n",
    "convert_to_conll(shawshank_info, \"annotated-data/shawshank.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs(jsonl_file):\n",
    "    recs = []\n",
    "    with jsonlines.open(jsonl_file) as reader:\n",
    "        for obj in reader:\n",
    "            recs.append(obj)\n",
    "    return recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_recs = get_recs(\"corefhoi/data/dev.english.512.jsonlines\")\n",
    "bourne_recs = get_recs(\"corefhoi/data/bourne.english.512.jsonlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1029, 1)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_recs), len(bourne_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_rec = dev_recs[0]\n",
    "bourne_rec = bourne_recs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['doc_key', 'tokens', 'sentences', 'speakers', 'constituents', 'ner', 'clusters', 'sentence_map', 'subtoken_map', 'pronouns'])\n",
      "dict_keys(['doc_key', 'tokens', 'sentences', 'speakers', 'constituents', 'ner', 'clusters', 'sentence_map', 'subtoken_map', 'pronouns'])\n"
     ]
    }
   ],
   "source": [
    "print(dev_rec.keys())\n",
    "print(bourne_rec.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 22)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_rec['sentences']), len(bourne_rec[\"sentences\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6.7",
   "language": "python",
   "name": "python3.6.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
