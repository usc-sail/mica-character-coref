{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                     \"word_level_coref/roberta_(e20_2021.05.02_01.16)_release.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bert', 'we', 'rough_scorer', 'pw', 'a_scorer', 'sp', 'epochs_trained'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n",
      "we\n",
      "\tattn.weight                                       : torch.float32 torch.Size([1, 1024])\n",
      "\tattn.bias                                         : torch.float32 torch.Size([1])\n",
      "rough_scorer\n",
      "\tbilinear.weight                                   : torch.float32 torch.Size([1024, 1024])\n",
      "\tbilinear.bias                                     : torch.float32 torch.Size([1024])\n",
      "pw\n",
      "\tgenre_emb.weight                                  : torch.float32 torch.Size([7, 20])\n",
      "\tdistance_emb.weight                               : torch.float32 torch.Size([9, 20])\n",
      "\tspeaker_emb.weight                                : torch.float32 torch.Size([2, 20])\n",
      "a_scorer\n",
      "\thidden.0.weight                                   : torch.float32 torch.Size([1024, 3132])\n",
      "\thidden.0.bias                                     : torch.float32 torch.Size([1024])\n",
      "\tout.weight                                        : torch.float32 torch.Size([1, 1024])\n",
      "\tout.bias                                          : torch.float32 torch.Size([1])\n",
      "sp\n",
      "\tffnn.0.weight                                     : torch.float32 torch.Size([1024, 2112])\n",
      "\tffnn.0.bias                                       : torch.float32 torch.Size([1024])\n",
      "\tffnn.3.weight                                     : torch.float32 torch.Size([256, 1024])\n",
      "\tffnn.3.bias                                       : torch.float32 torch.Size([256])\n",
      "\tffnn.6.weight                                     : torch.float32 torch.Size([64, 256])\n",
      "\tffnn.6.bias                                       : torch.float32 torch.Size([64])\n",
      "\tconv.0.weight                                     : torch.float32 torch.Size([4, 64, 3])\n",
      "\tconv.0.bias                                       : torch.float32 torch.Size([4])\n",
      "\tconv.1.weight                                     : torch.float32 torch.Size([2, 4, 3])\n",
      "\tconv.1.bias                                       : torch.float32 torch.Size([2])\n",
      "\temb.weight                                        : torch.float32 torch.Size([128, 64])\n",
      "epochs_trained: 20\n"
     ]
    }
   ],
   "source": [
    "for name, module in weights.items():\n",
    "    if isinstance(module, dict):\n",
    "        print(name)\n",
    "        if name != \"bert\":\n",
    "            for tensor_name, tensor in module.items():\n",
    "                print(f\"\\t{tensor_name:50s}: {tensor.dtype} {tensor.shape}\")\n",
    "    else:\n",
    "        print(f\"{name}: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "encoder = AutoModel.from_pretrained(\"roberta-large\", add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['pooler.dense.weight', 'pooler.dense.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(weights[\"bert\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0298, -0.0126, -0.0024,  ..., -0.0076, -0.0152, -0.0162],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[\"rough_scorer\"][\"bilinear.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = nn.Linear(1024, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Linear:\n\tMissing key(s) in state_dict: \"weight\", \"bias\". \n\tUnexpected key(s) in state_dict: \"bilinear.weight\", \"bilinear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sbaruah_usc_edu/mica_text_coref/coref/movie_coref/notebooks/check_load_weights.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsailnlp.us-west1-b.sail-ccmi/home/sbaruah_usc_edu/mica_text_coref/coref/movie_coref/notebooks/check_load_weights.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m ll\u001b[39m.\u001b[39;49mload_state_dict(weights[\u001b[39m\"\u001b[39;49m\u001b[39mrough_scorer\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/anaconda3/envs/coreference/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Linear:\n\tMissing key(s) in state_dict: \"weight\", \"bias\". \n\tUnexpected key(s) in state_dict: \"bilinear.weight\", \"bilinear.bias\". "
     ]
    }
   ],
   "source": [
    "ll.load_state_dict(weights[\"rough_scorer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weights[\"rough_scorer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "odict = weights[\"rough_scorer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['bilinear.weight', 'bilinear.bias'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "odict[\"weight\"] = odict.pop(\"bilinear.weight\")\n",
    "odict[\"bias\"] = odict.pop(\"bilinear.bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 1.7543e-01, -2.7519e-02,  1.4653e-02,  ..., -1.0604e-02,\n",
       "                        2.5374e-02, -1.7066e-03],\n",
       "                      [ 4.9204e-02,  1.3426e-01, -9.1869e-03,  ..., -4.4060e-03,\n",
       "                        8.7263e-03, -9.5577e-03],\n",
       "                      [-1.7296e-02, -1.0086e-02,  5.9279e-02,  ...,  1.6338e-05,\n",
       "                       -3.0986e-02, -2.6442e-02],\n",
       "                      ...,\n",
       "                      [-8.4727e-04, -7.4070e-02, -3.5817e-02,  ...,  1.2996e-01,\n",
       "                       -6.5809e-02, -1.6725e-02],\n",
       "                      [-2.8037e-02,  2.8999e-03,  3.8483e-02,  ..., -9.9639e-03,\n",
       "                        1.5081e-01, -3.2012e-02],\n",
       "                      [ 7.8245e-03,  2.4031e-02, -1.3260e-03,  ...,  1.7210e-02,\n",
       "                        2.2953e-02,  7.6747e-02]], device='cuda:0')),\n",
       "             ('bias',\n",
       "              tensor([ 0.0298, -0.0126, -0.0024,  ..., -0.0076, -0.0152, -0.0162],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.load_state_dict(odict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5322e+00,  3.5645e-01,  1.2496e+00, -6.6363e-01, -5.7696e-02,\n",
       "         -2.4032e+00, -1.1683e+00, -1.5347e+00, -9.0168e-01, -3.4147e-01],\n",
       "        [ 1.0448e+00,  8.6082e-01,  4.0752e-01,  1.2085e-01,  6.8700e-01,\n",
       "         -1.9015e+00,  4.4953e-01, -1.2387e+00,  3.4712e-01, -2.3638e-01],\n",
       "        [-7.8853e-01,  8.4060e-01,  1.6117e+00, -6.3626e-01, -2.2419e-01,\n",
       "          1.3773e+00,  7.7333e-01,  4.2581e-01, -3.2233e-01,  7.2474e-01],\n",
       "        [ 6.1090e-01, -1.0920e+00,  1.7118e+00, -1.7395e-01,  1.1789e+00,\n",
       "         -6.5169e-01,  6.7840e-01,  4.3342e-01, -8.1362e-01,  5.2562e-01],\n",
       "        [ 1.1133e-01, -6.5109e-02, -1.6205e-01, -5.5029e-01, -9.2431e-01,\n",
       "         -8.0238e-01,  8.7520e-01, -2.2727e-01,  6.7101e-01,  1.7409e-03],\n",
       "        [ 4.8888e-02,  1.2630e+00,  4.0822e-02, -1.3240e+00, -3.5563e-01,\n",
       "         -2.3136e-01, -2.4115e+00,  2.2647e-01, -3.2495e-01,  2.7960e-01]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.randn((6, 10))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True, False, False, False, False, False, False, False],\n",
       "        [ True,  True,  True,  True,  True, False,  True, False,  True, False],\n",
       "        [False,  True,  True, False, False,  True,  True,  True, False,  True],\n",
       "        [ True, False,  True, False,  True, False,  True,  True, False,  True],\n",
       "        [ True, False, False, False, False, False,  True, False,  True,  True],\n",
       "        [ True,  True,  True, False, False, False, False,  True, False,  True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m > 0).nonzero().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 2],\n",
       "        [1, 0],\n",
       "        [1, 1],\n",
       "        [1, 2],\n",
       "        [1, 3],\n",
       "        [1, 4],\n",
       "        [1, 6],\n",
       "        [1, 8],\n",
       "        [2, 1],\n",
       "        [2, 2],\n",
       "        [2, 5],\n",
       "        [2, 6],\n",
       "        [2, 7],\n",
       "        [2, 9],\n",
       "        [3, 0],\n",
       "        [3, 2],\n",
       "        [3, 4],\n",
       "        [3, 6],\n",
       "        [3, 7],\n",
       "        [3, 9],\n",
       "        [4, 0],\n",
       "        [4, 6],\n",
       "        [4, 8],\n",
       "        [4, 9],\n",
       "        [5, 0],\n",
       "        [5, 1],\n",
       "        [5, 2],\n",
       "        [5, 7],\n",
       "        [5, 9]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m > 0).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4,\n",
       "         4, 5, 5, 5, 5, 5]),\n",
       " tensor([1, 2, 0, 1, 2, 3, 4, 6, 8, 1, 2, 5, 6, 7, 9, 0, 2, 4, 6, 7, 9, 0, 6, 8,\n",
       "         9, 0, 1, 2, 7, 9]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m > 0).nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCoreference(nn.Module):\n",
    "    \"\"\"Movie screenplay coreference model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder_name: str,\n",
    "                 parsetag_size: int,\n",
    "                 postag_size: int,\n",
    "                 nertag_size: int,\n",
    "                 tag_embedding_size: int,\n",
    "                 gru_nlayers: int,\n",
    "                 gru_hidden_size: int,\n",
    "                 gru_bidirectional: bool,\n",
    "                 dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Word Encoder\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name, \n",
    "                                                 add_pooling_layer=False)\n",
    "        word_embedding_size = self.encoder.config.hidden_size\n",
    "        self.attn = nn.Linear(word_embedding_size, 1)\n",
    "\n",
    "        # Character Recognizer\n",
    "        gru_input_size = word_embedding_size + 3*tag_embedding_size + 2\n",
    "        gru_output_size = gru_hidden_size * (1 + gru_bidirectional)\n",
    "        self.parsetag_embedding = nn.Embedding(parsetag_size, \n",
    "                                               tag_embedding_size)\n",
    "        self.postag_embedding = nn.Embedding(postag_size, tag_embedding_size)\n",
    "        self.nertag_embedding = nn.Embedding(nertag_size, tag_embedding_size)\n",
    "        self.gru = nn.GRU(gru_input_size, gru_hidden_size,\n",
    "                          num_layers=gru_nlayers, batch_first=True,\n",
    "                          dropout=dropout, bidirectional=gru_bidirectional)\n",
    "        self.character_recognizer = nn.Linear(gru_output_size, 2)\n",
    "\n",
    "        # Coarse Coreference Scorer\n",
    "        self.coarse_scorer = nn.Linear(word_embedding_size, word_embedding_size)\n",
    "\n",
    "        # Pairwise Encoder\n",
    "        self.genre_embedding = nn.Embedding(7, 20)\n",
    "        self.distance_embedding = nn.Embedding(9, 20)\n",
    "        self.speaker_embedding = nn.Embedding(2, 20)\n",
    "\n",
    "        # Fine/Anaphoricity Coreference Scorer\n",
    "        pairwise_encoding_size = 3*word_embedding_size + 3*20\n",
    "        self.anaphoricity_scorer = nn.Sequential(\n",
    "            nn.Linear(pairwise_encoding_size, word_embedding_size), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(word_embedding_size, 1))\n",
    "\n",
    "        # Span Predictor\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(word_embedding_size * 2 + 64, word_embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(word_embedding_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(256, 64),\n",
    "        )\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(64, 4, 3, 1, 1),\n",
    "            nn.Conv1d(4, 2, 3, 1, 1)\n",
    "        )\n",
    "        self.sp_distance_embedding = nn.Embedding(128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sbaruah_usc_edu/anaconda3/envs/coreference/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = MovieCoreference(\"roberta-large\", 6, 5, 5, 16, 1, 256, True, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['pooler.dense.weight', 'pooler.dense.bias'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.load_state_dict(weights[\"bert\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.0274, -0.0074,  0.0170,  ..., -0.0028, -0.0003,  0.0308]])),\n",
       "             ('bias', tensor([0.0079]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attn.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('attn.weight',\n",
       "              tensor([[-0.0294,  0.0403, -0.0034,  ..., -0.0082, -0.0159, -0.0712]],\n",
       "                     device='cuda:0')),\n",
       "             ('attn.bias', tensor([-0.0024], device='cuda:0'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = weights[\"we\"]\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0294,  0.0403, -0.0034,  ..., -0.0082, -0.0159, -0.0712]],\n",
       "                     device='cuda:0')),\n",
       "             ('bias', tensor([-0.0024], device='cuda:0'))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights[\"weight\"] = attn_weights.pop(\"attn.weight\")\n",
    "attn_weights[\"bias\"] = attn_weights.pop(\"attn.bias\")\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.attn.load_state_dict(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-0.0012, -0.0090,  0.0198,  ...,  0.0059, -0.0276, -0.0110],\n",
       "                      [-0.0181,  0.0176, -0.0229,  ...,  0.0203, -0.0258, -0.0184],\n",
       "                      [-0.0262,  0.0248, -0.0231,  ...,  0.0295, -0.0271,  0.0060],\n",
       "                      ...,\n",
       "                      [-0.0188, -0.0187,  0.0175,  ..., -0.0174,  0.0008, -0.0183],\n",
       "                      [-0.0161,  0.0238, -0.0280,  ..., -0.0187, -0.0077,  0.0038],\n",
       "                      [ 0.0194, -0.0022,  0.0209,  ...,  0.0256,  0.0307, -0.0286]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.0171,  0.0281, -0.0117,  ...,  0.0109, -0.0084,  0.0265]))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coarse_scorer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.3629, -0.0317, -1.5324,  0.1600,  0.4448, -1.6962,  0.1380, -0.4193,\n",
       "                        0.8495,  0.3328,  0.3605, -0.6877, -0.9906, -1.3488, -0.1052, -1.1878,\n",
       "                        0.0500,  0.0109,  0.1253, -1.0734],\n",
       "                      [-0.8249,  0.6914,  0.4895,  1.4984, -0.7223,  0.5873, -0.4211, -0.8235,\n",
       "                       -0.0543, -2.6446, -0.3753,  0.8424,  0.1767, -0.1740, -0.9725, -1.1067,\n",
       "                        0.6391,  0.2110,  0.8932,  0.3329],\n",
       "                      [-0.5997,  0.1488, -1.2311,  0.7732,  1.2663, -0.0118,  1.1087,  1.2458,\n",
       "                        0.1896, -1.7539,  0.6302, -1.0682,  0.2268,  0.6963,  0.2854, -0.2986,\n",
       "                        0.7904,  0.0242, -1.0095, -1.2411],\n",
       "                      [-1.4591, -1.0668, -0.7136, -1.0395,  0.5830, -0.7281,  1.2340,  1.0527,\n",
       "                        0.4809,  0.3712, -1.2334,  1.8825, -0.0613,  0.4623,  0.9979, -1.0711,\n",
       "                        0.0243, -0.8337,  1.4439, -2.0121],\n",
       "                      [ 0.2474, -0.1701,  1.0547, -0.1409,  1.7882,  1.1593, -0.0929,  2.8107,\n",
       "                       -0.0997,  0.2932, -1.4443,  0.7148,  0.2941, -0.3788,  0.0878, -1.1242,\n",
       "                       -0.4589, -0.4639, -1.2650, -0.5163],\n",
       "                      [ 0.0563, -0.7713,  1.0885, -0.0361,  1.6820,  1.1901,  0.8899,  1.8708,\n",
       "                        1.8743,  1.6733,  1.1857, -0.2696, -1.8235, -1.5165, -0.1005,  0.4901,\n",
       "                       -0.6522, -0.9567, -0.5725, -0.8050],\n",
       "                      [ 1.2315, -0.9223,  0.3420, -1.9326,  0.0688,  1.0292,  0.5434, -0.1992,\n",
       "                       -0.6470,  0.4878,  0.5823,  1.9510, -0.5873,  0.3433,  0.5674,  0.7132,\n",
       "                       -0.5714,  0.3016,  1.6484,  0.5753]]))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.genre_embedding.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0051,  0.0125,  0.0166,  ...,  0.0075,  0.0118, -0.0066],\n",
       "                      [ 0.0081,  0.0166, -0.0081,  ...,  0.0043, -0.0017,  0.0057],\n",
       "                      [-0.0067, -0.0130,  0.0028,  ...,  0.0003, -0.0038,  0.0155],\n",
       "                      ...,\n",
       "                      [-0.0001, -0.0148, -0.0022,  ...,  0.0175, -0.0129, -0.0136],\n",
       "                      [ 0.0081, -0.0014,  0.0015,  ..., -0.0029, -0.0101, -0.0071],\n",
       "                      [-0.0125, -0.0109,  0.0078,  ...,  0.0142,  0.0116, -0.0080]])),\n",
       "             ('0.bias',\n",
       "              tensor([ 0.0112,  0.0117, -0.0118,  ..., -0.0012,  0.0024,  0.0057])),\n",
       "             ('3.weight',\n",
       "              tensor([[ 0.0188,  0.0092, -0.0285,  ..., -0.0025, -0.0029, -0.0098]])),\n",
       "             ('3.bias', tensor([-0.0025]))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.anaphoricity_scorer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '0.bias', '3.weight', '3.bias', '6.weight', '6.bias'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ffn.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '0.bias', '3.weight', '3.bias'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.anaphoricity_scorer.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
