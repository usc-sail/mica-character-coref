{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.movie_coref import baseline\n",
    "from mica_text_coref.coref.movie_coref import data\n",
    "from mica_text_coref.coref.movie_coref import rules\n",
    "from mica_text_coref.coref.movie_coref import split_and_merge\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "import itertools\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import tqdm\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lea(key: list[set[typing.Hashable]], response: list[set[typing.Hashable]]) -> tuple[float, float, float, float]:\n",
    "    key_importance = np.array([len(key_) for key_ in key])\n",
    "    response_importance = np.array([len(response_) for response_ in response])\n",
    "    assert np.all(key_importance > 0), \"Empty cluster in key\"\n",
    "    assert np.all(response_importance > 0), \"Empty cluster in response\"\n",
    "    intersection_counts = np.zeros((len(key), len(response)), dtype=int)\n",
    "    for i, key_ in enumerate(key):\n",
    "        for j, response_ in enumerate(response):\n",
    "            intersection_counts[i, j] = len(key_.intersection(response_))\n",
    "    link = intersection_counts * (intersection_counts - 1) / 2\n",
    "    singleton = (key_importance == 1).reshape(-1, 1) & (response_importance == 1).reshape(1, -1)\n",
    "    link[singleton] = intersection_counts[singleton]\n",
    "    key_link = np.maximum(key_importance * (key_importance - 1) / 2, 1)\n",
    "    response_link = np.maximum(response_importance * (response_importance - 1) / 2, 1)\n",
    "    recall_numer = (key_importance * link.sum(axis=1) / key_link).sum()\n",
    "    recall_denom = key_importance.sum()\n",
    "    precision_numer = (response_importance * link.sum(axis=0) / response_link).sum()\n",
    "    precision_denom = response_importance.sum()\n",
    "    return recall_numer, recall_denom, precision_numer, precision_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getenv(\"DATA_DIR\"), \"mica_text_coref/movie_coref/results/coreference/baselines\")\n",
    "input_dir = os.path.join(os.getenv(\"DATA_DIR\"), \"mica_text_coref/movie_coref/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:38<00:00, 13.14setting/s]\n"
     ]
    }
   ],
   "source": [
    "preprocess_arr = [\"none\", \"nocharacters\", \"addsays\"]\n",
    "genre_arr = [\"bc\", \"bn\", \"mz\", \"nw\", \"pt\", \"tc\", \"wb\"]\n",
    "entity_arr = [\"speaker\", \"person\", \"all\"]\n",
    "merge_speakers_arr = [False, True]\n",
    "provide_gold_mentions_arr = [False, True]\n",
    "remove_gold_singletons_arr = [False, True]\n",
    "header = [\"preprocess\", \"genre\", \"entity\", \"merge_speakers\", \"provide_gold_mentions\", \"remove_gold_singletons\",\n",
    "           \"movie\", \"precision\", \"recall\", \"f1\"]\n",
    "rows = []\n",
    "settings = itertools.product(preprocess_arr, genre_arr, entity_arr, merge_speakers_arr, provide_gold_mentions_arr,\n",
    "                             remove_gold_singletons_arr)\n",
    "n_settings = (len(preprocess_arr) * len(genre_arr) * len(entity_arr) * len(merge_speakers_arr)\n",
    "              * len(provide_gold_mentions_arr) * len(remove_gold_singletons_arr))\n",
    "for preprocess, genre, entity, merge_speakers, provide_gold_mentions, remove_gold_singletons in tqdm.tqdm(\n",
    "        settings, total=n_settings, unit=\"setting\"):\n",
    "    file_ = os.path.join(data_dir, f\"preprocess_{preprocess}.genre_{genre}.dev_wl.jsonlines\")\n",
    "    with jsonlines.open(file_, \"r\") as reader:\n",
    "        data = [doc for doc in reader]\n",
    "    scores = np.zeros(4)\n",
    "    for doc in data:\n",
    "        gold_clusters: list[set[tuple[int, int]]] = []\n",
    "        pred_clusters: list[set[tuple[int, int]]] = []\n",
    "        for cluster in doc[\"clusters\"].values():\n",
    "            gold_cluster = set((begin, end) for begin, end, head in cluster)\n",
    "            gold_clusters.append(gold_cluster)\n",
    "        for cluster in doc[\"span_clusters\"]:\n",
    "            pred_cluster = set((begin, end - 1) for begin, end in cluster)\n",
    "            pred_clusters.append(pred_cluster)\n",
    "\n",
    "        # Merge predicted clusters by speaker names\n",
    "        if merge_speakers:\n",
    "            pred_clusters = rules.merge_speakers(doc[\"token\"], doc[\"parse\"], pred_clusters)\n",
    "\n",
    "        # Filter predicted clusters by entity type\n",
    "        if entity == \"speaker\":\n",
    "            pred_clusters = rules.keep_speakers(doc[\"parse\"], pred_clusters)\n",
    "        elif entity == \"person\":\n",
    "            pred_clusters = rules.keep_persons(doc[\"ner\"], pred_clusters)\n",
    "\n",
    "        # Remove gold clusters containing single mention\n",
    "        if remove_gold_singletons:\n",
    "            gold_clusters = rules.remove_singleton_clusters(gold_clusters)\n",
    "\n",
    "        # Filter predicted mentions by gold mentions\n",
    "        if provide_gold_mentions:\n",
    "            gold_mentions = set([mention for cluster in gold_clusters for mention in cluster])\n",
    "            pred_clusters = rules.filter_mentions(gold_mentions, pred_clusters)\n",
    "        \n",
    "        # If preprocess == \"addsays\" or \"none\", remove spans from gold and pred clusters that overlap with a speaker\n",
    "        if preprocess == \"addsays\" or preprocess == \"none\":\n",
    "            parse_arr = np.array(doc[\"parse\"])\n",
    "            clusters_arr = []\n",
    "            for clusters in [gold_clusters, pred_clusters]:\n",
    "                clusters_ = []\n",
    "                for cluster in clusters:\n",
    "                    cluster_ = set()\n",
    "                    for begin, end in cluster:\n",
    "                        if np.all(parse_arr[begin: end + 1] != \"C\"):\n",
    "                            cluster_.add((begin, end))\n",
    "                    if cluster_:\n",
    "                        clusters_.append(cluster_)\n",
    "                clusters_arr.append(clusters_)\n",
    "            gold_clusters, pred_clusters = clusters_arr\n",
    "\n",
    "        # LEA\n",
    "        movie_scores = lea(gold_clusters, pred_clusters)\n",
    "        recall = movie_scores[0]/(movie_scores[1] + 1e-23)\n",
    "        precision = movie_scores[2]/(movie_scores[3] + 1e-23)\n",
    "        f1 = 2 * recall * precision / (recall + precision + 1e-23)\n",
    "        rows.append([preprocess, genre, entity, merge_speakers, provide_gold_mentions, remove_gold_singletons,\n",
    "                        doc[\"movie\"], precision, recall, f1])\n",
    "        scores += movie_scores\n",
    "    recall = scores[0]/(scores[1] + 1e-23)\n",
    "    precision = scores[2]/(scores[3] + 1e-23)\n",
    "    f1 = 2 * recall * precision / (recall + precision + 1e-23)\n",
    "    rows.append([preprocess, genre, entity, merge_speakers, provide_gold_mentions, remove_gold_singletons,\n",
    "                    \"all\", precision, recall, f1])\n",
    "dev_df = pd.DataFrame(rows, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2016, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocess</th>\n",
       "      <th>genre</th>\n",
       "      <th>entity</th>\n",
       "      <th>merge_speakers</th>\n",
       "      <th>provide_gold_mentions</th>\n",
       "      <th>remove_gold_singletons</th>\n",
       "      <th>movie</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>addsays</td>\n",
       "      <td>wb</td>\n",
       "      <td>speaker</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.653154</td>\n",
       "      <td>0.656889</td>\n",
       "      <td>0.655016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>addsays</td>\n",
       "      <td>mz</td>\n",
       "      <td>speaker</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.634836</td>\n",
       "      <td>0.661677</td>\n",
       "      <td>0.647979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>addsays</td>\n",
       "      <td>mz</td>\n",
       "      <td>speaker</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.644425</td>\n",
       "      <td>0.647715</td>\n",
       "      <td>0.646066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>addsays</td>\n",
       "      <td>wb</td>\n",
       "      <td>speaker</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.658277</td>\n",
       "      <td>0.633670</td>\n",
       "      <td>0.645739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>addsays</td>\n",
       "      <td>tc</td>\n",
       "      <td>speaker</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.627405</td>\n",
       "      <td>0.663864</td>\n",
       "      <td>0.645120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>nocharacters</td>\n",
       "      <td>bc</td>\n",
       "      <td>speaker</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.470970</td>\n",
       "      <td>0.205830</td>\n",
       "      <td>0.286465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>nocharacters</td>\n",
       "      <td>wb</td>\n",
       "      <td>speaker</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.511666</td>\n",
       "      <td>0.191656</td>\n",
       "      <td>0.278859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>nocharacters</td>\n",
       "      <td>wb</td>\n",
       "      <td>speaker</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.511666</td>\n",
       "      <td>0.191656</td>\n",
       "      <td>0.278859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>nocharacters</td>\n",
       "      <td>mz</td>\n",
       "      <td>speaker</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>0.193766</td>\n",
       "      <td>0.276275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>nocharacters</td>\n",
       "      <td>mz</td>\n",
       "      <td>speaker</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>all</td>\n",
       "      <td>0.481163</td>\n",
       "      <td>0.193766</td>\n",
       "      <td>0.276275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        preprocess genre   entity  merge_speakers  provide_gold_mentions  \\\n",
       "1939       addsays    wb  speaker            True                  False   \n",
       "1555       addsays    mz  speaker            True                  False   \n",
       "1539       addsays    mz  speaker           False                  False   \n",
       "1923       addsays    wb  speaker           False                  False   \n",
       "1843       addsays    tc  speaker            True                  False   \n",
       "...            ...   ...      ...             ...                    ...   \n",
       "691   nocharacters    bc  speaker            True                  False   \n",
       "1267  nocharacters    wb  speaker            True                  False   \n",
       "1251  nocharacters    wb  speaker           False                  False   \n",
       "867   nocharacters    mz  speaker           False                  False   \n",
       "883   nocharacters    mz  speaker            True                  False   \n",
       "\n",
       "      remove_gold_singletons movie  precision    recall        f1  \n",
       "1939                   False   all   0.653154  0.656889  0.655016  \n",
       "1555                   False   all   0.634836  0.661677  0.647979  \n",
       "1539                   False   all   0.644425  0.647715  0.646066  \n",
       "1923                   False   all   0.658277  0.633670  0.645739  \n",
       "1843                   False   all   0.627405  0.663864  0.645120  \n",
       "...                      ...   ...        ...       ...       ...  \n",
       "691                    False   all   0.470970  0.205830  0.286465  \n",
       "1267                   False   all   0.511666  0.191656  0.278859  \n",
       "1251                   False   all   0.511666  0.191656  0.278859  \n",
       "867                    False   all   0.481163  0.193766  0.276275  \n",
       "883                    False   all   0.481163  0.193766  0.276275  \n",
       "\n",
       "[126 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df[~dev_df[\"provide_gold_mentions\"] & ~dev_df[\"remove_gold_singletons\"] & (dev_df[\"movie\"] == \"all\")].sort_values(\n",
    "            by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df.to_csv(os.path.join(data_dir, \"excerpts.baseline.tsv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "none/bc/2048/128:avengers_endgame(19): 100%|██████████| 6/6 [00:23<00:00,  3.94s/it]\n",
      "none/bc/2048/128:dead_poets_society(14): 100%|██████████| 6/6 [00:12<00:00,  2.06s/it]\n",
      "none/bc/2048/128:john_wick(14): 100%|██████████| 6/6 [00:11<00:00,  1.94s/it]\n",
      "none/bc/2048/128:prestige(20): 100%|██████████| 6/6 [00:18<00:00,  3.00s/it]\n",
      "none/bc/2048/128:quiet_place(17): 100%|██████████| 6/6 [00:28<00:00,  4.67s/it]\n",
      "none/bc/2048/128:zootopia(15): 100%|██████████| 6/6 [00:12<00:00,  2.12s/it]\n",
      "none/bc/2048/256:avengers_endgame(21): 100%|██████████| 6/6 [00:39<00:00,  6.64s/it]\n",
      "none/bc/2048/256:dead_poets_society(15): 100%|██████████| 6/6 [00:22<00:00,  3.71s/it]\n",
      "none/bc/2048/256:john_wick(15): 100%|██████████| 6/6 [00:24<00:00,  4.10s/it]\n",
      "none/bc/2048/256:prestige(21): 100%|██████████| 6/6 [00:43<00:00,  7.22s/it]\n",
      "none/bc/2048/256:quiet_place(18):  33%|███▎      | 2/6 [00:15<00:30,  7.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224156414c4f4e227d/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m overlap_lens \u001b[39m=\u001b[39m [offsets[i][\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m offsets[i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_parts \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224156414c4f4e227d/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m strategy \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(strategy_arr, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpreprocess\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mgenre\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00msplit_len\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00moverlap_len\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224156414c4f4e227d/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m                                              \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmovie\u001b[39m}\u001b[39;00m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mn_parts\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224156414c4f4e227d/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     coref, ind \u001b[39m=\u001b[39m split_and_merge\u001b[39m.\u001b[39;49mcombine_coref_scores(corefs, inds, overlap_lens, strategy)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224156414c4f4e227d/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     word_clusters \u001b[39m=\u001b[39m baseline\u001b[39m.\u001b[39mclusterize(coref, ind)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224156414c4f4e227d/proj/sbaruah/mica_text_coref/coref/movie_coref/notebooks/baseline_lea.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     span_clusters \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/proj/sbaruah/mica_text_coref/coref/movie_coref/split_and_merge.py:174\u001b[0m, in \u001b[0;36mcombine_coref_scores\u001b[0;34m(corefs, inds, overlap_lens, strategy)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[39mfor\u001b[39;00m l, (h, s) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(head_to_score\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m    173\u001b[0m             coref[coref_start \u001b[39m+\u001b[39m j, l] \u001b[39m=\u001b[39m s\n\u001b[0;32m--> 174\u001b[0m             ind[coref_start \u001b[39m+\u001b[39m j, l] \u001b[39m=\u001b[39m h\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     coref[coref_start: coref_end, :k] \u001b[39m=\u001b[39m corefs[i][end:, \u001b[39m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_arr = [\"none\", \"nocharacters\", \"addsays\"]\n",
    "genre_arr = [\"bc\", \"bn\", \"mz\", \"nw\", \"pt\", \"tc\", \"wb\"]\n",
    "entity_arr = [\"speaker\", \"person\", \"all\"]\n",
    "merge_speakers_arr = [False, True]\n",
    "provide_gold_mentions_arr = [False, True]\n",
    "remove_gold_singletons_arr = [False, True]\n",
    "split_len_arr = [2048, 3072, 4096, 5120]\n",
    "overlap_len_arr = [128, 256, 512]\n",
    "strategy_arr = [\"none\", \"before\", \"after\", \"max\", \"min\", \"average\"]\n",
    "header = [\"preprocess\", \"genre\", \"entity\", \"merge_speakers\", \"provide_gold_mentions\", \"remove_gold_singletons\",\n",
    "          \"split_len\", \"overlap_len\", \"merge_strategy\", \"movie\", \"recall_numer\", \"recall_denom\", \"precision_numer\",\n",
    "          \"precision_denom\"]\n",
    "rows = []\n",
    "outer_settings = itertools.product(preprocess_arr, genre_arr, split_len_arr, overlap_len_arr)\n",
    "inner_settings = itertools.product(entity_arr, merge_speakers_arr, provide_gold_mentions_arr,\n",
    "                                   remove_gold_singletons_arr)\n",
    "n_outer_settings = len(preprocess_arr) * len(genre_arr) * len(split_len_arr) * len(overlap_len_arr)\n",
    "n_inner_settings = (len(entity_arr) * len(merge_speakers_arr) * len(provide_gold_mentions_arr)\n",
    "                    * len(remove_gold_singletons_arr))\n",
    "\n",
    "for preprocess, genre, split_len, overlap_len in outer_settings:\n",
    "    subdir = \"regular\" if preprocess == \"none\" else preprocess\n",
    "    input_file = os.path.join(input_dir, subdir, \"train_wl.jsonlines\")\n",
    "    data_file = os.path.join(data_dir, f\"preprocess_{preprocess}.genre_{genre}.split_{split_len}.overlap_{overlap_len}\"\n",
    "                                        \".train_wl\")\n",
    "    with jsonlines.open(data_file + \".jsonlines\") as reader:\n",
    "        pred_docs = {doc[\"document_id\"]: doc for doc in reader}\n",
    "    pt = torch.load(data_file + \".pt\", map_location=\"cpu\")\n",
    "    corpus = data.CorefCorpus(input_file)\n",
    "    gold_docs = {doc.movie: doc for doc in corpus}\n",
    "\n",
    "    movie_to_n_parts = collections.defaultdict(int)\n",
    "    for doc_id in pred_docs.keys():\n",
    "        match = re.match(r\"[a-z]{2}_(\\w+)_(\\d+)\", doc_id)\n",
    "        assert match is not None, \"Improperly formatted document id\"\n",
    "        movie = match.group(1)\n",
    "        part = int(match.group(2))\n",
    "        movie_to_n_parts[movie] = max(part, movie_to_n_parts[movie])\n",
    "    \n",
    "    # Loop over movie and parts\n",
    "    for movie, n_parts in movie_to_n_parts.items():\n",
    "\n",
    "        corefs, inds, offsets, head2span = [], [], [], {}\n",
    "        for i in range(1, n_parts + 1):\n",
    "            offset = pred_docs[f\"{genre}_{movie}_{i}\"][\"offset\"]\n",
    "            coref, ind, _head2span = baseline.get_scores_indices_heads(pt[f\"{genre}_{movie}_{i}\"], offset)\n",
    "            corefs.append(coref)\n",
    "            inds.append(ind)\n",
    "            offsets.append(offset)\n",
    "            head2span.update(_head2span)\n",
    "        overlap_lens = [offsets[i][1] - offsets[i + 1][0] for i in range(n_parts - 1)]\n",
    "\n",
    "        for strategy in tqdm.tqdm(strategy_arr, desc=f\"{preprocess}/{genre}/{split_len}/{overlap_len}:\"\n",
    "                                                     f\"{movie}({n_parts})\"):\n",
    "            coref, ind = split_and_merge.combine_coref_scores(corefs, inds, overlap_lens, strategy)\n",
    "            word_clusters = baseline.clusterize(coref, ind)\n",
    "            span_clusters = []\n",
    "            for cluster in word_clusters:\n",
    "                span_cluster = []\n",
    "                for head in cluster:\n",
    "                    if head in head2span:\n",
    "                        span_cluster.append(head2span[head])\n",
    "                if span_cluster:\n",
    "                    span_clusters.append(span_cluster)\n",
    "\n",
    "            gold_doc = gold_docs[movie]\n",
    "            gold_clusters_ = [set([(mention.begin, mention.end) for mention in mentions])\n",
    "                                for mentions in gold_doc.clusters.values()]\n",
    "            pred_clusters_ = [set([(i, j - 1) for i, j in cluster]) for cluster in span_clusters]\n",
    "\n",
    "            for entity, merge_speakers, provide_gold_mentions, remove_gold_singletons in inner_settings:\n",
    "                gold_clusters = copy.deepcopy(gold_clusters_)\n",
    "                pred_clusters = copy.deepcopy(pred_clusters_)\n",
    "\n",
    "                # Merge predicted clusters by speaker names\n",
    "                if merge_speakers:\n",
    "                    pred_clusters = rules.merge_speakers(gold_doc.token, gold_doc.parse, pred_clusters)\n",
    "\n",
    "                # Filter predicted clusters by entity type\n",
    "                if entity == \"speaker\":\n",
    "                    pred_clusters = rules.keep_speakers(gold_doc.parse, pred_clusters)\n",
    "                elif entity == \"person\":\n",
    "                    pred_clusters = rules.keep_persons(gold_doc.ner, pred_clusters)\n",
    "\n",
    "                # Remove gold clusters containing single mention\n",
    "                if remove_gold_singletons:\n",
    "                    gold_clusters = rules.remove_singleton_clusters(gold_clusters)\n",
    "\n",
    "                # Filter predicted mentions by gold mentions\n",
    "                if provide_gold_mentions:\n",
    "                    gold_mentions = set([mention for cluster in gold_clusters for mention in cluster])\n",
    "                    pred_clusters = rules.filter_mentions(gold_mentions, pred_clusters)\n",
    "                \n",
    "                # If preprocess == \"addsays\" or \"none\", remove spans from gold and pred clusters that overlap with a speaker\n",
    "                if preprocess == \"addsays\" or preprocess == \"none\":\n",
    "                    parse_arr = np.array(doc[\"parse\"])\n",
    "                    clusters_arr = []\n",
    "                    for clusters in [gold_clusters, pred_clusters]:\n",
    "                        clusters_ = []\n",
    "                        for cluster in clusters:\n",
    "                            cluster_ = set()\n",
    "                            for begin, end in cluster:\n",
    "                                if np.all(parse_arr[begin: end + 1] != \"C\"):\n",
    "                                    cluster_.add((begin, end))\n",
    "                            if cluster_:\n",
    "                                clusters_.append(cluster_)\n",
    "                        clusters_arr.append(clusters_)\n",
    "                    gold_clusters, pred_clusters = clusters_arr\n",
    "\n",
    "                # LEA\n",
    "                movie_scores = lea(gold_clusters, pred_clusters)\n",
    "                rows.append([preprocess, genre, entity, merge_speakers, provide_gold_mentions, remove_gold_singletons,\n",
    "                             split_len, overlap_len, strategy, movie] + list(movie_scores))\n",
    "train_df = pd.DataFrame(rows, columns=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coreference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6d6049dce941f60bb2eec2e35eb23f5ec878ebd724b7ecb8ba7d6ee7900594f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
