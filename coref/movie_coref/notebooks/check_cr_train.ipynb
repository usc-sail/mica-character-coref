{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.movie_coref.data import (CharacterRecognitionDataset, \n",
    "                                                    CorefCorpus)\n",
    "\n",
    "import accelerate\n",
    "from accelerate import logging\n",
    "import collections\n",
    "import contextlib\n",
    "import logging as pylogging\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from transformers import (get_linear_schedule_with_warmup, AutoTokenizer,\n",
    "                          AutoModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    \"\"\"General Metric class\"\"\"\n",
    "\n",
    "    @property\n",
    "    def score(self) -> float:\n",
    "        \"\"\"Main metric score used for comparison\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import logit\n",
    "\n",
    "\n",
    "class CharacterRecognition(nn.Module):\n",
    "    \"\"\"Character Recognition Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 encoder_name: str,\n",
    "                 num_parse_tags: int,\n",
    "                 parse_tag_embedding_size: int,\n",
    "                 gru_hidden_size: int,\n",
    "                 gru_num_layers: int,\n",
    "                 gru_dropout: float,\n",
    "                 gru_bidirectional: bool,\n",
    "                 num_labels: int) -> None:\n",
    "        \"\"\"Initializer for Character Recognition Model.\n",
    "\n",
    "        Args:\n",
    "            encoder_name: Language model encoder name from transformers hub\n",
    "                e.g. bert-base-cased\n",
    "            num_parse_tags: Parse tag set size\n",
    "            parse_tag_embedding_size: Embedding size of the parse tags\n",
    "            gru_hidden_size: Hidden size of the GRU\n",
    "            gru_num_layers: Number of layers of the GRU\n",
    "            gru_dropout: Dropout used between the GPU layers\n",
    "            gru_bidirectional: If true, the GRU is bidirectional\n",
    "            num_labels: Number of labels in the label set. 2 if label_type =\n",
    "                \"head\" or 3 if label_type = \"span\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            encoder_name, add_pooling_layer=False)\n",
    "        self.encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        self.subtoken = nn.Linear(self.encoder_hidden_size, 1)\n",
    "        self.parse_embedding = nn.Embedding(\n",
    "            num_parse_tags, parse_tag_embedding_size)\n",
    "        self.gru_input_size = (self.encoder_hidden_size +\n",
    "                               parse_tag_embedding_size)\n",
    "        self.gru_output_size = gru_hidden_size * (1 + int(gru_bidirectional))\n",
    "        self.gru = nn.GRU(self.gru_input_size, gru_hidden_size,\n",
    "                          num_layers=gru_num_layers, batch_first=True,\n",
    "                          dropout=gru_dropout, bidirectional=gru_bidirectional)\n",
    "        self.output = nn.Linear(self.gru_output_size, num_labels)\n",
    "        self._device = \"cpu\"\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"Getter for model device.\"\"\"\n",
    "        return self._device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, device):\n",
    "        \"\"\"Setter for model device. Used by accelerate.\"\"\"\n",
    "        self._device = device\n",
    "    \n",
    "    def forward(self, subtoken_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                token_offset: torch.Tensor, parse_ids: torch.Tensor,\n",
    "                labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward propagation for the Character Recognition Model.\n",
    "\n",
    "        Args:\n",
    "            subtoken_ids: `batch_size x max_n_subtokens` Long Tensor\n",
    "            attention_mask: `batch_size x max_n_subtokens` Float/Long Tensor\n",
    "            token_offset: `batch_size x max_n_tokens x 2` Long Tensor\n",
    "            parse_ids: `batch_size x max_n_tokens` Long Tensor\n",
    "            labels: `batch_size x max_n_tokens` Long Tensor\n",
    "        \n",
    "        Returns:\n",
    "            Return the loss value if model is begin trained, else the logits \n",
    "            `batch_size x max_n_tokens x num_labels` Float Tensor\n",
    "        \"\"\"\n",
    "        batch_size = len(subtoken_ids)\n",
    "\n",
    "        # subtoken_embedding = batch_size x max_n_subtokens x encoder_hidden_size\n",
    "        encoder_output = self.encoder(subtoken_ids, attention_mask)\n",
    "        subtoken_embedding = encoder_output.last_hidden_state\n",
    "\n",
    "        # _subtoken_embedding = batch_size * max_n_subtokens x encoder_hidden_size\n",
    "        # subtoken_attn = batch_size * max_n_tokens x batch_size * max_n_subtokens\n",
    "        _subtoken_embedding = subtoken_embedding.view(-1, self.encoder_hidden_size)\n",
    "        subtoken_attn = self._attn_scores(_subtoken_embedding,\n",
    "                                          token_offset.view(-1, 2))\n",
    "        \n",
    "        # token_embedding = batch_size x max_n_tokens x encoder_hidden_size\n",
    "        token_embedding = torch.mm(\n",
    "            subtoken_attn, _subtoken_embedding).reshape(\n",
    "                batch_size, -1, self.encoder_hidden_size)\n",
    "        \n",
    "        # gru_input = batch_size x max_n_tokens x (encoder_hidden_size +\n",
    "        # parse_tag_embedding_size)\n",
    "        parse_input = self.parse_embedding(parse_ids)\n",
    "        gru_input = torch.cat((token_embedding, parse_input), dim=2).contiguous()\n",
    "\n",
    "        # logits = batch_size x max_n_tokens x num_labels\n",
    "        gru_output, _ = self.gru(gru_input)\n",
    "        logits = self.output(gru_output)\n",
    "\n",
    "        # TODO = remove this\n",
    "        nz = (logits[:,:,1] > logits[:,:,0]).sum().item()\n",
    "        print(f\"Inside Model: logits sum = {nz}\")\n",
    "\n",
    "        if self.training:\n",
    "            token_attention_mask = torch.any(subtoken_attn > 0, dim=1).reshape(\n",
    "                batch_size, -1)\n",
    "            loss = compute_loss(logits, labels, token_attention_mask,\n",
    "                                self.num_labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def _attn_scores(self,\n",
    "                     subtoken_embeddings: torch.FloatTensor,\n",
    "                     token_offset: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\" Calculates attention scores for each of the subtokens of a token.\n",
    "\n",
    "        Args:\n",
    "            subtoken_embedding: `n_subtokens x embedding_size` Float Tensor,\n",
    "                embeddings for each subtoken\n",
    "            token_offset: `n_tokens x 2` Long Tensor, subtoken offset of each\n",
    "                token\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: `n_tokens x n_subtokens` Float Tensor, attention\n",
    "            weights for each subtoken of a token\n",
    "        \"\"\"\n",
    "        n_subtokens, n_tokens = len(subtoken_embeddings), len(token_offset)\n",
    "        token_begin, token_end = token_offset[:,0], token_offset[:,1]\n",
    "        \n",
    "        # attn_mask: n_tokens x n_subtokens, contains -∞ for subtokens outside\n",
    "        # the token's offsets and 0 for subtokens inside the token's offsets\n",
    "        attn_mask = torch.arange(0, n_subtokens, device=self.device).expand(\n",
    "            (n_tokens, n_subtokens))\n",
    "        attn_mask = ((attn_mask >= token_begin.unsqueeze(1)) * \n",
    "                     (attn_mask <= token_end.unsqueeze(1)))\n",
    "        attn_mask = torch.log(attn_mask.to(torch.float))\n",
    "\n",
    "        # attn_scores: 1 x n_subtokens\n",
    "        attn_scores = self.subtoken(subtoken_embeddings).T\n",
    "\n",
    "        # attn_scores: n_tokens x n_subtokens\n",
    "        attn_scores = attn_scores.expand((n_tokens, n_subtokens))\n",
    "\n",
    "        # -∞ for subtokens outside the token's offsets and attn_scores for\n",
    "        # inside the token's offsets\n",
    "        attn_scores = attn_mask + attn_scores\n",
    "        del attn_mask\n",
    "\n",
    "        # subtoken_attn contains 0 for subtokens outside the token's offsets\n",
    "        subtoken_attn = torch.softmax(attn_scores, dim=1)\n",
    "        return subtoken_attn\n",
    "    \n",
    "def compute_loss(\n",
    "    logits: torch.FloatTensor, label_ids: torch.LongTensor,\n",
    "    attn_mask: torch.FloatTensor, n_labels: int) -> torch.FloatTensor:\n",
    "    \"\"\"Compute cross entropy loss\"\"\"\n",
    "    active_labels = label_ids[attn_mask == 1.]\n",
    "    active_logits = logits.flatten(0, 1)[attn_mask.flatten() == 1.]\n",
    "    label_distribution = torch.bincount(active_labels, minlength=n_labels)\n",
    "    class_weight = len(active_labels)/(1 + label_distribution)\n",
    "    print(class_weight)\n",
    "    cross_entrop_loss_fn = nn.CrossEntropyLoss(\n",
    "        weight=class_weight, reduction=\"mean\")\n",
    "    loss = cross_entrop_loss_fn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"General trainer class that uses huggingface \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                accelerator: accelerate.Accelerator,\n",
    "                logger: logging.MultiProcessAdapter,\n",
    "                model: nn.Module,\n",
    "                train_dataloader: DataLoader,\n",
    "                dev_dataloader: DataLoader,\n",
    "                optimizer: Optimizer,\n",
    "                use_scheduler: bool = False,\n",
    "                warmup_ratio: float = None,\n",
    "                warmup_steps: int = None,\n",
    "                max_epochs: int = 1,\n",
    "                max_grad_norm: float = None,\n",
    "                patience: int = 1,\n",
    "                log_batch_frequency: int = 1,\n",
    "                evaluate_train: bool = False,\n",
    "                save_model: bool = False,\n",
    "                save_tensors: bool = False,\n",
    "                save_tensors_name: list[str] = None,\n",
    "                save_dir: str = None\n",
    "                ) -> None:\n",
    "        \"\"\"Initializer for the general trainer class that uses accelerate to\n",
    "        train your model.\n",
    "\n",
    "        Args:\n",
    "            accelerator: Instance of the Accelerator class.\n",
    "            logger: Instance of the Accelerator logger used for distributed\n",
    "                logging.\n",
    "            model: Torch nn.Module subclass to train.\n",
    "            train_dataloader: Train set dataloader.\n",
    "            dev_dataloader: Dev set dataloader.\n",
    "            optimizer: Optimizer.\n",
    "            use_scheduler: Whether to use scheduler.\n",
    "            warmup_ratio: Ratio of training steps to use in the scheduler's\n",
    "                warmup. warmup_steps has to be None if you want to use this\n",
    "                parameter.\n",
    "            warmup_steps: Number of steps to use in the scheduler's warmup.\n",
    "                This parameter supercedes warmup_ratio.\n",
    "            max_epochs: Maximum number of epochs to train for.\n",
    "            max_grad_norm: Maximum norm of gradient to be used in gradient\n",
    "                clipping. If None, gradient clipping is not done.\n",
    "            patience: Maximum number of epochs to wait for development set\n",
    "                performance to improve before early-stopping.\n",
    "            log_batch_frequency: Training loop logs training loss and timing\n",
    "                information after every log_batch_frequency batches.\n",
    "            evaluate_train: Whether to evaluate on the training set.\n",
    "            save_model: Whether to save model after every epoch.\n",
    "            save_tensors: Whether to save the tensors of development set, along\n",
    "                with the logits.\n",
    "            save_tensors_name: List of tensor names which are to be saved. If\n",
    "                none, all tensors are saved.\n",
    "            save_dir: Directory to which the model weights, ground truth, and\n",
    "                predictions will be saved.\n",
    "        \"\"\"\n",
    "        self.accelerator = accelerator\n",
    "        self.logger = logger\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.dev_dataloader = dev_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_epochs = max_epochs\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.patience = patience\n",
    "        self.log_batch_frequency = log_batch_frequency\n",
    "        self.evaluate_train = evaluate_train\n",
    "        self.save_model = save_model\n",
    "        self.save_tensors = save_tensors\n",
    "        self.save_tensors_name = save_tensors_name\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            assert (self.warmup_ratio is not None or \n",
    "                    self.warmup_steps is not None), (\n",
    "                    \"Set warmup_ratio or warmup_steps \"\n",
    "                    \"if you are using scheduler\")\n",
    "        \n",
    "        if self.save_model or self.save_predictions:\n",
    "            assert self.save_dir is not None, (\n",
    "                \"Set save_dir if you are saving model and/or predictions\")\n",
    "        \n",
    "        self.n_training_samples = len(self.train_dataloader.dataset)\n",
    "        self.n_dev_samples = len(self.dev_dataloader.dataset)\n",
    "        self.model.eval()\n",
    "        self.model.device = self.accelerator.device\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Logging or printing\"\"\"\n",
    "        self.accelerator.print(message)\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def _timer(self, message):\n",
    "        \"\"\"Context manager for timing a codeblock\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.log(f\"Starting {message}\")\n",
    "        yield\n",
    "        time_taken = time.time() - start_time\n",
    "        time_taken_str = self._convert_float_seconds_to_time_string(time_taken)\n",
    "        self.log(f\"{message} done, time taken = {time_taken_str}\")\n",
    "\n",
    "    def _convert_float_seconds_to_time_string(self, seconds: float) -> str:\n",
    "        \"\"\"Convert seconds to h m s format\"\"\"\n",
    "        seconds = int(seconds)\n",
    "        minutes, seconds = seconds//60, seconds%60\n",
    "        hours, minutes = minutes//60, minutes%60\n",
    "        return f\"{hours}h {minutes}m {seconds}s\"\n",
    "    \n",
    "    def _save_model(self, model: nn.Module, directory: str):\n",
    "        \"\"\"Save model's weights to directory with filename `model.pt`.\n",
    "\n",
    "        Args:\n",
    "            model: Torch nn.Module.\n",
    "            directory: Directory where model's weights will be saved.\n",
    "        \"\"\"\n",
    "        self.accelerator.save(\n",
    "            model.state_dict(), os.path.join(directory, \"model.pt\"))\n",
    "\n",
    "    def _save_tensors(self, directory: str, **tensors):\n",
    "        \"\"\"Save the tensors returned from inference to directory.\n",
    "\n",
    "        Args:\n",
    "            directory: Directory where the tensors will be saved.\n",
    "            tensors: Dictionary of tensor name to tensor.\n",
    "        \"\"\"\n",
    "        for name, pt in tensors.items():\n",
    "            if self.save_tensors_name is None or name in self.save_tensors_name:\n",
    "                self.accelerator.save(pt, os.path.join(directory, f\"{name}.pt\"))\n",
    "    \n",
    "    def run(self):\n",
    "        best_dev_score = None\n",
    "        best_epoch = None\n",
    "        epochs_left = self.patience\n",
    "        save = self.save_model or self.save_tensors\n",
    "        \n",
    "        # Accelerate model, dataloaders, and optimizer\n",
    "        (self.model, self.train_dataloader, self.dev_dataloader,\n",
    "         self.optimizer) = (self.accelerator.prepare(\n",
    "            self.model, self.train_dataloader, self.dev_dataloader, \n",
    "            self.optimizer))\n",
    "\n",
    "        # Log number of training and inference batches, \n",
    "        # and number of training steps\n",
    "        n_train_batches = len(self.train_dataloader)\n",
    "        n_dev_batches = len(self.dev_dataloader)\n",
    "        effective_train_batch_size = round(\n",
    "            self.n_training_samples/n_train_batches)\n",
    "        effective_dev_batch_size = round(self.n_dev_samples/n_dev_batches)\n",
    "        n_training_steps = self.max_epochs * n_train_batches\n",
    "        self.log(\"Effective train batch size = \"\n",
    "                        f\"{effective_train_batch_size}\")\n",
    "        self.log(\"Effective dev batch size = \"\n",
    "                        f\"{effective_dev_batch_size}\")\n",
    "        self.log(f\"Number of training batches = {n_train_batches}\")\n",
    "        self.log(f\"Number of inference batches = {n_dev_batches}\")\n",
    "        self.log(f\"Number of training steps = {n_training_steps}\")\n",
    "\n",
    "        # Initialize and accelerate scheduler\n",
    "        if self.use_scheduler:\n",
    "            n_warmup_steps = self.warmup_steps if (\n",
    "                self.warmup_steps is not None) else (\n",
    "                    int(self.warmup_ratio * n_training_steps))\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer, num_warmup_steps=n_warmup_steps,\n",
    "                num_training_steps=n_training_steps)\n",
    "            scheduler = self.accelerator.prepare_scheduler(scheduler)\n",
    "            self.log(f\"Number of warmup steps = {n_warmup_steps}\")\n",
    "        \n",
    "        # Training and evaluation loop\n",
    "        with self._timer(\"training\"):\n",
    "            for epoch in range(self.max_epochs):\n",
    "                \n",
    "                if save:\n",
    "                    # Create epoch directories\n",
    "                    epoch_dir = os.path.join(\n",
    "                        self.save_dir, f\"epoch_{epoch + 1}\")\n",
    "                    epoch_dev_dir = os.path.join(epoch_dir, \"dev\")\n",
    "                    os.makedirs(epoch_dir, exist_ok=True)\n",
    "                    os.makedirs(epoch_dev_dir, exist_ok=True)\n",
    "\n",
    "                # Training for one epoch\n",
    "                with self._timer(f\"epoch {epoch + 1} training\"):\n",
    "                    self.model.train()\n",
    "                    running_batch_loss = []\n",
    "                    running_batch_train_time = []\n",
    "                    \n",
    "                    # Batch training loop\n",
    "                    for i, batch in enumerate(self.train_dataloader):\n",
    "                        batch_start_time = time.time()\n",
    "                        \n",
    "                        # One training step\n",
    "                        with self.accelerator.accumulate(self.model):\n",
    "                            self.optimizer.zero_grad()\n",
    "                            with self.accelerator.autocast():\n",
    "                                batch_loss = self.model(**batch)\n",
    "                            self.accelerator.backward(batch_loss)\n",
    "                            if self.optimizer.gradient_state.sync_gradients and self.max_grad_norm is not None:\n",
    "                                self.accelerator.clip_grad_norm_(\n",
    "                                    self.model.parameters(), self.max_grad_norm)\n",
    "                            self.optimizer.step()\n",
    "                            if self.use_scheduler and (\n",
    "                            not self.accelerator.optimizer_step_was_skipped):\n",
    "                                scheduler.step()\n",
    "\n",
    "                        batch_time_taken = time.time() - batch_start_time\n",
    "                        running_batch_loss.append(batch_loss.detach().item())\n",
    "                        running_batch_train_time.append(batch_time_taken)\n",
    "\n",
    "                        # Log after log_batch_frequency batches\n",
    "                        if (i + 1) % self.log_batch_frequency == 0:\n",
    "                            average_batch_loss = np.mean(running_batch_loss)\n",
    "                            average_batch_train_time = np.mean(\n",
    "                                running_batch_train_time)\n",
    "                            estimated_time_remaining = (\n",
    "                                self._convert_float_seconds_to_time_string(\n",
    "                                average_batch_train_time * (\n",
    "                                    n_train_batches-i-1)))\n",
    "                            average_batch_train_time_str = (\n",
    "                                self._convert_float_seconds_to_time_string(\n",
    "                                average_batch_train_time))\n",
    "                            self.log(f\"Batch {i + 1}\")\n",
    "                            self.log(\n",
    "                                \"Average training loss @ batch = \"\n",
    "                                f\"{average_batch_loss:.4f}\")\n",
    "                            self.log(\n",
    "                                \"Average training time taken @ batch = \"\n",
    "                                f\"{average_batch_train_time_str}\")\n",
    "                            self.log(\n",
    "                                \"Estimated training time remaining for epoch \"\n",
    "                                f\"{epoch + 1} = {estimated_time_remaining}\")\n",
    "                            running_batch_loss = []\n",
    "                            running_batch_train_time = []\n",
    "\n",
    "                # Wait for all process to complete\n",
    "                self.accelerator.wait_for_everyone()\n",
    "\n",
    "                # Save model\n",
    "                if self.save_model:\n",
    "                    self.log(f\"Saving model after epoch {epoch + 1}\")\n",
    "                    unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "                    self._save_model(unwrapped_model, epoch_dir)\n",
    "\n",
    "                # Inference and evaluation on training set\n",
    "                if self.evaluate_train:\n",
    "                    with self._timer(\n",
    "                        f\"epoch {epoch + 1} training inference and evaluation\"):\n",
    "                        train_inference_output = self._infer(\n",
    "                            self.train_dataloader, self.model)\n",
    "                        train_metric = self.evaluate(**train_inference_output)\n",
    "                        self.log(\n",
    "                            f\"Training Performance = {train_metric.score}\")\n",
    "                    self.accelerator.wait_for_everyone()\n",
    "            \n",
    "                # Inference and evaluation on dev set\n",
    "                with self._timer(\n",
    "                    f\"epoch {epoch + 1} dev inference and evaluation\"):\n",
    "                    dev_inference_output = self._infer(\n",
    "                        self.dev_dataloader, self.model)\n",
    "                    dev_metric = self.evaluate(**dev_inference_output)\n",
    "                    self.log(f\"Dev Performance = {dev_metric.score}\")\n",
    "                self.accelerator.wait_for_everyone()\n",
    "                if self.save_tensors:\n",
    "                    self.log(\n",
    "                        f\"Saving dev tensors after epoch {epoch + 1}\")\n",
    "                    self._save_tensors(epoch_dev_dir, **dev_inference_output)\n",
    "\n",
    "                # Early-stopping\n",
    "                self.log(\"Checking for early-stopping\")\n",
    "                dev_score = dev_metric.score\n",
    "                if best_dev_score is None or dev_score > best_dev_score:\n",
    "                    epochs_left = self.patience\n",
    "                    best_epoch = epoch + 1\n",
    "                    if best_dev_score is not None:\n",
    "                        delta = 100 * (dev_score - best_dev_score)\n",
    "                        self.log(f\"Dev score improved by {delta:.1f}\")\n",
    "                    best_dev_score = dev_score\n",
    "                else:\n",
    "                    epochs_left -= 1\n",
    "                    delta = 100 * (best_dev_score - dev_score)\n",
    "                    self.log(\n",
    "                        f\"Dev score is {delta:.1f} lower than best Dev score \"\n",
    "                        f\"({100*best_dev_score:.1f})\")\n",
    "                    self.log(\n",
    "                        f\"{epochs_left} epochs left until Dev score to improve to\"\n",
    "                        \" avoid early-stopping!\")\n",
    "                if epochs_left == 0:\n",
    "                    self.log(\"Early stopping!\")\n",
    "                    break\n",
    "\n",
    "                self.log(f\"Epoch {epoch + 1} done\")\n",
    "\n",
    "        self.log(f\"Best Dev score = {100*best_dev_score:.1f}\")\n",
    "        self.log(f\"Best epoch = {best_epoch}\")\n",
    "    \n",
    "    def _infer(self, dataloader: DataLoader, model: nn.Module) -> (\n",
    "        dict[str, torch.Tensor]):\n",
    "        \"\"\"Run inference on the dataloader.\n",
    "        Args:\n",
    "            dataloader: PyTorch dataloader.\n",
    "            model: PyTorch module.\n",
    "        \n",
    "        Returns:\n",
    "            Labels and predictions tensors.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        model.eval()\n",
    "        tensors: dict[str, list[torch.Tensor]] = collections.defaultdict(list)\n",
    "        n_batches = len(dataloader)\n",
    "        self.log(f\"Number of inference batches = {n_batches}\")\n",
    "\n",
    "        # Inference Loop\n",
    "        with self._timer(\"inference\"), torch.no_grad():\n",
    "            running_batch_times = []\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # One inference step\n",
    "                start_time = time.time()\n",
    "                batch_logits = model(**batch)\n",
    "                batch_logits = self.accelerator.gather_for_metrics(batch_logits)\n",
    "                batch = self.accelerator.gather_for_metrics(batch)\n",
    "                batch[\"logits\"] = batch_logits\n",
    "                nz = (batch_logits[:,:,1] > batch_logits[:,:,0]).sum().item()\n",
    "                self.log(f\"Outside Model: logits sum = {nz}\")\n",
    "                for name, tensor in batch.items():\n",
    "                    tensors[name].append(tensor)\n",
    "                time_taken = time.time() - start_time\n",
    "                running_batch_times.append(time_taken)\n",
    "\n",
    "                # Log after log_batch_frequency batches\n",
    "                if (i + 1) % self.log_batch_frequency == 0:\n",
    "                    average_time_per_batch = np.mean(running_batch_times)\n",
    "                    estimated_time_remaining = (n_batches - i - 1) * (\n",
    "                                                average_time_per_batch)\n",
    "                    average_time_per_batch_str = (\n",
    "                        self._convert_float_seconds_to_time_string(\n",
    "                            average_time_per_batch))\n",
    "                    estimated_time_remaining_str = (\n",
    "                        self._convert_float_seconds_to_time_string(\n",
    "                            estimated_time_remaining))\n",
    "                    running_batch_times = []\n",
    "\n",
    "                    self.log(f\"Batch {i + 1}\")\n",
    "                    self.log(\"Average inference time @ batch = \"\n",
    "                                f\"{average_time_per_batch_str}\")\n",
    "                    self.log(\"Estimated inference time remaining = \"\n",
    "                                f\"{estimated_time_remaining_str}\")\n",
    "\n",
    "        # Concat tensors\n",
    "        output: dict[str, torch.Tensor] = {}\n",
    "        for name, tensor_list in tensors.items():\n",
    "            output[name] = torch.cat(tensor_list, dim=0)\n",
    "        return output\n",
    "    \n",
    "    def evaluate(self, **tensors) -> Metric:\n",
    "        \"\"\"Evaluate the output of inference\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Recognition - Metric & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRecognitionMetric(Metric):\n",
    "\n",
    "    def __init__(self, precision, recall) -> None:\n",
    "        super().__init__()\n",
    "        self.precision = precision\n",
    "        self.recall = recall\n",
    "    \n",
    "    @property\n",
    "    def score(self) -> float:\n",
    "        return 2*self.precision*self.recall/(self.precision+self.recall+1e-23)\n",
    "\n",
    "class CharacterRecognitionTrainer(Trainer):\n",
    "\n",
    "    def evaluate(self, **tensors) -> Metric:\n",
    "        logits, labels, offset = (\n",
    "            tensors[\"logits\"], tensors[\"labels\"], tensors[\"token_offset\"])\n",
    "        pred = logits.argmax(dim=2)\n",
    "        mask = ~((offset[:,:,0] == 0) & (offset[:,:,1] == 0))\n",
    "        pred = pred[mask]\n",
    "        labels = labels[mask]\n",
    "        tp = ((labels == pred) & (pred != 0)).sum().item()\n",
    "        fp = ((labels != pred) & (pred != 0)).sum().item()\n",
    "        fn = ((labels != pred) & (labels != 0)).sum().item()\n",
    "        precision = tp/(tp + fp + 1e-23)\n",
    "        recall = tp/(tp + fn + 1e-23)\n",
    "        return CharacterRecognitionMetric(precision, recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training w/ Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    train_batch_size = 16\n",
    "    infer_batch_size = 16\n",
    "    parse_tag_embedding_size = 32\n",
    "    encoder_name = \"roberta-base\"\n",
    "    gru_hidden_size = 768\n",
    "    gru_num_layers = 2\n",
    "    gru_dropout = 0.2\n",
    "    gru_bidirectional = True\n",
    "    lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    use_scheduler = True\n",
    "    warmup_ratio = 0.1\n",
    "    warmup_steps = None\n",
    "    max_epochs = 5\n",
    "    max_grad_norm = 0.1\n",
    "    patience = 3\n",
    "    log_batch_frequency = 5\n",
    "    evaluate_train = False\n",
    "    save_model = True\n",
    "    save_tensors = True\n",
    "    # save_tensors_name = [\"logits\", \"labels\"]\n",
    "    save_tensors_name = None\n",
    "    save_dir = (\"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/\"\n",
    "                \"character_recognition/\")\n",
    "\n",
    "    accelerator = accelerate.Accelerator(mixed_precision=\"fp16\")\n",
    "    logger = logging.get_logger(\"\")\n",
    "\n",
    "    corpus = CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                        \"movie_coref/results/regular/movie.jsonlines\")\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(encoder_name, use_fast=True)\n",
    "    dataset = CharacterRecognitionDataset(\n",
    "        corpus, roberta_tokenizer, seq_length=256, obey_scene_boundaries=False)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset, batch_size=infer_batch_size)\n",
    "\n",
    "    model = CharacterRecognition(encoder_name=encoder_name,\n",
    "                    num_parse_tags=len(dataset.parse_tag_to_id),\n",
    "                    parse_tag_embedding_size=parse_tag_embedding_size,\n",
    "                    gru_hidden_size=gru_hidden_size,\n",
    "                    gru_num_layers=gru_num_layers,\n",
    "                    gru_dropout=gru_dropout,\n",
    "                    gru_bidirectional=gru_bidirectional,\n",
    "                    num_labels=2)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    trainer = CharacterRecognitionTrainer(\n",
    "        accelerator,\n",
    "        logger,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        optimizer,\n",
    "        use_scheduler = use_scheduler,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        warmup_steps = warmup_steps,\n",
    "        max_epochs = max_epochs,\n",
    "        max_grad_norm = max_grad_norm,\n",
    "        patience = patience,\n",
    "        log_batch_frequency = log_batch_frequency,\n",
    "        evaluate_train = evaluate_train,\n",
    "        save_model = save_model,\n",
    "        save_tensors = save_tensors,\n",
    "        save_tensors_name = save_tensors_name,\n",
    "        save_dir = save_dir)\n",
    "    \n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective train batch size = 61\n",
      "Effective dev batch size = 61\n",
      "Number of training batches = 13\n",
      "Number of inference batches = 13\n",
      "Number of training steps = 65\n",
      "Number of warmup steps = 6\n",
      "Starting training\n",
      "Starting epoch 1 training\n",
      "Inside Model: logits sum = 2632\n",
      "Inside Model: logits sum = 1680\n",
      "tensor([1.1784, 6.5852], device='cuda:2')\n",
      "tensor([1.1384, 8.1920], device='cuda:3')\n",
      "Inside Model: logits sum = 2633\n",
      "tensor([1.1419, 8.0157], device='cuda:1')\n",
      "Inside Model: logits sum = 1927\n",
      "tensor([1.1337, 8.4454], device='cuda:0')\n",
      "Inside Model: logits sum = 1680Inside Model: logits sum = 2498\n",
      "\n",
      "Inside Model: logits sum = 1893\n",
      "tensor([1.1554, 7.4069], device='cuda:3')tensor([1.1686, 6.9073], device='cuda:1')\n",
      "\n",
      "tensor([1.1496, 7.6561], device='cuda:0')\n",
      "Inside Model: logits sum = 2195\n",
      "tensor([1.1343, 8.4107], device='cuda:2')\n",
      "Inside Model: logits sum = 1733\n",
      "Inside Model: logits sum = 1361\n",
      "tensor([1.1397, 8.1270], device='cuda:2')\n",
      "tensor([1.1525, 7.5294], device='cuda:0')\n",
      "Inside Model: logits sum = 1749Inside Model: logits sum = 1354\n",
      "\n",
      "tensor([1.1419, 8.0157], device='cuda:1')tensor([1.1409, 8.0630], device='cuda:3')\n",
      "\n",
      "Inside Model: logits sum = 2356Inside Model: logits sum = 1953\n",
      "\n",
      "Inside Model: logits sum = 1678\n",
      "tensor([1.1636, 7.0865], device='cuda:3')tensor([1.1438, 7.9226], device='cuda:1')\n",
      "\n",
      "tensor([1.1416, 8.0314], device='cuda:0')\n",
      "Inside Model: logits sum = 1854\n",
      "tensor([1.1541, 7.4608], device='cuda:2')\n",
      "Inside Model: logits sum = 2111\n",
      "Inside Model: logits sum = 2057tensor([1.1381, 8.2084], device='cuda:0')\n",
      "\n",
      "tensor([1.1574, 7.3274], device='cuda:2')\n",
      "Inside Model: logits sum = 1891\n",
      "Inside Model: logits sum = 2429\n",
      "tensor([1.1416, 8.0314], device='cuda:3')tensor([1.1432, 7.9534], device='cuda:1')\n",
      "\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.6934\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 1 = 0h 0m 7s\n",
      "Inside Model: logits sum = 1646\n",
      "Inside Model: logits sum = 1895\n",
      "Inside Model: logits sum = 1551tensor([1.1528, 7.5156], device='cuda:1')\n",
      "\n",
      "tensor([1.1315, 8.5690], device='cuda:0')\n",
      "tensor([1.1353, 8.3592], device='cuda:3')\n",
      "Inside Model: logits sum = 1967\n",
      "tensor([1.1365, 8.2915], device='cuda:2')\n",
      "Inside Model: logits sum = 1989\n",
      "tensor([1.1281, 8.7709], device='cuda:2')Inside Model: logits sum = 1465\n",
      "\n",
      "tensor([1.1222, 9.1429], device='cuda:0')\n",
      "Inside Model: logits sum = 2202\n",
      "Inside Model: logits sum = 2370\n",
      "tensor([1.1445, 7.8921], device='cuda:3')\n",
      "tensor([1.1660, 7.0017], device='cuda:1')\n",
      "Inside Model: logits sum = 2100Inside Model: logits sum = 1464Inside Model: logits sum = 2345\n",
      "\n",
      "\n",
      "tensor([1.1340, 8.4280], device='cuda:0')tensor([1.1567, 7.3537], device='cuda:1')tensor([1.1650, 7.0378], device='cuda:3')\n",
      "\n",
      "\n",
      "Inside Model: logits sum = 1675\n",
      "tensor([1.1290, 8.7149], device='cuda:2')\n",
      "Inside Model: logits sum = 1926\n",
      "tensor([1.1528, 7.5156], device='cuda:2')\n",
      "Inside Model: logits sum = 1963\n",
      "tensor([1.1506, 7.6134], device='cuda:0')\n",
      "Inside Model: logits sum = 2178Inside Model: logits sum = 2180\n",
      "\n",
      "tensor([1.1561, 7.3802], device='cuda:3')tensor([1.1413, 8.0472], device='cuda:1')\n",
      "\n",
      "Inside Model: logits sum = 1640Inside Model: logits sum = 1778\n",
      "\n",
      "tensor([1.1541, 7.4608], device='cuda:0')Inside Model: logits sum = 1409tensor([1.1397, 8.1270], device='cuda:3')\n",
      "\n",
      "\n",
      "tensor([1.1403, 8.0949], device='cuda:1')Inside Model: logits sum = 1954\n",
      "\n",
      "tensor([1.1740, 6.7258], device='cuda:2')\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.6924\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 1 = 0h 0m 2s\n",
      "Inside Model: logits sum = 1950\n",
      "Inside Model: logits sum = 1833\n",
      "tensor([1.1349, 8.3763], device='cuda:0')Inside Model: logits sum = 1623tensor([1.1532, 7.5018], device='cuda:3')\n",
      "\n",
      "\n",
      "tensor([1.1321, 8.5333], device='cuda:1')Inside Model: logits sum = 1904\n",
      "\n",
      "tensor([1.1309, 8.6050], device='cuda:2')\n",
      "Inside Model: logits sum = 1867\n",
      "tensor([1.1451, 7.8618], device='cuda:2')\n",
      "Inside Model: logits sum = 1387Inside Model: logits sum = 1915\n",
      "\n",
      "Inside Model: logits sum = 1661tensor([1.1290, 8.7149], device='cuda:1')tensor([1.1290, 8.7149], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([1.1368, 8.2747], device='cuda:3')\n",
      "Inside Model: logits sum = 2204\n",
      "Inside Model: logits sum = 2306\n",
      "tensor([1.1594, 7.2496], device='cuda:0')\n",
      "Inside Model: logits sum = 1727\n",
      "tensor([1.1650, 7.0378], device='cuda:3')\n",
      "tensor([1.1600, 7.2240], device='cuda:1')\n",
      "Inside Model: logits sum = 1588\n",
      "tensor([1.1259, 8.9043], device='cuda:2')\n",
      "epoch 1 training done, time taken = 0h 0m 12s\n",
      "Saving model after epoch 1\n",
      "Starting epoch 1 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Inside Model: logits sum = 2416Inside Model: logits sum = 2816\n",
      "\n",
      "Inside Model: logits sum = 2372\n",
      "Inside Model: logits sum = 2362\n",
      "Outside Model: logits sum = 9966\n",
      "Inside Model: logits sum = 2279\n",
      "Inside Model: logits sum = 2308\n",
      "Inside Model: logits sum = 1195\n",
      "Inside Model: logits sum = 2000\n",
      "Outside Model: logits sum = 7782\n",
      "Inside Model: logits sum = 2057\n",
      "Inside Model: logits sum = 2744\n",
      "Inside Model: logits sum = 2815\n",
      "Inside Model: logits sum = 2922\n",
      "Outside Model: logits sum = 10538\n",
      "Inside Model: logits sum = 2963\n",
      "Inside Model: logits sum = 2921\n",
      "Inside Model: logits sum = 1062\n",
      "Inside Model: logits sum = 2411\n",
      "Outside Model: logits sum = 9357\n",
      "Inside Model: logits sum = 788\n",
      "Inside Model: logits sum = 1233Inside Model: logits sum = 1701\n",
      "\n",
      "Inside Model: logits sum = 1392\n",
      "Outside Model: logits sum = 5114\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 2s\n",
      "Inside Model: logits sum = 2034\n",
      "Inside Model: logits sum = 761\n",
      "Inside Model: logits sum = 1599\n",
      "Inside Model: logits sum = 2052\n",
      "Outside Model: logits sum = 6446\n",
      "Inside Model: logits sum = 2356Inside Model: logits sum = 2009\n",
      "\n",
      "Inside Model: logits sum = 2302\n",
      "Inside Model: logits sum = 2104\n",
      "Outside Model: logits sum = 8771\n",
      "Inside Model: logits sum = 1832\n",
      "Inside Model: logits sum = 3\n",
      "Inside Model: logits sum = 1842\n",
      "Inside Model: logits sum = 249\n",
      "Outside Model: logits sum = 3926\n",
      "Inside Model: logits sum = 514\n",
      "Inside Model: logits sum = 194\n",
      "Inside Model: logits sum = 143Inside Model: logits sum = 284\n",
      "\n",
      "Outside Model: logits sum = 1135\n",
      "Inside Model: logits sum = 147\n",
      "Inside Model: logits sum = 2384\n",
      "Inside Model: logits sum = 3068\n",
      "Inside Model: logits sum = 2552\n",
      "Outside Model: logits sum = 8151\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Inside Model: logits sum = 2266Inside Model: logits sum = 2729Inside Model: logits sum = 2529\n",
      "\n",
      "\n",
      "Inside Model: logits sum = 2629\n",
      "Outside Model: logits sum = 10153\n",
      "Inside Model: logits sum = 1888\n",
      "Inside Model: logits sum = 2032\n",
      "Inside Model: logits sum = 2150\n",
      "Inside Model: logits sum = 696\n",
      "Outside Model: logits sum = 6766\n",
      "Inside Model: logits sum = 2954Inside Model: logits sum = 2569\n",
      "\n",
      "Inside Model: logits sum = 2947\n",
      "Inside Model: logits sum = 2559\n",
      "Outside Model: logits sum = 4354\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.24137610256496994\n",
      "epoch 1 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 1\n",
      "Checking for early-stopping\n",
      "Epoch 1 done\n",
      "Starting epoch 2 training\n",
      "Inside Model: logits sum = 1539\n",
      "Inside Model: logits sum = 1847\n",
      "Inside Model: logits sum = 2376tensor([1.1416, 8.0314], device='cuda:1')\n",
      "Inside Model: logits sum = 1601\n",
      "tensor([1.1231, 9.0820], device='cuda:2')\n",
      "\n",
      "tensor([1.1620, 7.1483], device='cuda:0')tensor([1.1473, 7.7576], device='cuda:3')\n",
      "\n",
      "Inside Model: logits sum = 1766\n",
      "tensor([1.1296, 8.6780], device='cuda:1')\n",
      "Inside Model: logits sum = 1481Inside Model: logits sum = 2383\n",
      "\n",
      "Inside Model: logits sum = 1974\n",
      "tensor([1.1525, 7.5294], device='cuda:0')tensor([1.1467, 7.7871], device='cuda:2')\n",
      "\n",
      "tensor([1.1296, 8.6780], device='cuda:3')\n",
      "Inside Model: logits sum = 2269Inside Model: logits sum = 1911\n",
      "Inside Model: logits sum = 2049\n",
      "\n",
      "tensor([1.1406, 8.0789], device='cuda:2')\n",
      "tensor([1.1633, 7.0988], device='cuda:0')tensor([1.1590, 7.2624], device='cuda:3')Inside Model: logits sum = 2621\n",
      "\n",
      "\n",
      "tensor([1.1496, 7.6561], device='cuda:1')\n",
      "Inside Model: logits sum = 1674\n",
      "Inside Model: logits sum = 2047\n",
      "tensor([1.1302, 8.6413], device='cuda:1')Inside Model: logits sum = 1881\n",
      "\n",
      "Inside Model: logits sum = 2344tensor([1.1532, 7.5018], device='cuda:0')\n",
      "\n",
      "tensor([1.1496, 7.6561], device='cuda:3')\n",
      "tensor([1.1643, 7.0621], device='cuda:2')\n",
      "Inside Model: logits sum = 1728\n",
      "tensor([1.1454, 7.8467], device='cuda:1')Inside Model: logits sum = 1716Inside Model: logits sum = 2732\n",
      "\n",
      "Inside Model: logits sum = 2237\n",
      "\n",
      "tensor([1.1253, 8.9432], device='cuda:3')\n",
      "tensor([1.1577, 7.3143], device='cuda:0')tensor([1.1584, 7.2883], device='cuda:2')\n",
      "\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.6879\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 2 = 0h 0m 7s\n",
      "Inside Model: logits sum = 1888\n",
      "tensor([1.1306, 8.6232], device='cuda:0')\n",
      "Inside Model: logits sum = 2321\n",
      "Inside Model: logits sum = 1773\n",
      "tensor([1.1767, 6.6386], device='cuda:3')\n",
      "tensor([1.1353, 8.3592], device='cuda:1')Inside Model: logits sum = 2344\n",
      "\n",
      "tensor([1.1554, 7.4069], device='cuda:2')\n",
      "Inside Model: logits sum = 1977\n",
      "Inside Model: logits sum = 2021tensor([1.1515, 7.5712], device='cuda:1')\n",
      "Inside Model: logits sum = 1860\n",
      "\n",
      "Inside Model: logits sum = 1527\n",
      "tensor([1.1371, 8.2581], device='cuda:3')\n",
      "tensor([1.1281, 8.7709], device='cuda:2')tensor([1.1522, 7.5433], device='cuda:0')\n",
      "\n",
      "Inside Model: logits sum = 1766\n",
      "tensor([1.1577, 7.3143], device='cuda:1')\n",
      "Inside Model: logits sum = 1273\n",
      "Inside Model: logits sum = 1934\n",
      "Inside Model: logits sum = 1661tensor([ 1.1085, 10.1638], device='cuda:3')\n",
      "\n",
      "tensor([1.1349, 8.3763], device='cuda:0')\n",
      "tensor([1.1387, 8.1756], device='cuda:2')\n",
      "Inside Model: logits sum = 1656\n",
      "Inside Model: logits sum = 1934tensor([1.1584, 7.2883], device='cuda:3')\n",
      "\n",
      "tensor([1.1210, 9.2252], device='cuda:0')\n",
      "Inside Model: logits sum = 2246\n",
      "Inside Model: logits sum = 2152\n",
      "tensor([1.1561, 7.3802], device='cuda:2')\n",
      "tensor([1.1461, 7.8168], device='cuda:1')\n",
      "Inside Model: logits sum = 1891\n",
      "tensor([1.1480, 7.7283], device='cuda:0')\n",
      "Inside Model: logits sum = 1845\n",
      "tensor([1.1281, 8.7709], device='cuda:1')\n",
      "Inside Model: logits sum = 1385\n",
      "Inside Model: logits sum = 1986\n",
      "tensor([1.1397, 8.1270], device='cuda:3')\n",
      "tensor([1.1429, 7.9689], device='cuda:2')\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.6864\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 2 = 0h 0m 2s\n",
      "Inside Model: logits sum = 2026Inside Model: logits sum = 2012\n",
      "\n",
      "tensor([1.1368, 8.2747], device='cuda:0')tensor([1.1397, 8.1270], device='cuda:3')\n",
      "\n",
      "Inside Model: logits sum = 2169\n",
      "tensor([1.1623, 7.1359], device='cuda:1')\n",
      "Inside Model: logits sum = 1635\n",
      "tensor([1.1340, 8.4280], device='cuda:2')\n",
      "Inside Model: logits sum = 2139Inside Model: logits sum = 1969\n",
      "\n",
      "tensor([1.1470, 7.7723], device='cuda:1')tensor([1.1656, 7.0137], device='cuda:0')\n",
      "\n",
      "Inside Model: logits sum = 2023\n",
      "tensor([1.1580, 7.3012], device='cuda:3')\n",
      "Inside Model: logits sum = 1675\n",
      "tensor([1.1477, 7.7429], device='cuda:2')\n",
      "Inside Model: logits sum = 1333\n",
      "Inside Model: logits sum = 1888\n",
      "tensor([1.1268, 8.8467], device='cuda:3')\n",
      "tensor([1.1663, 6.9898], device='cuda:0')\n",
      "Inside Model: logits sum = 2118\n",
      "Inside Model: logits sum = 1907tensor([1.1473, 7.7576], device='cuda:1')\n",
      "\n",
      "tensor([1.1594, 7.2496], device='cuda:2')\n",
      "epoch 2 training done, time taken = 0h 0m 11s\n",
      "Saving model after epoch 2\n",
      "Starting epoch 2 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Inside Model: logits sum = 2400\n",
      "Inside Model: logits sum = 2814\n",
      "Inside Model: logits sum = 2365\n",
      "Inside Model: logits sum = 2361\n",
      "Outside Model: logits sum = 9940\n",
      "Inside Model: logits sum = 1190\n",
      "Inside Model: logits sum = 2274\n",
      "Inside Model: logits sum = 2309\n",
      "Inside Model: logits sum = 2001\n",
      "Outside Model: logits sum = 7774\n",
      "Inside Model: logits sum = 2055\n",
      "Inside Model: logits sum = 2748\n",
      "Inside Model: logits sum = 2817\n",
      "Inside Model: logits sum = 2923\n",
      "Outside Model: logits sum = 10543\n",
      "Inside Model: logits sum = 2958Inside Model: logits sum = 1054\n",
      "\n",
      "Inside Model: logits sum = 2916\n",
      "Inside Model: logits sum = 2408\n",
      "Outside Model: logits sum = 9336\n",
      "Inside Model: logits sum = 785\n",
      "Inside Model: logits sum = 1230\n",
      "Inside Model: logits sum = 1691\n",
      "Inside Model: logits sum = 1383\n",
      "Outside Model: logits sum = 5089\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 2s\n",
      "Inside Model: logits sum = 2031Inside Model: logits sum = 755\n",
      "\n",
      "Inside Model: logits sum = 1595\n",
      "Inside Model: logits sum = 2050\n",
      "Outside Model: logits sum = 6431\n",
      "Inside Model: logits sum = 2005\n",
      "Inside Model: logits sum = 2302Inside Model: logits sum = 2353\n",
      "\n",
      "Inside Model: logits sum = 2094\n",
      "Outside Model: logits sum = 8754\n",
      "Inside Model: logits sum = 1833\n",
      "Inside Model: logits sum = 2\n",
      "Inside Model: logits sum = 1843\n",
      "Inside Model: logits sum = 249\n",
      "Outside Model: logits sum = 3927\n",
      "Inside Model: logits sum = 507\n",
      "Inside Model: logits sum = 193\n",
      "Inside Model: logits sum = 143\n",
      "Inside Model: logits sum = 281\n",
      "Outside Model: logits sum = 1124\n",
      "Inside Model: logits sum = 148\n",
      "Inside Model: logits sum = 3069\n",
      "Inside Model: logits sum = 2379\n",
      "Inside Model: logits sum = 2551\n",
      "Outside Model: logits sum = 8147\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Inside Model: logits sum = 2526Inside Model: logits sum = 2259\n",
      "\n",
      "Inside Model: logits sum = 2728\n",
      "Inside Model: logits sum = 2633\n",
      "Outside Model: logits sum = 10146\n",
      "Inside Model: logits sum = 1877\n",
      "Inside Model: logits sum = 2029\n",
      "Inside Model: logits sum = 2134\n",
      "Inside Model: logits sum = 686\n",
      "Outside Model: logits sum = 6726\n",
      "Inside Model: logits sum = 2953\n",
      "Inside Model: logits sum = 2944\n",
      "Inside Model: logits sum = 2568\n",
      "Inside Model: logits sum = 2552\n",
      "Outside Model: logits sum = 4353\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.24151831839970192\n",
      "epoch 2 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 2\n",
      "Checking for early-stopping\n",
      "Dev score improved by 0.0\n",
      "Epoch 2 done\n",
      "Starting epoch 3 training\n",
      "Inside Model: logits sum = 1957\n",
      "tensor([1.1387, 8.1756], device='cuda:0')Inside Model: logits sum = 1581\n",
      "\n",
      "Inside Model: logits sum = 2276tensor([1.1185, 9.3945], device='cuda:3')\n",
      "\n",
      "tensor([1.1473, 7.7576], device='cuda:1')\n",
      "Inside Model: logits sum = 1743\n",
      "tensor([1.1340, 8.4280], device='cuda:2')\n",
      "Inside Model: logits sum = 1529\n",
      "tensor([1.1188, 9.3730], device='cuda:0')\n",
      "Inside Model: logits sum = 1899\n",
      "Inside Model: logits sum = 2224tensor([1.1337, 8.4454], device='cuda:1')\n",
      "\n",
      "tensor([1.1509, 7.5993], device='cuda:3')\n",
      "Inside Model: logits sum = 1444\n",
      "tensor([1.1416, 8.0314], device='cuda:2')\n",
      "Inside Model: logits sum = 1989\n",
      "Inside Model: logits sum = 1864\n",
      "tensor([1.1519, 7.5572], device='cuda:0')\n",
      "tensor([1.1425, 7.9844], device='cuda:3')\n",
      "Inside Model: logits sum = 1956\n",
      "tensor([1.1653, 7.0257], device='cuda:1')\n",
      "Inside Model: logits sum = 2228\n",
      "tensor([1.1554, 7.4069], device='cuda:2')\n",
      "Inside Model: logits sum = 1833\n",
      "Inside Model: logits sum = 1740tensor([1.1525, 7.5294], device='cuda:0')\n",
      "\n",
      "tensor([1.1143, 9.7062], device='cuda:3')Inside Model: logits sum = 2177\n",
      "\n",
      "Inside Model: logits sum = 1668tensor([1.1643, 7.0621], device='cuda:1')\n",
      "\n",
      "tensor([1.1384, 8.1920], device='cuda:2')\n",
      "Inside Model: logits sum = 1569\n",
      "Inside Model: logits sum = 2172\n",
      "tensor([1.1306, 8.6232], device='cuda:0')\n",
      "tensor([1.1623, 7.1359], device='cuda:3')\n",
      "Inside Model: logits sum = 1936\n",
      "tensor([1.1362, 8.3083], device='cuda:1')Inside Model: logits sum = 1789\n",
      "\n",
      "tensor([1.1525, 7.5294], device='cuda:2')\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.6882\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 3 = 0h 0m 7s\n",
      "Inside Model: logits sum = 1985\n",
      "Inside Model: logits sum = 1811tensor([1.1763, 6.6494], device='cuda:0')\n",
      "\n",
      "tensor([ 1.0993, 11.0108], device='cuda:3')\n",
      "Inside Model: logits sum = 1815\n",
      "tensor([1.1613, 7.1734], device='cuda:1')\n",
      "Inside Model: logits sum = 2201\n",
      "tensor([1.1512, 7.5852], device='cuda:2')\n",
      "Inside Model: logits sum = 1695\n",
      "tensor([1.1409, 8.0630], device='cuda:0')\n",
      "Inside Model: logits sum = 2395\n",
      "tensor([1.1660, 7.0017], device='cuda:3')Inside Model: logits sum = 1616\n",
      "\n",
      "tensor([1.1528, 7.5156], device='cuda:1')\n",
      "Inside Model: logits sum = 1598\n",
      "tensor([1.1470, 7.7723], device='cuda:2')\n",
      "Inside Model: logits sum = 1470\n",
      "tensor([1.1302, 8.6413], device='cuda:0')Inside Model: logits sum = 1874\n",
      "\n",
      "tensor([1.1438, 7.9226], device='cuda:3')\n",
      "Inside Model: logits sum = 2313\n",
      "tensor([1.1302, 8.6413], device='cuda:1')\n",
      "Inside Model: logits sum = 2232\n",
      "tensor([1.1499, 7.6418], device='cuda:2')\n",
      "Inside Model: logits sum = 2163\n",
      "Inside Model: logits sum = 1896\n",
      "tensor([1.1496, 7.6561], device='cuda:0')\n",
      "Inside Model: logits sum = 1952\n",
      "tensor([1.1413, 8.0472], device='cuda:1')\n",
      "tensor([1.1435, 7.9380], device='cuda:3')\n",
      "Inside Model: logits sum = 1962\n",
      "tensor([1.1587, 7.2753], device='cuda:2')\n",
      "Inside Model: logits sum = 1608\n",
      "tensor([1.1554, 7.4069], device='cuda:0')\n",
      "Inside Model: logits sum = 1530\n",
      "tensor([1.1525, 7.5294], device='cuda:3')Inside Model: logits sum = 2169\n",
      "\n",
      "tensor([1.1400, 8.1109], device='cuda:1')\n",
      "Inside Model: logits sum = 1664\n",
      "tensor([1.1480, 7.7283], device='cuda:2')\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.6884\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 3 = 0h 0m 2s\n",
      "Inside Model: logits sum = 1875\n",
      "tensor([1.1636, 7.0865], device='cuda:3')\n",
      "Inside Model: logits sum = 2039\n",
      "Inside Model: logits sum = 2098\n",
      "tensor([1.1617, 7.1608], device='cuda:0')\n",
      "tensor([1.1473, 7.7576], device='cuda:1')\n",
      "Inside Model: logits sum = 2491\n",
      "tensor([1.1571, 7.3405], device='cuda:2')\n",
      "Inside Model: logits sum = 1864\n",
      "Inside Model: logits sum = 1910\n",
      "tensor([1.1334, 8.4628], device='cuda:3')\n",
      "tensor([1.1461, 7.8168], device='cuda:0')\n",
      "Inside Model: logits sum = 1972\n",
      "tensor([1.1406, 8.0789], device='cuda:1')\n",
      "Inside Model: logits sum = 2192\n",
      "tensor([1.1438, 7.9226], device='cuda:2')\n",
      "Inside Model: logits sum = 1894\n",
      "tensor([1.1375, 8.2414], device='cuda:0')\n",
      "Inside Model: logits sum = 1838\n",
      "Inside Model: logits sum = 1511\n",
      "tensor([1.1541, 7.4608], device='cuda:1')\n",
      "tensor([1.1346, 8.3934], device='cuda:3')\n",
      "Inside Model: logits sum = 1909\n",
      "tensor([1.1400, 8.1109], device='cuda:2')\n",
      "epoch 3 training done, time taken = 0h 0m 11s\n",
      "Saving model after epoch 3\n",
      "Starting epoch 3 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Inside Model: logits sum = 2400\n",
      "Inside Model: logits sum = 2814\n",
      "Inside Model: logits sum = 2365\n",
      "Inside Model: logits sum = 2361\n",
      "Outside Model: logits sum = 9940\n",
      "Inside Model: logits sum = 1190\n",
      "Inside Model: logits sum = 2274\n",
      "Inside Model: logits sum = 2001\n",
      "Inside Model: logits sum = 2309\n",
      "Outside Model: logits sum = 7774\n",
      "Inside Model: logits sum = 2055\n",
      "Inside Model: logits sum = 2748\n",
      "Inside Model: logits sum = 2817Inside Model: logits sum = 2923\n",
      "\n",
      "Outside Model: logits sum = 10543\n",
      "Inside Model: logits sum = 1054Inside Model: logits sum = 2958\n",
      "\n",
      "Inside Model: logits sum = 2916Inside Model: logits sum = 2408\n",
      "\n",
      "Outside Model: logits sum = 9336\n",
      "Inside Model: logits sum = 785\n",
      "Inside Model: logits sum = 1230\n",
      "Inside Model: logits sum = 1691\n",
      "Inside Model: logits sum = 1383\n",
      "Outside Model: logits sum = 5089\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 2s\n",
      "Inside Model: logits sum = 755\n",
      "Inside Model: logits sum = 2031\n",
      "Inside Model: logits sum = 2050Inside Model: logits sum = 1595\n",
      "\n",
      "Outside Model: logits sum = 6431\n",
      "Inside Model: logits sum = 2005\n",
      "Inside Model: logits sum = 2353\n",
      "Inside Model: logits sum = 2302\n",
      "Inside Model: logits sum = 2094\n",
      "Outside Model: logits sum = 8754\n",
      "Inside Model: logits sum = 1833\n",
      "Inside Model: logits sum = 249\n",
      "Inside Model: logits sum = 2Inside Model: logits sum = 1843\n",
      "\n",
      "Outside Model: logits sum = 3927\n",
      "Inside Model: logits sum = 507\n",
      "Inside Model: logits sum = 281\n",
      "Inside Model: logits sum = 143\n",
      "Inside Model: logits sum = 193\n",
      "Outside Model: logits sum = 1124\n",
      "Inside Model: logits sum = 148\n",
      "Inside Model: logits sum = 2551\n",
      "Inside Model: logits sum = 2379\n",
      "Inside Model: logits sum = 3069\n",
      "Outside Model: logits sum = 8147\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Inside Model: logits sum = 2526\n",
      "Inside Model: logits sum = 2633\n",
      "Inside Model: logits sum = 2728Inside Model: logits sum = 2259\n",
      "\n",
      "Outside Model: logits sum = 10146\n",
      "Inside Model: logits sum = 1877\n",
      "Inside Model: logits sum = 686\n",
      "Inside Model: logits sum = 2134\n",
      "Inside Model: logits sum = 2029\n",
      "Outside Model: logits sum = 6726\n",
      "Inside Model: logits sum = 2953\n",
      "Inside Model: logits sum = 2552\n",
      "Inside Model: logits sum = 2568\n",
      "Inside Model: logits sum = 2944\n",
      "Outside Model: logits sum = 4353\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.24151831839970192\n",
      "epoch 3 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 3\n",
      "Checking for early-stopping\n",
      "Dev score is 0.0 lower than best Dev score (24.2)\n",
      "2 epochs left until Dev score to improve to avoid early-stopping!\n",
      "Epoch 3 done\n",
      "Starting epoch 4 training\n",
      "Inside Model: logits sum = 2493Inside Model: logits sum = 1986\n",
      "\n",
      "tensor([1.1467, 7.7871], device='cuda:0')tensor([1.1432, 7.9534], device='cuda:3')\n",
      "\n",
      "Inside Model: logits sum = 2528\n",
      "tensor([1.1580, 7.3012], device='cuda:1')\n",
      "Inside Model: logits sum = 2040\n",
      "tensor([1.1457, 7.8317], device='cuda:2')\n",
      "Inside Model: logits sum = 1440\n",
      "tensor([1.1124, 9.8462], device='cuda:0')\n",
      "Inside Model: logits sum = 1927\n",
      "tensor([1.1432, 7.9534], device='cuda:3')\n",
      "Inside Model: logits sum = 2128\n",
      "Inside Model: logits sum = 2087\n",
      "tensor([1.1394, 8.1431], device='cuda:2')\n",
      "tensor([1.1378, 8.2249], device='cuda:1')\n",
      "Inside Model: logits sum = 1713\n",
      "tensor([1.1321, 8.5333], device='cuda:3')Inside Model: logits sum = 1871\n",
      "\n",
      "Inside Model: logits sum = 1927\n",
      "tensor([1.1438, 7.9226], device='cuda:1')\n",
      "tensor([1.1736, 6.7368], device='cuda:0')\n",
      "Inside Model: logits sum = 1795\n",
      "tensor([1.1623, 7.1359], device='cuda:2')\n",
      "Inside Model: logits sum = 2176\n",
      "tensor([1.1646, 7.0499], device='cuda:0')\n",
      "Inside Model: logits sum = 1542\n",
      "tensor([1.1525, 7.5294], device='cuda:3')\n",
      "Inside Model: logits sum = 2173\n",
      "tensor([1.1525, 7.5294], device='cuda:1')\n",
      "Inside Model: logits sum = 1688\n",
      "tensor([1.1541, 7.4608], device='cuda:2')\n",
      "Inside Model: logits sum = 1613\n",
      "tensor([1.1435, 7.9380], device='cuda:3')\n",
      "Inside Model: logits sum = 1850Inside Model: logits sum = 1662\n",
      "\n",
      "tensor([1.1293, 8.6964], device='cuda:0')tensor([1.1613, 7.1734], device='cuda:1')\n",
      "\n",
      "Inside Model: logits sum = 1986\n",
      "tensor([1.1600, 7.2240], device='cuda:2')\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.6875\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 4 = 0h 0m 7s\n",
      "Inside Model: logits sum = 2383\n",
      "tensor([1.1558, 7.3935], device='cuda:0')\n",
      "Inside Model: logits sum = 1964\n",
      "tensor([1.1509, 7.5993], device='cuda:3')\n",
      "Inside Model: logits sum = 2030\n",
      "Inside Model: logits sum = 1665\n",
      "tensor([1.1567, 7.3537], device='cuda:1')\n",
      "tensor([1.1499, 7.6418], device='cuda:2')\n",
      "Inside Model: logits sum = 1958\n",
      "tensor([1.1567, 7.3537], device='cuda:0')\n",
      "Inside Model: logits sum = 1562\n",
      "tensor([ 1.0978, 11.1608], device='cuda:3')\n",
      "Inside Model: logits sum = 1697\n",
      "tensor([1.1397, 8.1270], device='cuda:1')\n",
      "Inside Model: logits sum = 1853\n",
      "tensor([1.1567, 7.3537], device='cuda:2')\n",
      "Inside Model: logits sum = 2259\n",
      "tensor([1.1633, 7.0988], device='cuda:0')\n",
      "Inside Model: logits sum = 2010\n",
      "Inside Model: logits sum = 2090tensor([1.1464, 7.8019], device='cuda:3')\n",
      "\n",
      "tensor([1.1703, 6.8495], device='cuda:1')\n",
      "Inside Model: logits sum = 1446\n",
      "tensor([1.1318, 8.5511], device='cuda:2')\n",
      "Inside Model: logits sum = 2016\n",
      "tensor([1.1760, 6.6602], device='cuda:0')\n",
      "Inside Model: logits sum = 1416\n",
      "Inside Model: logits sum = 2045\n",
      "tensor([1.1315, 8.5690], device='cuda:3')\n",
      "tensor([1.1197, 9.3091], device='cuda:1')\n",
      "Inside Model: logits sum = 1471\n",
      "tensor([1.1394, 8.1431], device='cuda:2')\n",
      "Inside Model: logits sum = 1584\n",
      "tensor([1.1551, 7.4203], device='cuda:0')\n",
      "Inside Model: logits sum = 2234\n",
      "tensor([1.1293, 8.6964], device='cuda:3')\n",
      "Inside Model: logits sum = 1372Inside Model: logits sum = 2033\n",
      "\n",
      "tensor([1.1397, 8.1270], device='cuda:1')tensor([1.1375, 8.2414], device='cuda:2')\n",
      "\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.6880\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 4 = 0h 0m 2s\n",
      "Inside Model: logits sum = 2223\n",
      "Inside Model: logits sum = 1427Inside Model: logits sum = 1571\n",
      "\n",
      "tensor([1.1594, 7.2496], device='cuda:0')tensor([1.1290, 8.7149], device='cuda:3')tensor([1.1509, 7.5993], device='cuda:1')\n",
      "\n",
      "\n",
      "Inside Model: logits sum = 2462\n",
      "tensor([1.1700, 6.8610], device='cuda:2')\n",
      "Inside Model: logits sum = 1741\n",
      "tensor([1.1231, 9.0820], device='cuda:0')\n",
      "Inside Model: logits sum = 1817\n",
      "tensor([1.1486, 7.6992], device='cuda:3')\n",
      "Inside Model: logits sum = 1870\n",
      "Inside Model: logits sum = 1952\n",
      "tensor([1.1327, 8.4979], device='cuda:1')\n",
      "tensor([1.1321, 8.5333], device='cuda:2')\n",
      "Inside Model: logits sum = 1865\n",
      "tensor([1.1284, 8.7521], device='cuda:0')\n",
      "Inside Model: logits sum = 2281\n",
      "tensor([1.1483, 7.7137], device='cuda:3')\n",
      "Inside Model: logits sum = 2157\n",
      "Inside Model: logits sum = 2408\n",
      "tensor([1.1594, 7.2496], device='cuda:1')\n",
      "tensor([1.1445, 7.8921], device='cuda:2')\n",
      "epoch 4 training done, time taken = 0h 0m 11s\n",
      "Saving model after epoch 4\n",
      "Starting epoch 4 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Inside Model: logits sum = 2365Inside Model: logits sum = 2400\n",
      "\n",
      "Inside Model: logits sum = 2814\n",
      "Inside Model: logits sum = 2361\n",
      "Outside Model: logits sum = 9940\n",
      "Inside Model: logits sum = 2274Inside Model: logits sum = 2001\n",
      "\n",
      "Inside Model: logits sum = 1190\n",
      "Inside Model: logits sum = 2309\n",
      "Outside Model: logits sum = 7774\n",
      "Inside Model: logits sum = 2055Inside Model: logits sum = 2748\n",
      "\n",
      "Inside Model: logits sum = 2923\n",
      "Inside Model: logits sum = 2817\n",
      "Outside Model: logits sum = 10543\n",
      "Inside Model: logits sum = 2408Inside Model: logits sum = 1054\n",
      "Inside Model: logits sum = 2958\n",
      "\n",
      "Inside Model: logits sum = 2916\n",
      "Outside Model: logits sum = 9336\n",
      "Inside Model: logits sum = 785Inside Model: logits sum = 1383\n",
      "\n",
      "Inside Model: logits sum = 1230\n",
      "Inside Model: logits sum = 1691\n",
      "Outside Model: logits sum = 5089\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 2s\n",
      "Inside Model: logits sum = 755Inside Model: logits sum = 2031\n",
      "\n",
      "Inside Model: logits sum = 2050\n",
      "Inside Model: logits sum = 1595\n",
      "Outside Model: logits sum = 6431\n",
      "Inside Model: logits sum = 2005\n",
      "Inside Model: logits sum = 2302\n",
      "Inside Model: logits sum = 2094\n",
      "Inside Model: logits sum = 2353\n",
      "Outside Model: logits sum = 8754\n",
      "Inside Model: logits sum = 1833\n",
      "Inside Model: logits sum = 2\n",
      "Inside Model: logits sum = 249\n",
      "Inside Model: logits sum = 1843\n",
      "Outside Model: logits sum = 3927\n",
      "Inside Model: logits sum = 507\n",
      "Inside Model: logits sum = 281Inside Model: logits sum = 193\n",
      "\n",
      "Inside Model: logits sum = 143\n",
      "Outside Model: logits sum = 1124\n",
      "Inside Model: logits sum = 148\n",
      "Inside Model: logits sum = 3069Inside Model: logits sum = 2551\n",
      "\n",
      "Inside Model: logits sum = 2379\n",
      "Outside Model: logits sum = 8147\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Inside Model: logits sum = 2526\n",
      "Inside Model: logits sum = 2259\n",
      "Inside Model: logits sum = 2633\n",
      "Inside Model: logits sum = 2728\n",
      "Outside Model: logits sum = 10146\n",
      "Inside Model: logits sum = 1877\n",
      "Inside Model: logits sum = 2029Inside Model: logits sum = 686\n",
      "\n",
      "Inside Model: logits sum = 2134\n",
      "Outside Model: logits sum = 6726\n",
      "Inside Model: logits sum = 2953\n",
      "Inside Model: logits sum = 2944Inside Model: logits sum = 2552\n",
      "\n",
      "Inside Model: logits sum = 2568\n",
      "Outside Model: logits sum = 4353\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.24151831839970192\n",
      "epoch 4 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 4\n",
      "Checking for early-stopping\n",
      "Dev score is 0.0 lower than best Dev score (24.2)\n",
      "1 epochs left until Dev score to improve to avoid early-stopping!\n",
      "Epoch 4 done\n",
      "Starting epoch 5 training\n",
      "Inside Model: logits sum = 1969Inside Model: logits sum = 2750\n",
      "\n",
      "tensor([1.1515, 7.5712], device='cuda:1')tensor([1.1643, 7.0621], device='cuda:0')\n",
      "\n",
      "Inside Model: logits sum = 2030\n",
      "Inside Model: logits sum = 1933\n",
      "tensor([1.1259, 8.9043], device='cuda:3')\n",
      "tensor([1.1371, 8.2581], device='cuda:2')\n",
      "Inside Model: logits sum = 1677Inside Model: logits sum = 2053\n",
      "\n",
      "Inside Model: logits sum = 2238tensor([1.1413, 8.0472], device='cuda:1')\n",
      "tensor([1.1700, 6.8610], device='cuda:0')\n",
      "\n",
      "tensor([1.1646, 7.0499], device='cuda:3')Inside Model: logits sum = 1811\n",
      "\n",
      "tensor([1.1512, 7.5852], device='cuda:2')\n",
      "Inside Model: logits sum = 2447\n",
      "Inside Model: logits sum = 1515\n",
      "tensor([1.1432, 7.9534], device='cuda:3')\n",
      "tensor([1.1432, 7.9534], device='cuda:0')Inside Model: logits sum = 1906\n",
      "\n",
      "tensor([1.1603, 7.2113], device='cuda:1')Inside Model: logits sum = 1788\n",
      "\n",
      "tensor([1.1470, 7.7723], device='cuda:2')\n",
      "Inside Model: logits sum = 1736\n",
      "tensor([1.1381, 8.2084], device='cuda:3')Inside Model: logits sum = 2306\n",
      "\n",
      "Inside Model: logits sum = 2021Inside Model: logits sum = 1600\n",
      "\n",
      "tensor([1.1375, 8.2414], device='cuda:0')\n",
      "tensor([1.1334, 8.4628], device='cuda:2')tensor([1.1146, 9.6832], device='cuda:1')\n",
      "\n",
      "Inside Model: logits sum = 1814\n",
      "tensor([1.1435, 7.9380], device='cuda:0')\n",
      "Inside Model: logits sum = 2036\n",
      "tensor([1.1679, 6.9306], device='cuda:3')Inside Model: logits sum = 2042\n",
      "\n",
      "Inside Model: logits sum = 1850\n",
      "tensor([1.1767, 6.6386], device='cuda:1')tensor([1.1253, 8.9432], device='cuda:2')\n",
      "\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.6892\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 5 = 0h 0m 7s\n",
      "Inside Model: logits sum = 1851Inside Model: logits sum = 1668\n",
      "\n",
      "Inside Model: logits sum = 1937tensor([1.1256, 8.9237], device='cuda:1')tensor([1.1315, 8.5690], device='cuda:0')\n",
      "\n",
      "\n",
      "tensor([1.1561, 7.3802], device='cuda:3')Inside Model: logits sum = 2267\n",
      "\n",
      "tensor([1.1710, 6.8267], device='cuda:2')\n",
      "Inside Model: logits sum = 1796\n",
      "Inside Model: logits sum = 2341Inside Model: logits sum = 1345\n",
      "\n",
      "tensor([1.1327, 8.4979], device='cuda:0')\n",
      "tensor([1.1493, 7.6704], device='cuda:3')Inside Model: logits sum = 1573tensor([1.1309, 8.6050], device='cuda:1')\n",
      "\n",
      "\n",
      "tensor([1.1441, 7.9073], device='cuda:2')\n",
      "Inside Model: logits sum = 2051\n",
      "Inside Model: logits sum = 1807\n",
      "tensor([1.1406, 8.0789], device='cuda:0')\n",
      "tensor([1.1290, 8.7149], device='cuda:1')Inside Model: logits sum = 1417\n",
      "\n",
      "Inside Model: logits sum = 1603\n",
      "tensor([1.1384, 8.1920], device='cuda:3')\n",
      "tensor([1.1457, 7.8317], device='cuda:2')\n",
      "Inside Model: logits sum = 1983Inside Model: logits sum = 1527\n",
      "\n",
      "tensor([1.1409, 8.0630], device='cuda:0')tensor([1.1359, 8.3252], device='cuda:1')Inside Model: logits sum = 2155\n",
      "\n",
      "\n",
      "tensor([1.1435, 7.9380], device='cuda:3')Inside Model: logits sum = 1986\n",
      "\n",
      "tensor([1.1535, 7.4881], device='cuda:2')\n",
      "Inside Model: logits sum = 1199\n",
      "tensor([1.1309, 8.6050], device='cuda:3')Inside Model: logits sum = 2076\n",
      "\n",
      "Inside Model: logits sum = 1681tensor([1.1306, 8.6232], device='cuda:0')\n",
      "\n",
      "Inside Model: logits sum = 2370\n",
      "tensor([1.1349, 8.3763], device='cuda:1')\n",
      "tensor([1.1502, 7.6276], device='cuda:2')\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.6890\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 5 = 0h 0m 2s\n",
      "Inside Model: logits sum = 1776\n",
      "Inside Model: logits sum = 2069\n",
      "Inside Model: logits sum = 2357tensor([1.1451, 7.8618], device='cuda:0')\n",
      "\n",
      "tensor([1.1580, 7.3012], device='cuda:1')\n",
      "tensor([1.1706, 6.8381], device='cuda:3')\n",
      "Inside Model: logits sum = 1505\n",
      "tensor([1.1564, 7.3669], device='cuda:2')\n",
      "Inside Model: logits sum = 1493\n",
      "Inside Model: logits sum = 2379\n",
      "tensor([1.1425, 7.9844], device='cuda:0')\n",
      "Inside Model: logits sum = 1839tensor([1.1633, 7.0988], device='cuda:3')\n",
      "\n",
      "tensor([1.1480, 7.7283], device='cuda:1')\n",
      "Inside Model: logits sum = 1712\n",
      "tensor([1.1243, 9.0022], device='cuda:2')\n",
      "Inside Model: logits sum = 2250\n",
      "tensor([1.1818, 6.4810], device='cuda:0')\n",
      "Inside Model: logits sum = 2401\n",
      "Inside Model: logits sum = 2063\n",
      "tensor([1.1315, 8.5690], device='cuda:1')\n",
      "tensor([1.1435, 7.9380], device='cuda:3')Inside Model: logits sum = 2528\n",
      "\n",
      "tensor([1.1753, 6.6819], device='cuda:2')\n",
      "epoch 5 training done, time taken = 0h 0m 11s\n",
      "Saving model after epoch 5\n",
      "Starting epoch 5 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Inside Model: logits sum = 2365Inside Model: logits sum = 2400Inside Model: logits sum = 2361\n",
      "\n",
      "\n",
      "Inside Model: logits sum = 2814\n",
      "Outside Model: logits sum = 9940\n",
      "Inside Model: logits sum = 2274\n",
      "Inside Model: logits sum = 2001Inside Model: logits sum = 1190\n",
      "\n",
      "Inside Model: logits sum = 2309\n",
      "Outside Model: logits sum = 7774\n",
      "Inside Model: logits sum = 2055Inside Model: logits sum = 2748\n",
      "\n",
      "Inside Model: logits sum = 2817Inside Model: logits sum = 2923\n",
      "\n",
      "Outside Model: logits sum = 10543\n",
      "Inside Model: logits sum = 2958\n",
      "Inside Model: logits sum = 1054\n",
      "Inside Model: logits sum = 2408\n",
      "Inside Model: logits sum = 2916\n",
      "Outside Model: logits sum = 9336\n",
      "Inside Model: logits sum = 785Inside Model: logits sum = 1230\n",
      "\n",
      "Inside Model: logits sum = 1383\n",
      "Inside Model: logits sum = 1691\n",
      "Outside Model: logits sum = 5089\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 2s\n",
      "Inside Model: logits sum = 2031\n",
      "Inside Model: logits sum = 755\n",
      "Inside Model: logits sum = 2050\n",
      "Inside Model: logits sum = 1595\n",
      "Outside Model: logits sum = 6431\n",
      "Inside Model: logits sum = 2005\n",
      "Inside Model: logits sum = 2302\n",
      "Inside Model: logits sum = 2094\n",
      "Inside Model: logits sum = 2353\n",
      "Outside Model: logits sum = 8754\n",
      "Inside Model: logits sum = 2\n",
      "Inside Model: logits sum = 1833\n",
      "Inside Model: logits sum = 249\n",
      "Inside Model: logits sum = 1843\n",
      "Outside Model: logits sum = 3927\n",
      "Inside Model: logits sum = 193\n",
      "Inside Model: logits sum = 507\n",
      "Inside Model: logits sum = 281\n",
      "Inside Model: logits sum = 143\n",
      "Outside Model: logits sum = 1124\n",
      "Inside Model: logits sum = 148\n",
      "Inside Model: logits sum = 3069\n",
      "Inside Model: logits sum = 2551\n",
      "Inside Model: logits sum = 2379\n",
      "Outside Model: logits sum = 8147\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Inside Model: logits sum = 2259Inside Model: logits sum = 2526\n",
      "\n",
      "Inside Model: logits sum = 2633\n",
      "Inside Model: logits sum = 2728\n",
      "Outside Model: logits sum = 10146\n",
      "Inside Model: logits sum = 2029Inside Model: logits sum = 1877\n",
      "\n",
      "Inside Model: logits sum = 686Inside Model: logits sum = 2134\n",
      "\n",
      "Outside Model: logits sum = 6726\n",
      "Inside Model: logits sum = 2953\n",
      "Inside Model: logits sum = 2944\n",
      "Inside Model: logits sum = 2568\n",
      "Inside Model: logits sum = 2552\n",
      "Outside Model: logits sum = 4353\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.24151831839970192\n",
      "epoch 5 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 5\n",
      "Checking for early-stopping\n",
      "Dev score is 0.0 lower than best Dev score (24.2)\n",
      "0 epochs left until Dev score to improve to avoid early-stopping!\n",
      "Early stopping!\n",
      "training done, time taken = 0h 1m 42s\n",
      "Best Dev score = 24.2\n",
      "Best epoch = 2\n"
     ]
    }
   ],
   "source": [
    "accelerate.notebook_launcher(\n",
    "    training_function, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
