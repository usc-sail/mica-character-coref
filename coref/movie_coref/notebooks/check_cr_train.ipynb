{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.movie_coref.data import (CharacterRecognitionDataset, \n",
    "                                                    CorefCorpus)\n",
    "\n",
    "import accelerate\n",
    "from accelerate import logging\n",
    "import collections\n",
    "import contextlib\n",
    "import logging as pylogging\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from transformers import (get_linear_schedule_with_warmup, AutoTokenizer,\n",
    "                          AutoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    \"\"\"General Metric class\"\"\"\n",
    "\n",
    "    @property\n",
    "    def score(self) -> float:\n",
    "        \"\"\"Main metric score used for comparison\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"General trainer class that uses huggingface \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                accelerator: accelerate.Accelerator,\n",
    "                logger: logging.MultiProcessAdapter,\n",
    "                model: nn.Module,\n",
    "                train_dataloader: DataLoader,\n",
    "                dev_dataloader: DataLoader,\n",
    "                optimizer: Optimizer,\n",
    "                use_scheduler: bool = False,\n",
    "                warmup_ratio: float = None,\n",
    "                warmup_steps: int = None,\n",
    "                max_epochs: int = 1,\n",
    "                max_grad_norm: float = None,\n",
    "                patience: int = 1,\n",
    "                log_batch_frequency: int = 1,\n",
    "                evaluate_train: bool = False,\n",
    "                save_model: bool = False,\n",
    "                save_tensors: bool = False,\n",
    "                save_tensors_name: list[str] = None,\n",
    "                save_dir: str = None\n",
    "                ) -> None:\n",
    "        \"\"\"Initializer for the general trainer class that uses accelerate to\n",
    "        train your model.\n",
    "\n",
    "        Args:\n",
    "            accelerator: Instance of the Accelerator class.\n",
    "            logger: Instance of the Accelerator logger used for distributed\n",
    "                logging.\n",
    "            model: Torch nn.Module subclass to train.\n",
    "            train_dataloader: Train set dataloader.\n",
    "            dev_dataloader: Dev set dataloader.\n",
    "            optimizer: Optimizer.\n",
    "            use_scheduler: Whether to use scheduler.\n",
    "            warmup_ratio: Ratio of training steps to use in the scheduler's\n",
    "                warmup. warmup_steps has to be None if you want to use this\n",
    "                parameter.\n",
    "            warmup_steps: Number of steps to use in the scheduler's warmup.\n",
    "                This parameter supercedes warmup_ratio.\n",
    "            max_epochs: Maximum number of epochs to train for.\n",
    "            max_grad_norm: Maximum norm of gradient to be used in gradient\n",
    "                clipping. If None, gradient clipping is not done.\n",
    "            patience: Maximum number of epochs to wait for development set\n",
    "                performance to improve before early-stopping.\n",
    "            log_batch_frequency: Training loop logs training loss and timing\n",
    "                information after every log_batch_frequency batches.\n",
    "            evaluate_train: Whether to evaluate on the training set.\n",
    "            save_model: Whether to save model after every epoch.\n",
    "            save_tensors: Whether to save the tensors of development set, along\n",
    "                with the logits.\n",
    "            save_tensors_name: List of tensor names which are to be saved. If\n",
    "                none, all tensors are saved.\n",
    "            save_dir: Directory to which the model weights, ground truth, and\n",
    "                predictions will be saved.\n",
    "        \"\"\"\n",
    "        self.accelerator = accelerator\n",
    "        self.logger = logger\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.dev_dataloader = dev_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.use_scheduler = use_scheduler\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_epochs = max_epochs\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.patience = patience\n",
    "        self.log_batch_frequency = log_batch_frequency\n",
    "        self.evaluate_train = evaluate_train\n",
    "        self.save_model = save_model\n",
    "        self.save_tensors = save_tensors\n",
    "        self.save_tensors_name = save_tensors_name\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            assert (self.warmup_ratio is not None or \n",
    "                    self.warmup_steps is not None), (\n",
    "                    \"Set warmup_ratio or warmup_steps \"\n",
    "                    \"if you are using scheduler\")\n",
    "        \n",
    "        if self.save_model or self.save_predictions:\n",
    "            assert self.save_dir is not None, (\n",
    "                \"Set save_dir if you are saving model and/or predictions\")\n",
    "        \n",
    "        self.n_training_samples = len(self.train_dataloader.dataset)\n",
    "        self.n_dev_samples = len(self.dev_dataloader.dataset)\n",
    "        self.model.eval()\n",
    "        self.model.device = self.accelerator.device\n",
    "    \n",
    "    def log(self, message):\n",
    "        \"\"\"Logging or printing\"\"\"\n",
    "        self.accelerator.print(message)\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def _timer(self, message):\n",
    "        \"\"\"Context manager for timing a codeblock\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.log(f\"Starting {message}\")\n",
    "        yield\n",
    "        time_taken = time.time() - start_time\n",
    "        time_taken_str = self._convert_float_seconds_to_time_string(time_taken)\n",
    "        self.log(f\"{message} done, time taken = {time_taken_str}\")\n",
    "\n",
    "    def _convert_float_seconds_to_time_string(self, seconds: float) -> str:\n",
    "        \"\"\"Convert seconds to h m s format\"\"\"\n",
    "        seconds = int(seconds)\n",
    "        minutes, seconds = seconds//60, seconds%60\n",
    "        hours, minutes = minutes//60, minutes%60\n",
    "        return f\"{hours}h {minutes}m {seconds}s\"\n",
    "    \n",
    "    def _save_model(self, model: nn.Module, directory: str):\n",
    "        \"\"\"Save model's weights to directory with filename `model.pt`.\n",
    "\n",
    "        Args:\n",
    "            model: Torch nn.Module.\n",
    "            directory: Directory where model's weights will be saved.\n",
    "        \"\"\"\n",
    "        self.accelerator.save(\n",
    "            model.state_dict(), os.path.join(directory, \"model.pt\"))\n",
    "\n",
    "    def _save_tensors(self, directory: str, **tensors):\n",
    "        \"\"\"Save the tensors returned from inference to directory.\n",
    "\n",
    "        Args:\n",
    "            directory: Directory where the tensors will be saved.\n",
    "            tensors: Dictionary of tensor name to tensor.\n",
    "        \"\"\"\n",
    "        for name, pt in tensors.items():\n",
    "            if self.save_tensors_name is None or name in self.save_tensors_name:\n",
    "                self.accelerator.save(pt, os.path.join(directory, f\"{name}.pt\"))\n",
    "    \n",
    "    def run(self):\n",
    "        best_dev_score = None\n",
    "        best_epoch = None\n",
    "        epochs_left = self.patience\n",
    "        save = self.save_model or self.save_tensors\n",
    "        \n",
    "        # Accelerate model, dataloaders, and optimizer\n",
    "        (self.model, self.train_dataloader, self.dev_dataloader,\n",
    "         self.optimizer) = (self.accelerator.prepare(\n",
    "            self.model, self.train_dataloader, self.dev_dataloader, \n",
    "            self.optimizer))\n",
    "\n",
    "        # Log number of training and inference batches, \n",
    "        # and number of training steps\n",
    "        n_train_batches = len(self.train_dataloader)\n",
    "        n_dev_batches = len(self.dev_dataloader)\n",
    "        effective_train_batch_size = round(\n",
    "            self.n_training_samples/n_train_batches)\n",
    "        effective_dev_batch_size = round(self.n_dev_samples/n_dev_batches)\n",
    "        n_training_steps = self.max_epochs * n_train_batches\n",
    "        self.log(\"Effective train batch size = \"\n",
    "                        f\"{effective_train_batch_size}\")\n",
    "        self.log(\"Effective dev batch size = \"\n",
    "                        f\"{effective_dev_batch_size}\")\n",
    "        self.log(f\"Number of training batches = {n_train_batches}\")\n",
    "        self.log(f\"Number of inference batches = {n_dev_batches}\")\n",
    "        self.log(f\"Number of training steps = {n_training_steps}\")\n",
    "\n",
    "        # Initialize and accelerate scheduler\n",
    "        if self.use_scheduler:\n",
    "            n_warmup_steps = self.warmup_steps if (\n",
    "                self.warmup_steps is not None) else (\n",
    "                    int(self.warmup_ratio * n_training_steps))\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer, num_warmup_steps=n_warmup_steps,\n",
    "                num_training_steps=n_training_steps)\n",
    "            scheduler = self.accelerator.prepare_scheduler(scheduler)\n",
    "            self.log(f\"Number of warmup steps = {n_warmup_steps}\")\n",
    "        \n",
    "        # Training and evaluation loop\n",
    "        with self._timer(\"training\"):\n",
    "            for epoch in range(self.max_epochs):\n",
    "                \n",
    "                if save:\n",
    "                    # Create epoch directories\n",
    "                    epoch_dir = os.path.join(\n",
    "                        self.save_dir, f\"epoch_{epoch + 1}\")\n",
    "                    epoch_dev_dir = os.path.join(epoch_dir, \"dev\")\n",
    "                    os.makedirs(epoch_dir, exist_ok=True)\n",
    "                    os.makedirs(epoch_dev_dir, exist_ok=True)\n",
    "\n",
    "                # Training for one epoch\n",
    "                with self._timer(f\"epoch {epoch + 1} training\"):\n",
    "                    self.model.train()\n",
    "                    running_batch_loss = []\n",
    "                    running_batch_train_time = []\n",
    "                    \n",
    "                    # Batch training loop\n",
    "                    for i, batch in enumerate(self.train_dataloader):\n",
    "                        batch_start_time = time.time()\n",
    "                        \n",
    "                        # One training step\n",
    "                        with self.accelerator.accumulate(self.model):\n",
    "                            self.optimizer.zero_grad()\n",
    "                            with self.accelerator.autocast():\n",
    "                                batch_loss = self.model(**batch)\n",
    "                            self.accelerator.backward(batch_loss)\n",
    "                            if self.optimizer.gradient_state.sync_gradients:\n",
    "                                self.accelerator.clip_grad_norm_(\n",
    "                                    self.model.parameters(), self.max_grad_norm)\n",
    "                            self.optimizer.step()\n",
    "                            if self.use_scheduler and (\n",
    "                            not self.accelerator.optimizer_step_was_skipped):\n",
    "                                scheduler.step()\n",
    "\n",
    "                        batch_time_taken = time.time() - batch_start_time\n",
    "                        running_batch_loss.append(batch_loss.detach().item())\n",
    "                        running_batch_train_time.append(batch_time_taken)\n",
    "\n",
    "                        # Log after log_batch_frequency batches\n",
    "                        if (i + 1) % self.log_batch_frequency == 0:\n",
    "                            average_batch_loss = np.mean(running_batch_loss)\n",
    "                            average_batch_train_time = np.mean(\n",
    "                                running_batch_train_time)\n",
    "                            estimated_time_remaining = (\n",
    "                                self._convert_float_seconds_to_time_string(\n",
    "                                average_batch_train_time * (\n",
    "                                    n_train_batches-i-1)))\n",
    "                            average_batch_train_time_str = (\n",
    "                                self._convert_float_seconds_to_time_string(\n",
    "                                average_batch_train_time))\n",
    "                            self.log(f\"Batch {i + 1}\")\n",
    "                            self.log(\n",
    "                                \"Average training loss @ batch = \"\n",
    "                                f\"{average_batch_loss:.4f}\")\n",
    "                            self.log(\n",
    "                                \"Average training time taken @ batch = \"\n",
    "                                f\"{average_batch_train_time_str}\")\n",
    "                            self.log(\n",
    "                                \"Estimated training time remaining for epoch \"\n",
    "                                f\"{epoch + 1} = {estimated_time_remaining}\")\n",
    "                            running_batch_loss = []\n",
    "                            running_batch_train_time = []\n",
    "\n",
    "                # Wait for all process to complete\n",
    "                self.accelerator.wait_for_everyone()\n",
    "\n",
    "                # Save model\n",
    "                if self.save_model:\n",
    "                    self.log(f\"Saving model after epoch {epoch + 1}\")\n",
    "                    unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "                    self._save_model(unwrapped_model, epoch_dir)\n",
    "\n",
    "                # Inference and evaluation on training set\n",
    "                if self.evaluate_train:\n",
    "                    with self._timer(\n",
    "                        f\"epoch {epoch + 1} training inference and evaluation\"):\n",
    "                        train_inference_output = self._infer(\n",
    "                            self.train_dataloader, self.model)\n",
    "                        train_metric = self.evaluate(**train_inference_output)\n",
    "                        self.log(\n",
    "                            f\"Training Performance = {train_metric.score}\")\n",
    "                    self.accelerator.wait_for_everyone()\n",
    "            \n",
    "                # Inference and evaluation on dev set\n",
    "                with self._timer(\n",
    "                    f\"epoch {epoch + 1} dev inference and evaluation\"):\n",
    "                    dev_inference_output = self._infer(\n",
    "                        self.dev_dataloader, self.model)\n",
    "                    dev_metric = self.evaluate(**dev_inference_output)\n",
    "                    self.log(f\"Dev Performance = {dev_metric.score}\")\n",
    "                self.accelerator.wait_for_everyone()\n",
    "                if self.save_tensors:\n",
    "                    self.log(\n",
    "                        f\"Saving dev tensors after epoch {epoch + 1}\")\n",
    "                    self._save_tensors(epoch_dev_dir, **dev_inference_output)\n",
    "\n",
    "                # Early-stopping\n",
    "                self.log(\"Checking for early-stopping\")\n",
    "                dev_score = dev_metric.score\n",
    "                if best_dev_score is None or dev_score > best_dev_score:\n",
    "                    epochs_left = self.patience\n",
    "                    best_epoch = epoch + 1\n",
    "                    if best_dev_score is not None:\n",
    "                        delta = 100 * (dev_score - best_dev_score)\n",
    "                        self.log(f\"Dev score improved by {delta:.1f}\")\n",
    "                    best_dev_score = dev_score\n",
    "                else:\n",
    "                    epochs_left -= 1\n",
    "                    delta = 100 * (best_dev_score - dev_score)\n",
    "                    self.log(\n",
    "                        f\"Dev score is {delta:.1f} lower than best Dev score \"\n",
    "                        f\"({100*best_dev_score:.1f})\")\n",
    "                    self.log(\n",
    "                        f\"{epochs_left} epochs left until Dev score to improve to\"\n",
    "                        \" avoid early-stopping!\")\n",
    "                if epochs_left == 0:\n",
    "                    self.log(\"Early stopping!\")\n",
    "                    break\n",
    "\n",
    "                self.log(f\"Epoch {epoch + 1} done\")\n",
    "\n",
    "        self.log(f\"Best Dev score = {100*best_dev_score:.1f}\")\n",
    "        self.log(f\"Best epoch = {best_epoch}\")\n",
    "    \n",
    "    def _infer(self, dataloader: DataLoader, model: nn.Module) -> (\n",
    "        dict[str, torch.Tensor]):\n",
    "        \"\"\"Run inference on the dataloader.\n",
    "        Args:\n",
    "            dataloader: PyTorch dataloader.\n",
    "            model: PyTorch module.\n",
    "        \n",
    "        Returns:\n",
    "            Labels and predictions tensors.\n",
    "        \"\"\"\n",
    "        # Initialize variables\n",
    "        model.eval()\n",
    "        tensors: dict[str, list[torch.Tensor]] = collections.defaultdict(list)\n",
    "        n_batches = len(dataloader)\n",
    "        self.log(f\"Number of inference batches = {n_batches}\")\n",
    "\n",
    "        # Inference Loop\n",
    "        with self._timer(\"inference\"), torch.no_grad():\n",
    "            running_batch_times = []\n",
    "            for i, batch in enumerate(dataloader):\n",
    "\n",
    "                # One inference step\n",
    "                start_time = time.time()\n",
    "                batch_logits = model(**batch)\n",
    "                batch[\"logits\"] = batch_logits\n",
    "                batch = self.accelerator.gather_for_metrics(batch)\n",
    "                for name, tensor in batch.items():\n",
    "                    tensors[name].append(tensor)\n",
    "                time_taken = time.time() - start_time\n",
    "                running_batch_times.append(time_taken)\n",
    "\n",
    "                # Log after log_batch_frequency batches\n",
    "                if (i + 1) % self.log_batch_frequency == 0:\n",
    "                    average_time_per_batch = np.mean(running_batch_times)\n",
    "                    estimated_time_remaining = (n_batches - i - 1) * (\n",
    "                                                average_time_per_batch)\n",
    "                    average_time_per_batch_str = (\n",
    "                        self._convert_float_seconds_to_time_string(\n",
    "                            average_time_per_batch))\n",
    "                    estimated_time_remaining_str = (\n",
    "                        self._convert_float_seconds_to_time_string(\n",
    "                            estimated_time_remaining))\n",
    "                    running_batch_times = []\n",
    "\n",
    "                    self.log(f\"Batch {i + 1}\")\n",
    "                    self.log(\"Average inference time @ batch = \"\n",
    "                                f\"{average_time_per_batch_str}\")\n",
    "                    self.log(\"Estimated inference time remaining = \"\n",
    "                                f\"{estimated_time_remaining_str}\")\n",
    "\n",
    "        # Concat tensors\n",
    "        output: dict[str, torch.Tensor] = {}\n",
    "        for name, tensor_list in tensors.items():\n",
    "            output[name] = torch.cat(tensor_list, dim=0)\n",
    "        return output\n",
    "    \n",
    "    def evaluate(self, **tensors) -> Metric:\n",
    "        \"\"\"Evaluate the output of inference\"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRecognition(nn.Module):\n",
    "    \"\"\"Character Recognition Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 encoder_name: str,\n",
    "                 num_parse_tags: int,\n",
    "                 parse_tag_embedding_size: int,\n",
    "                 gru_hidden_size: int,\n",
    "                 gru_num_layers: int,\n",
    "                 gru_dropout: float,\n",
    "                 gru_bidirectional: bool,\n",
    "                 num_labels: int) -> None:\n",
    "        \"\"\"Initializer for Character Recognition Model.\n",
    "\n",
    "        Args:\n",
    "            encoder_name: Language model encoder name from transformers hub\n",
    "                e.g. bert-base-cased\n",
    "            num_parse_tags: Parse tag set size\n",
    "            parse_tag_embedding_size: Embedding size of the parse tags\n",
    "            gru_hidden_size: Hidden size of the GRU\n",
    "            gru_num_layers: Number of layers of the GRU\n",
    "            gru_dropout: Dropout used between the GPU layers\n",
    "            gru_bidirectional: If true, the GRU is bidirectional\n",
    "            num_labels: Number of labels in the label set. 2 if label_type =\n",
    "                \"head\" or 3 if label_type = \"span\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            encoder_name, add_pooling_layer=False)\n",
    "        self.encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        self.subtoken = nn.Linear(self.encoder_hidden_size, 1)\n",
    "        self.parse_embedding = nn.Embedding(\n",
    "            num_parse_tags, parse_tag_embedding_size)\n",
    "        self.gru_input_size = (self.encoder_hidden_size +\n",
    "                               parse_tag_embedding_size)\n",
    "        self.gru_output_size = gru_hidden_size * (1 + int(gru_bidirectional))\n",
    "        self.gru = nn.GRU(self.gru_input_size, gru_hidden_size,\n",
    "                          num_layers=gru_num_layers, batch_first=True,\n",
    "                          dropout=gru_dropout, bidirectional=gru_bidirectional)\n",
    "        self.output = nn.Linear(self.gru_output_size, num_labels)\n",
    "        self._device = \"cpu\"\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"Getter for model device.\"\"\"\n",
    "        return self._device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, device):\n",
    "        \"\"\"Setter for model device. Used by accelerate.\"\"\"\n",
    "        self._device = device\n",
    "    \n",
    "    def forward(self, subtoken_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                token_offset: torch.Tensor, parse_ids: torch.Tensor,\n",
    "                labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward propagation for the Character Recognition Model.\n",
    "\n",
    "        Args:\n",
    "            subtoken_ids: `batch_size x max_n_subtokens` Long Tensor\n",
    "            attention_mask: `batch_size x max_n_subtokens` Float/Long Tensor\n",
    "            token_offset: `batch_size x max_n_tokens x 2` Long Tensor\n",
    "            parse_ids: `batch_size x max_n_tokens` Long Tensor\n",
    "            labels: `batch_size x max_n_tokens` Long Tensor\n",
    "        \n",
    "        Returns:\n",
    "            Return the loss value if model is begin trained, else the logits \n",
    "            `batch_size x max_n_tokens x num_labels` Float Tensor\n",
    "        \"\"\"\n",
    "        batch_size = len(subtoken_ids)\n",
    "\n",
    "        # subtoken_embedding = batch_size x max_n_subtokens x encoder_hidden_size\n",
    "        encoder_output = self.encoder(subtoken_ids, attention_mask)\n",
    "        subtoken_embedding = encoder_output.last_hidden_state\n",
    "\n",
    "        # subtoken_attn = batch_size * max_n_tokens x batch_size * max_n_subtokens\n",
    "        _subtoken_embedding = subtoken_embedding.view(-1, self.encoder_hidden_size)\n",
    "        subtoken_attn = self._attn_scores(_subtoken_embedding,\n",
    "                                          token_offset.view(-1, 2))\n",
    "        \n",
    "        # token_embedding = batch_size x max_n_tokens x encoder_hidden_size\n",
    "        token_embedding = torch.mm(\n",
    "            subtoken_attn, _subtoken_embedding).reshape(\n",
    "                batch_size, -1, self.encoder_hidden_size)\n",
    "        \n",
    "        # gru_input = batch_size x max_n_tokens x (encoder_hidden_size +\n",
    "        # parse_tag_embedding_size)\n",
    "        parse_input = self.parse_embedding(parse_ids)\n",
    "        gru_input = torch.cat((token_embedding, parse_input), dim=2).contiguous()\n",
    "\n",
    "        # logits = batch_size x max_n_tokens x num_labels\n",
    "        gru_output, _ = self.gru(gru_input)\n",
    "        logits = self.output(gru_output)\n",
    "\n",
    "        if self.training:\n",
    "            token_attention_mask = torch.any(subtoken_attn > 0, dim=1).reshape(\n",
    "                batch_size, -1)\n",
    "            loss = compute_loss(logits, labels, token_attention_mask,\n",
    "                                self.num_labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def _attn_scores(self,\n",
    "                     subtoken_embeddings: torch.FloatTensor,\n",
    "                     token_offset: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\" Calculates attention scores for each of the subtokens of a token.\n",
    "\n",
    "        Args:\n",
    "            subtoken_embedding: `n_subtokens x embedding_size` Float Tensor,\n",
    "                embeddings for each subtoken\n",
    "            token_offset: `n_tokens x 2` Long Tensor, subtoken offset of each\n",
    "                token\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: `n_tokens x n_subtokens` Float Tensor, attention\n",
    "            weights for each subtoken of a token\n",
    "        \"\"\"\n",
    "        n_subtokens, n_tokens = len(subtoken_embeddings), len(token_offset)\n",
    "        token_begin, token_end = token_offset[:,0], token_offset[:,1]\n",
    "        \n",
    "        # attn_mask: n_tokens x n_subtokens, contains -∞ for subtokens outside\n",
    "        # the token's offsets and 0 for subtokens inside the token's offsets\n",
    "        attn_mask = torch.arange(0, n_subtokens, device=self.device).expand(\n",
    "            (n_tokens, n_subtokens))\n",
    "        attn_mask = ((attn_mask >= token_begin.unsqueeze(1)) * \n",
    "                     (attn_mask <= token_end.unsqueeze(1)))\n",
    "        attn_mask = torch.log(attn_mask.to(torch.float))\n",
    "\n",
    "        # attn_scores: 1 x n_subtokens\n",
    "        attn_scores = self.subtoken(subtoken_embeddings).T\n",
    "\n",
    "        # attn_scores: n_tokens x n_subtokens\n",
    "        attn_scores = attn_scores.expand((n_tokens, n_subtokens))\n",
    "\n",
    "        # -∞ for subtokens outside the token's offsets and attn_scores for\n",
    "        # inside the token's offsets\n",
    "        attn_scores = attn_mask + attn_scores\n",
    "        del attn_mask\n",
    "\n",
    "        # subtoken_attn contains 0 for subtokens outside the token's offsets\n",
    "        subtoken_attn = torch.softmax(attn_scores, dim=1)\n",
    "        return subtoken_attn\n",
    "    \n",
    "def compute_loss(\n",
    "    logits: torch.FloatTensor, label_ids: torch.LongTensor,\n",
    "    attn_mask: torch.FloatTensor, n_labels: int) -> torch.FloatTensor:\n",
    "    \"\"\"Compute cross entropy loss\"\"\"\n",
    "    active_labels = label_ids[attn_mask == 1.]\n",
    "    active_logits = logits.flatten(0, 1)[attn_mask.flatten() == 1.]\n",
    "    label_distribution = torch.bincount(active_labels,\n",
    "        minlength=n_labels)\n",
    "    class_weight = torch.sqrt(len(active_labels)/(1 + label_distribution))\n",
    "    cross_entrop_loss_fn = nn.CrossEntropyLoss(weight=class_weight, \n",
    "        reduction=\"mean\")\n",
    "    loss = cross_entrop_loss_fn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRecognitionMetric(Metric):\n",
    "\n",
    "    def __init__(self, precision, recall) -> None:\n",
    "        super().__init__()\n",
    "        self.precision = precision\n",
    "        self.recall = recall\n",
    "    \n",
    "    @property\n",
    "    def score(self) -> float:\n",
    "        return 2*self.precision*self.recall/(self.precision+self.recall+1e-23)\n",
    "\n",
    "class CharacterRecognitionTrainer(Trainer):\n",
    "\n",
    "    def evaluate(self, **tensors) -> Metric:\n",
    "        logits, labels, offset = (\n",
    "            tensors[\"logits\"], tensors[\"labels\"], tensors[\"token_offset\"])\n",
    "        pred = logits.argmax(dim=2)\n",
    "        mask = ~((offset[:,:,0] == 0) & (offset[:,:,1] == 0))\n",
    "        pred = pred[mask]\n",
    "        labels = labels[mask]\n",
    "        tp = ((labels == pred) & (pred != 0)).sum().item()\n",
    "        fp = ((labels != pred) & (pred != 0)).sum().item()\n",
    "        fn = ((labels != pred) & (labels != 0)).sum().item()\n",
    "        precision = tp/(tp + fp + 1e-23)\n",
    "        recall = tp/(tp + fn + 1e-23)\n",
    "        return CharacterRecognitionMetric(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "    train_batch_size = 16\n",
    "    infer_batch_size = 16\n",
    "    parse_tag_embedding_size = 32\n",
    "    encoder_name = \"roberta-base\"\n",
    "    gru_hidden_size = 768\n",
    "    gru_num_layers = 2\n",
    "    gru_dropout = 0.2\n",
    "    gru_bidirectional = True\n",
    "    lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    use_scheduler = True\n",
    "    warmup_ratio = 0.1\n",
    "    warmup_steps = None\n",
    "    max_epochs = 10\n",
    "    max_grad_norm = 0.1\n",
    "    patience = 3\n",
    "    log_batch_frequency = 5\n",
    "    evaluate_train = False\n",
    "    save_model = True\n",
    "    save_tensors = True\n",
    "    save_tensors_name = [\"logits\", \"labels\"]\n",
    "    save_dir = (\"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/\"\n",
    "                \"character_recognition/\")\n",
    "\n",
    "    accelerator = accelerate.Accelerator(mixed_precision=\"fp16\")\n",
    "    logger = logging.get_logger(\"\")\n",
    "\n",
    "    corpus = CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                        \"movie_coref/results/regular/movie.jsonlines\")\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(encoder_name, use_fast=True)\n",
    "    dataset = CharacterRecognitionDataset(\n",
    "        corpus, roberta_tokenizer, seq_length=256, obey_scene_boundaries=False)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset, batch_size=train_batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(\n",
    "        dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    model = CharacterRecognition(encoder_name=encoder_name,\n",
    "                    num_parse_tags=len(dataset.parse_tag_to_id),\n",
    "                    parse_tag_embedding_size=parse_tag_embedding_size,\n",
    "                    gru_hidden_size=gru_hidden_size,\n",
    "                    gru_num_layers=gru_num_layers,\n",
    "                    gru_dropout=gru_dropout,\n",
    "                    gru_bidirectional=gru_bidirectional,\n",
    "                    num_labels=2)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    trainer = CharacterRecognitionTrainer(\n",
    "        accelerator,\n",
    "        logger,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        dev_dataloader,\n",
    "        optimizer,\n",
    "        use_scheduler = use_scheduler,\n",
    "        warmup_ratio = warmup_ratio,\n",
    "        warmup_steps = warmup_steps,\n",
    "        max_epochs = max_epochs,\n",
    "        max_grad_norm = max_grad_norm,\n",
    "        patience = patience,\n",
    "        log_batch_frequency = log_batch_frequency,\n",
    "        evaluate_train = evaluate_train,\n",
    "        save_model = save_model,\n",
    "        save_tensors = save_tensors,\n",
    "        save_tensors_name = save_tensors_name,\n",
    "        save_dir = save_dir)\n",
    "    \n",
    "    trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective train batch size = 61\n",
      "Effective dev batch size = 61\n",
      "Number of training batches = 13\n",
      "Number of inference batches = 13\n",
      "Number of training steps = 130\n",
      "Number of warmup steps = 13\n",
      "Starting training\n",
      "Starting epoch 1 training\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.6506\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 1 = 0h 0m 7s\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.5904\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 1 = 0h 0m 2s\n",
      "epoch 1 training done, time taken = 0h 0m 12s\n",
      "Saving model after epoch 1\n",
      "Starting epoch 1 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.0\n",
      "epoch 1 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 1\n",
      "Checking for early-stopping\n",
      "Epoch 1 done\n",
      "Starting epoch 2 training\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.5890\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 2 = 0h 0m 7s\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.5817\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 2 = 0h 0m 2s\n",
      "epoch 2 training done, time taken = 0h 0m 11s\n",
      "Saving model after epoch 2\n",
      "Starting epoch 2 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.0\n",
      "epoch 2 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 2\n",
      "Checking for early-stopping\n",
      "Dev score is 0.0 lower than best Dev score (0.0)\n",
      "2 epochs left until Dev score to improve to avoid early-stopping!\n",
      "Epoch 2 done\n",
      "Starting epoch 3 training\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.5874\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 3 = 0h 0m 7s\n",
      "Batch 10\n",
      "Average training loss @ batch = 0.5828\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 3 = 0h 0m 2s\n",
      "epoch 3 training done, time taken = 0h 0m 11s\n",
      "Saving model after epoch 3\n",
      "Starting epoch 3 dev inference and evaluation\n",
      "Number of inference batches = 13\n",
      "Starting inference\n",
      "Batch 5\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "Batch 10\n",
      "Average inference time @ batch = 0h 0m 0s\n",
      "Estimated inference time remaining = 0h 0m 0s\n",
      "inference done, time taken = 0h 0m 3s\n",
      "Dev Performance = 0.0\n",
      "epoch 3 dev inference and evaluation done, time taken = 0h 0m 3s\n",
      "Saving dev tensors after epoch 3\n",
      "Checking for early-stopping\n",
      "Dev score is 0.0 lower than best Dev score (0.0)\n",
      "1 epochs left until Dev score to improve to avoid early-stopping!\n",
      "Epoch 3 done\n",
      "Starting epoch 4 training\n",
      "Batch 5\n",
      "Average training loss @ batch = 0.5812\n",
      "Average training time taken @ batch = 0h 0m 0s\n",
      "Estimated training time remaining for epoch 4 = 0h 0m 7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sbaruah_usc_edu/mica_text_coref/coref/movie_coref/notebooks/check_cr_train.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsailnlp.us-west1-b.sail-ccmi/home/sbaruah_usc_edu/mica_text_coref/coref/movie_coref/notebooks/check_cr_train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m accelerate\u001b[39m.\u001b[39;49mnotebook_launcher(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsailnlp.us-west1-b.sail-ccmi/home/sbaruah_usc_edu/mica_text_coref/coref/movie_coref/notebooks/check_cr_train.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     training_function, num_processes\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, mixed_precision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfp16\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/coreference/lib/python3.10/site-packages/accelerate/launchers.py:127\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, use_fp16, mixed_precision, use_port)\u001b[0m\n\u001b[1;32m    124\u001b[0m         launcher \u001b[39m=\u001b[39m PrepareForLaunch(function, distributed_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMULTI_GPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLaunching training on \u001b[39m\u001b[39m{\u001b[39;00mnum_processes\u001b[39m}\u001b[39;00m\u001b[39m GPUs.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m         start_processes(launcher, args\u001b[39m=\u001b[39;49margs, nprocs\u001b[39m=\u001b[39;49mnum_processes, start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    129\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39m# No need for a distributed launch otherwise as it's either CPU or one GPU.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/anaconda3/envs/coreference/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[39m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39;49mjoin():\n\u001b[1;32m    199\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/coreference/lib/python3.10/site-packages/torch/multiprocessing/spawn.py:109\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m ready \u001b[39m=\u001b[39m multiprocessing\u001b[39m.\u001b[39;49mconnection\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinels\u001b[39m.\u001b[39;49mkeys(),\n\u001b[1;32m    111\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m error_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[39mfor\u001b[39;00m sentinel \u001b[39min\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/anaconda3/envs/coreference/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    937\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/coreference/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accelerate.notebook_launcher(\n",
    "    training_function, num_processes=4, mixed_precision=\"fp16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.Size([792, 256]), torch.float32, torch.Size([792, 256, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.load(\"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/character_recognition/epoch_1/dev/labels.pt\")\n",
    "logits = torch.load(\"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/character_recognition/epoch_1/dev/logits.pt\")\n",
    "labels.dtype, labels.shape, logits.dtype, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = logits.argmax(dim=2)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(176964, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((labels == pred) & (labels == 0)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                     \"movie_coref/results/regular/movie.jsonlines\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)\n",
    "dataset = CharacterRecognitionDataset(\n",
    "    corpus, roberta_tokenizer, seq_length=256, obey_scene_boundaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = dataset.token_offset\n",
    "mask = ~((offset[:,:,0] == 0) & (offset[:,:,1] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(201804)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask == 1.).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
