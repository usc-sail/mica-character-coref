{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a09127-8442-4fc4-8d31-be673756cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import os\n",
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee4b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/proj/sbaruah/data/mica_text_coref\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.getenv(\"DATA_DIR\"), \"mica_text_coref\") # type: ignore\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366706e9-5822-4957-926c-c7f3f6a6da3d",
   "metadata": {},
   "source": [
    "# Spans from word clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ce3a7b-19fa-4ef9-96e1-f2d1292bd116",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = torch.load(os.path.join(data_dir, \"movie_coref/results/coreference/baselines/preprocess_none.genre_bc.split_5120.overlap_512.train_wl.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a72b71b5-9b77-4bab-ae21-bd613c5c8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(os.path.join(data_dir, \"movie_coref/results/coreference/baselines/preprocess_none.genre_bc.split_5120.overlap_512.train_wl.jsonlines\"), mode = \"r\") as reader:\n",
    "    docs = {doc[\"document_id\"]: doc for doc in reader} # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c8acb0-f12f-47ba-8d90-e6debdac6998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['movie', 'rater', 'token', 'pos', 'ner', 'parse', 'speaker', 'document_id', 'cased_words', 'offset', 'clusters', 'sent_offset', 'sent_id', 'word_clusters', 'span_clusters'])\n",
      "dict_keys(['coref_scores', 'coref_y', 'top_indices', 'word_clusters', 'span_clusters', 'span_scores', 'span_y'])\n"
     ]
    }
   ],
   "source": [
    "print(docs[\"bc_avengers_endgame_1\"].keys())\n",
    "print(pt[\"bc_avengers_endgame_1\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56389f71-bd67-45f8-bc91-b04e16114c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id = bc_avengers_endgame_1, 922 heads, 922 spans\n",
      "doc_id = bc_avengers_endgame_2, 957 heads, 957 spans\n",
      "doc_id = bc_avengers_endgame_3, 929 heads, 929 spans\n",
      "doc_id = bc_avengers_endgame_4, 1006 heads, 1006 spans\n",
      "doc_id = bc_avengers_endgame_5, 1001 heads, 1001 spans\n",
      "doc_id = bc_avengers_endgame_6, 984 heads, 984 spans\n",
      "doc_id = bc_avengers_endgame_7, 956 heads, 956 spans\n",
      "doc_id = bc_avengers_endgame_8, 683 heads, 683 spans\n",
      "doc_id = bc_dead_poets_society_1, 1053 heads, 1053 spans\n",
      "doc_id = bc_dead_poets_society_2, 990 heads, 990 spans\n",
      "doc_id = bc_dead_poets_society_3, 1033 heads, 1033 spans\n",
      "doc_id = bc_dead_poets_society_4, 1068 heads, 1068 spans\n",
      "doc_id = bc_dead_poets_society_5, 1152 heads, 1152 spans\n",
      "doc_id = bc_dead_poets_society_6, 725 heads, 725 spans\n",
      "doc_id = bc_john_wick_1, 776 heads, 776 spans\n",
      "doc_id = bc_john_wick_2, 786 heads, 786 spans\n",
      "doc_id = bc_john_wick_3, 841 heads, 841 spans\n",
      "doc_id = bc_john_wick_4, 813 heads, 813 spans\n",
      "doc_id = bc_john_wick_5, 711 heads, 711 spans\n",
      "doc_id = bc_john_wick_6, 323 heads, 323 spans\n",
      "doc_id = bc_prestige_1, 929 heads, 929 spans\n",
      "doc_id = bc_prestige_2, 1084 heads, 1084 spans\n",
      "doc_id = bc_prestige_3, 955 heads, 955 spans\n",
      "doc_id = bc_prestige_4, 1101 heads, 1101 spans\n",
      "doc_id = bc_prestige_5, 1158 heads, 1158 spans\n",
      "doc_id = bc_prestige_6, 1117 heads, 1117 spans\n",
      "doc_id = bc_prestige_7, 1086 heads, 1086 spans\n",
      "doc_id = bc_prestige_8, 749 heads, 749 spans\n",
      "doc_id = bc_quiet_place_1, 750 heads, 750 spans\n",
      "doc_id = bc_quiet_place_2, 697 heads, 697 spans\n",
      "doc_id = bc_quiet_place_3, 825 heads, 825 spans\n",
      "doc_id = bc_quiet_place_4, 814 heads, 814 spans\n",
      "doc_id = bc_quiet_place_5, 904 heads, 904 spans\n",
      "doc_id = bc_quiet_place_6, 902 heads, 902 spans\n",
      "doc_id = bc_quiet_place_7, 127 heads, 127 spans\n",
      "doc_id = bc_zootopia_1, 827 heads, 827 spans\n",
      "doc_id = bc_zootopia_2, 882 heads, 882 spans\n",
      "doc_id = bc_zootopia_3, 914 heads, 914 spans\n",
      "doc_id = bc_zootopia_4, 921 heads, 921 spans\n",
      "doc_id = bc_zootopia_5, 933 heads, 933 spans\n",
      "doc_id = bc_zootopia_6, 733 heads, 733 spans\n"
     ]
    }
   ],
   "source": [
    "for doc_id, pt_doc in pt.items():\n",
    "    word_clusters = pt_doc[\"word_clusters\"]\n",
    "    span_clusters = pt_doc[\"span_clusters\"]\n",
    "    heads = [word for cluster in word_clusters for word in cluster]\n",
    "    spans = [span for cluster in span_clusters for span in cluster]\n",
    "    print(f\"doc_id = {doc_id}, {len(heads)} heads, {len(spans)} spans\")\n",
    "    for head, span in zip(heads, spans):\n",
    "        if head < span[0] or head >= span[1]:\n",
    "            print(head, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904f991-77bd-4b94-b769-6b24e6df2103",
   "metadata": {},
   "source": [
    "# Merge coref scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2728468f-7564-4e6d-a350-b0303695e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_coref_scores(corefs: list[torch.Tensor], inds: list[torch.Tensor], overlap_lens: list[int], strategy: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Combine corefs and inds into a single coref and ind tensor.\n",
    "    \n",
    "    Args:\n",
    "        corefs: list[tensor[*, k + 1]]\n",
    "        inds: list[tensor[*, k]]\n",
    "        overlap_lens: list[int]\n",
    "        strategy: Can be one of \"before\", \"after\", \"average\", \"max\", \"min\", or \"none\"\n",
    "        \n",
    "    Return:\n",
    "        coref: [n, 2k + 1]\n",
    "        top_indices: [n, 2k]\n",
    "    \"\"\"\n",
    "    # Assertions\n",
    "    assert len(corefs) > 0, \"Number of coref tensors should be atleast 1\"\n",
    "    assert len(corefs) == len(inds), \"Number of coref tensors should equal number of indices tensors\"\n",
    "    if len(corefs) == 1: return corefs[0], inds[0]\n",
    "    assert len(overlap_lens) == len(corefs) - 1, \"Number of overlap lengths should equal one less than the number of coref tensors\"\n",
    "\n",
    "    # Intialize\n",
    "    n = sum([len(coref) - overlap_len for coref, overlap_len in zip(corefs[:-1], overlap_lens)]) + len(corefs[-1])\n",
    "    k = inds[0].shape[1]\n",
    "    device = corefs[0].device\n",
    "    coref = torch.full((n, 2*k), fill_value=-torch.inf, device=device)\n",
    "    ind = torch.full((n, 2*k), fill_value=-1, device=device)\n",
    "    coref_start, coref_end = 0, 0\n",
    "    overlap_lens.extend([0, 0])\n",
    "\n",
    "    # Combine\n",
    "    for i in range(len(corefs)):\n",
    "        assert len(corefs[i]) - overlap_lens[i - 1] - overlap_lens[i] > 0, \"Atmost two segments should overlap\"\n",
    "        coref_start, coref_end = coref_end, coref_end + len(corefs[i]) - overlap_lens[i - 1] - overlap_lens[i]\n",
    "        start, end = overlap_lens[i - 1], len(corefs[i]) - overlap_lens[i]\n",
    "\n",
    "        # Non-overlapping\n",
    "        coref[coref_start: coref_end, :k] = corefs[i][start: end, 1:]\n",
    "        ind[coref_start: coref_end, :k] = inds[i][start: end]\n",
    "\n",
    "        # Overlapping\n",
    "        coref_start, coref_end = coref_end, coref_end + overlap_lens[i]\n",
    "        if strategy != \"none\" and i < len(corefs) - 1:\n",
    "            for j in range(overlap_lens[i]):\n",
    "                heads_x, heads_y = inds[i][end + j].tolist(), inds[i + 1][j].tolist()\n",
    "                scores_x, scores_y = corefs[i][end + j, 1:].tolist(), corefs[i + 1][j, 1:].tolist()\n",
    "                head_to_score = {h: s for h, s in zip(heads_x, scores_x) if s != -torch.inf}\n",
    "                for h, s in zip(heads_y, scores_y):\n",
    "                    if s != -torch.inf:\n",
    "                        if h in head_to_score:\n",
    "                            if strategy == \"after\": head_to_score[h] = s\n",
    "                            elif strategy == \"mean\": head_to_score[h] = 0.5 * (head_to_score[h] + s)\n",
    "                            elif strategy == \"max\": head_to_score[h] = max(head_to_score[h], s)\n",
    "                            elif strategy == \"min\": head_to_score[h] = min(head_to_score[h], s)\n",
    "                        else: head_to_score[h] = s\n",
    "                for l, (h, s) in enumerate(head_to_score.items()):\n",
    "                    coref[coref_start + j, l] = s\n",
    "                    ind[coref_start + j, l] = h\n",
    "        else:\n",
    "            coref[coref_start: coref_end, :k] = corefs[i][end:, 1:]\n",
    "            ind[coref_start: coref_end, :k] = inds[i][end:]\n",
    "\n",
    "    # Add dummy\n",
    "    dummy = torch.zeros((n, 1), device=coref.device)\n",
    "    coref = torch.cat((dummy, coref), dim=1)\n",
    "    return coref, ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a20298-81ee-493a-8edc-9a246b568a2c",
   "metadata": {},
   "source": [
    "# Get clusters from fused predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5029b59a-06ac-4ae8-965c-6a1e0882070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}\n",
    "strategy_arr = [\"before\", \"after\", \"average\", \"max\", \"min\", \"none\"]\n",
    "strategy = \"max\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7801bd98-a694-480e-aa91-d55a4704bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_indices_heads(pt: dict, offset: tuple[int, int]) -> tuple[torch.Tensor, torch.Tensor, dict[int, tuple[int, int]]]:\n",
    "    coref, ind, word_clusters, span_cluster = pt[\"coref_scores\"], pt[\"top_indices\"], pt[\"word_clusters\"], pt[\"span_clusters\"]\n",
    "    ind = ind + offset[0]\n",
    "    heads = [word + offset[0] for cluster in word_clusters for word in cluster]\n",
    "    spans = [(p + offset[0], q + offset[0]) for cluster in span_clusters for p, q in cluster]\n",
    "    head2span = {head: span for head, span in zip(heads, spans)}\n",
    "    return coref, ind, head2span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf1c256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode:\n",
    "    def __init__(self, node_id: int):\n",
    "        self.id = node_id\n",
    "        self.links: set[GraphNode] = set()\n",
    "        self.visited = False\n",
    "\n",
    "    def link(self, another: \"GraphNode\"):\n",
    "        self.links.add(another)\n",
    "        another.links.add(self)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "293c7021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(scores: torch.Tensor, top_indices: torch.Tensor):\n",
    "    antecedents = scores.argmax(dim=1) - 1\n",
    "    not_dummy = antecedents >= 0\n",
    "    coref_span_heads = torch.arange(0, len(scores))[not_dummy]\n",
    "    antecedents = top_indices[coref_span_heads, antecedents[not_dummy]]\n",
    "\n",
    "    nodes = [GraphNode(i) for i in range(len(scores))]\n",
    "    for i, j in zip(coref_span_heads.tolist(), antecedents.tolist()):\n",
    "        nodes[i].link(nodes[j])\n",
    "        assert nodes[i] is not nodes[j]\n",
    "\n",
    "    clusters = []\n",
    "    for node in nodes:\n",
    "        if len(node.links) > 0 and not node.visited:\n",
    "            cluster = []\n",
    "            stack = [node]\n",
    "            while stack:\n",
    "                current_node = stack.pop()\n",
    "                current_node.visited = True\n",
    "                cluster.append(current_node.id)\n",
    "                stack.extend(link for link in current_node.links if not link.visited)\n",
    "            assert len(cluster) > 1\n",
    "            clusters.append(sorted(cluster))\n",
    "    return sorted(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1c955b0-4691-4d0e-996b-3ac2d2af90c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess=none genre=bc split_len=2048 overlap_len=128\n",
      "\n",
      "avengers_endgame\n",
      "19 parts, sub-document lens = [2014, 2035, 2044, 2046, 2045, 2047, 2028, 2041, 2027, 2042, 2027, 2016, 2043, 2039, 2042, 2031, 1932, 2025, 1528], overlap lens = [108, 127, 171, 133, 125, 131, 114, 146, 112, 128, 150, 105, 135, 125, 122, 114, 80, 110]\n",
      "merged document len = 35816, coref shape = torch.Size([35816, 101]), ind shape = torch.Size([35816, 100])\n",
      "6737 word mentions, 6737 span mentions\n",
      "\n",
      "dead_poets_society\n",
      "14 parts, sub-document lens = [2029, 2041, 2041, 2022, 2025, 1978, 2043, 1992, 2024, 2015, 2045, 2047, 2047, 1487], overlap lens = [110, 125, 195, 171, 113, 64, 127, 83, 105, 119, 137, 158, 129]\n",
      "merged document len = 26200, coref shape = torch.Size([26200, 101]), ind shape = torch.Size([26200, 100])\n",
      "5435 word mentions, 5435 span mentions\n",
      "\n",
      "john_wick\n",
      "14 parts, sub-document lens = [2046, 1998, 2021, 2048, 2038, 2039, 2046, 2045, 2024, 2048, 2031, 1956, 2046, 239], overlap lens = [126, 143, 154, 132, 128, 120, 163, 157, 115, 133, 125, 37, 138]\n",
      "merged document len = 24954, coref shape = torch.Size([24954, 101]), ind shape = torch.Size([24954, 100])\n",
      "3799 word mentions, 3799 span mentions\n",
      "\n",
      "prestige\n",
      "19 parts, sub-document lens = [2036, 1997, 2044, 2032, 2033, 2047, 2045, 2041, 1980, 2040, 2047, 2048, 2042, 2041, 2048, 1940, 2048, 2031, 1687], overlap lens = [146, 151, 181, 115, 143, 128, 128, 132, 89, 122, 132, 147, 137, 127, 131, 62, 132, 114]\n",
      "merged document len = 35910, coref shape = torch.Size([35910, 101]), ind shape = torch.Size([35910, 100])\n",
      "7326 word mentions, 7326 span mentions\n",
      "\n",
      "quiet_place\n",
      "16 parts, sub-document lens = [2043, 2048, 2032, 1860, 2048, 2019, 1988, 1969, 2020, 1844, 2020, 1903, 2011, 1935, 1872, 250], overlap lens = [195, 194, 233, 0, 139, 107, 138, 186, 116, 0, 118, 0, 120, 473, 0]\n",
      "merged document len = 27843, coref shape = torch.Size([27843, 101]), ind shape = torch.Size([27843, 100])\n",
      "4393 word mentions, 4393 span mentions\n",
      "\n",
      "zootopia\n",
      "15 parts, sub-document lens = [2045, 2022, 2046, 2044, 2039, 2019, 2037, 2043, 2044, 2037, 2033, 2040, 2038, 2046, 495], overlap lens = [150, 104, 126, 131, 125, 110, 124, 144, 134, 125, 153, 171, 143, 161]\n",
      "merged document len = 27127, coref shape = torch.Size([27127, 101]), ind shape = torch.Size([27127, 100])\n",
      "4714 word mentions, 4714 span mentions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_arr = [\"none\", \"addsays\", \"nocharacters\"]\n",
    "genre_arr = [\"bc\", \"bn\", \"mz\", \"nw\", \"pt\", \"tc\", \"wb\"]\n",
    "split_len_arr = [2048, 3072, 4096, 5120]\n",
    "overlap_len_arr = [128, 256, 512]\n",
    "n_settings = len(preprocess_arr) * len(genre_arr) * len(split_len_arr) * len(overlap_len_arr)\n",
    "strategy = \"average\"\n",
    "\n",
    "for preprocess, genre, split_len, overlap_len in itertools.product(preprocess_arr, genre_arr, split_len_arr, overlap_len_arr):\n",
    "    setting = (preprocess, genre, split_len, overlap_len)\n",
    "    print(f\"preprocess={preprocess} genre={genre} split_len={split_len} overlap_len={overlap_len}\\n\")\n",
    "    \n",
    "    # Read docs\n",
    "    with jsonlines.open(os.path.join(data_dir, f\"movie_coref/results/coreference/baselines/preprocess_{preprocess}.genre_{genre}.split_{split_len}.overlap_{overlap_len}.train_wl.jsonlines\")) as reader:\n",
    "        docs = {doc[\"document_id\"]:doc for doc in reader} # type: ignore\n",
    "    pt = cache.get(setting, torch.load(os.path.join(data_dir, f\"movie_coref/results/coreference/baselines/preprocess_{preprocess}.genre_{genre}.split_{split_len}.overlap_{overlap_len}.train_wl.pt\")))\n",
    "    cache[setting] = pt\n",
    "    \n",
    "    # Movie to number of parts\n",
    "    movie_to_n_parts = defaultdict(int)\n",
    "    for doc_id in docs.keys():\n",
    "        match = re.match(r\"[a-z]{2}_(\\w+)_(\\d+)\", doc_id)\n",
    "        assert match is not None, \"Improperly formatted document id\"\n",
    "        movie = match.group(1)\n",
    "        part = int(match.group(2))\n",
    "        movie_to_n_parts[movie] = max(part, movie_to_n_parts[movie])\n",
    "    \n",
    "    # Loop over movie and parts\n",
    "    for movie, n_parts in movie_to_n_parts.items():\n",
    "        print(movie)\n",
    "        corefs, inds, offsets, head2span = [], [], [], {}\n",
    "        for i in range(1, n_parts + 1):\n",
    "            offset = docs[f\"{genre}_{movie}_{i}\"][\"offset\"]\n",
    "            coref, ind, _head2span = get_scores_indices_heads(pt[f\"{genre}_{movie}_{i}\"], offset)\n",
    "            corefs.append(coref)\n",
    "            inds.append(ind)\n",
    "            offsets.append(offset)\n",
    "            head2span.update(_head2span)\n",
    "        overlap_lens = [offsets[i][1] - offsets[i + 1][0] for i in range(n_parts - 1)]\n",
    "        coref_lens = [len(coref) for coref in corefs]\n",
    "        print(f\"{n_parts} parts, sub-document lens = {coref_lens}, overlap lens = {overlap_lens}\")\n",
    "        coref, ind = combine_coref_scores(corefs, inds, overlap_lens, strategy)\n",
    "        print(f\"merged document len = {len(coref)}, coref shape = {coref.shape}, ind shape = {ind.shape}\")\n",
    "\n",
    "        word_clusters = clusterize(coref, ind)\n",
    "        span_clusters = []\n",
    "        for cluster in word_clusters:\n",
    "            span_cluster = []\n",
    "            for head in cluster:\n",
    "                if head in head2span:\n",
    "                    span_cluster.append(head2span[head])\n",
    "            if span_cluster:\n",
    "                span_clusters.append(span_cluster)\n",
    "        n_word_mentions = sum([len(cluster) for cluster in word_clusters])\n",
    "        n_span_mentions = sum([len(cluster) for cluster in span_clusters])\n",
    "        print(f\"{n_word_mentions} word mentions, {n_span_mentions} span mentions\")\n",
    "\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fca4863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bc_avengers_endgame_19\"\n",
    "match = re.match(r\"\\w+_(\\d+)\", text)\n",
    "match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb52df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6d6049dce941f60bb2eec2e35eb23f5ec878ebd724b7ecb8ba7d6ee7900594f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
