{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mica_text_coref.coref.movie_coref.data import (CharacterRecognitionDataset, \n",
    "                                                    CorefCorpus)\n",
    "\n",
    "import collections\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                     \"movie_coref/results/regular/movie.jsonlines\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)\n",
    "dataset = CharacterRecognitionDataset(\n",
    "    corpus, roberta_tokenizer, seq_length=256, obey_scene_boundaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: torch.int64 torch.Size([792, 256])\n",
      "label distribution = Counter({0: 176964, 1: 25788})\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.label_ids\n",
    "label_distribution = collections.Counter(labels.flatten().tolist())\n",
    "print(f\"labels: {labels.dtype} {labels.shape}\")\n",
    "print(f\"label distribution = {label_distribution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_labels: torch.int64 torch.Size([792, 256]) distribution = Counter({0: 176964, 1: 25788})\n",
      "saved_logits: torch.float32 torch.Size([792, 256, 2]) >0 = 0\n"
     ]
    }
   ],
   "source": [
    "saved_labels = torch.load(\n",
    "    \"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/\"\n",
    "    \"character_recognition/epoch_1/dev/labels.pt\")\n",
    "saved_logits = torch.load(\n",
    "    \"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/\"\n",
    "    \"character_recognition/epoch_1/dev/logits.pt\")\n",
    "\n",
    "saved_labels_distribution = collections.Counter(saved_labels.flatten().tolist())\n",
    "print(f\"saved_labels: {saved_labels.dtype} {saved_labels.shape} \"\n",
    "      f\"distribution = {saved_labels_distribution}\")\n",
    "\n",
    "n_pos_saved_logits = (saved_logits[:,:,1] > saved_logits[:,:,0]).sum().item()\n",
    "print(f\"saved_logits: {saved_logits.dtype} {saved_logits.shape} \"\n",
    "      f\">0 = {n_pos_saved_logits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_token_attn_mask = torch.load(\n",
    "    \"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/\"\n",
    "    \"character_recognition/epoch_1/dev/token_attention_mask.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.bool, torch.Size([792, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_token_attn_mask.dtype, saved_token_attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_token_attn_mask.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRecognition(nn.Module):\n",
    "    \"\"\"Character Recognition Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 encoder_name: str,\n",
    "                 num_parse_tags: int,\n",
    "                 parse_tag_embedding_size: int,\n",
    "                 gru_hidden_size: int,\n",
    "                 gru_num_layers: int,\n",
    "                 gru_dropout: float,\n",
    "                 gru_bidirectional: bool,\n",
    "                 num_labels: int) -> None:\n",
    "        \"\"\"Initializer for Character Recognition Model.\n",
    "\n",
    "        Args:\n",
    "            encoder_name: Language model encoder name from transformers hub\n",
    "                e.g. bert-base-cased\n",
    "            num_parse_tags: Parse tag set size\n",
    "            parse_tag_embedding_size: Embedding size of the parse tags\n",
    "            gru_hidden_size: Hidden size of the GRU\n",
    "            gru_num_layers: Number of layers of the GRU\n",
    "            gru_dropout: Dropout used between the GPU layers\n",
    "            gru_bidirectional: If true, the GRU is bidirectional\n",
    "            num_labels: Number of labels in the label set. 2 if label_type =\n",
    "                \"head\" or 3 if label_type = \"span\"\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            encoder_name, add_pooling_layer=False)\n",
    "        self.encoder_hidden_size = self.encoder.config.hidden_size\n",
    "        self.subtoken = nn.Linear(self.encoder_hidden_size, 1)\n",
    "        self.parse_embedding = nn.Embedding(\n",
    "            num_parse_tags, parse_tag_embedding_size)\n",
    "        self.gru_input_size = (self.encoder_hidden_size +\n",
    "                               parse_tag_embedding_size)\n",
    "        self.gru_output_size = gru_hidden_size * (1 + int(gru_bidirectional))\n",
    "        self.gru = nn.GRU(self.gru_input_size, gru_hidden_size,\n",
    "                          num_layers=gru_num_layers, batch_first=True,\n",
    "                          dropout=gru_dropout, bidirectional=gru_bidirectional)\n",
    "        self.output = nn.Linear(self.gru_output_size, num_labels)\n",
    "        self._device = \"cpu\"\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"Getter for model device.\"\"\"\n",
    "        return self._device\n",
    "    \n",
    "    @device.setter\n",
    "    def device(self, device):\n",
    "        \"\"\"Setter for model device. Used by accelerate.\"\"\"\n",
    "        self._device = device\n",
    "    \n",
    "    def forward(self, subtoken_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
    "                token_offset: torch.Tensor, parse_ids: torch.Tensor,\n",
    "                labels: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward propagation for the Character Recognition Model.\n",
    "\n",
    "        Args:\n",
    "            subtoken_ids: `batch_size x max_n_subtokens` Long Tensor\n",
    "            attention_mask: `batch_size x max_n_subtokens` Float/Long Tensor\n",
    "            token_offset: `batch_size x max_n_tokens x 2` Long Tensor\n",
    "            parse_ids: `batch_size x max_n_tokens` Long Tensor\n",
    "            labels: `batch_size x max_n_tokens` Long Tensor\n",
    "        \n",
    "        Returns:\n",
    "            Return the loss value if model is begin trained, else the logits \n",
    "            `batch_size x max_n_tokens x num_labels` Float Tensor\n",
    "        \"\"\"\n",
    "        batch_size = len(subtoken_ids)\n",
    "\n",
    "        # subtoken_embedding = batch_size x max_n_subtokens x encoder_hidden_size\n",
    "        encoder_output = self.encoder(subtoken_ids, attention_mask)\n",
    "        subtoken_embedding = encoder_output.last_hidden_state\n",
    "\n",
    "        # _subtoken_embedding = batch_size * max_n_subtokens x encoder_hidden_size\n",
    "        # subtoken_attn = batch_size * max_n_tokens x batch_size * max_n_subtokens\n",
    "        _subtoken_embedding = subtoken_embedding.view(-1, self.encoder_hidden_size)\n",
    "        subtoken_attn = self._attn_scores(_subtoken_embedding,\n",
    "                                          token_offset.view(-1, 2))\n",
    "        \n",
    "        # token_embedding = batch_size x max_n_tokens x encoder_hidden_size\n",
    "        token_embedding = torch.mm(\n",
    "            subtoken_attn, _subtoken_embedding).reshape(\n",
    "                batch_size, -1, self.encoder_hidden_size)\n",
    "        \n",
    "        # gru_input = batch_size x max_n_tokens x (encoder_hidden_size +\n",
    "        # parse_tag_embedding_size)\n",
    "        parse_input = self.parse_embedding(parse_ids)\n",
    "        gru_input = torch.cat((token_embedding, parse_input), dim=2).contiguous()\n",
    "\n",
    "        # logits = batch_size x max_n_tokens x num_labels\n",
    "        gru_output, _ = self.gru(gru_input)\n",
    "        logits = self.output(gru_output)\n",
    "\n",
    "        # token attention mask = batch_size x max_n_tokens\n",
    "        # TODO sabyasachee put this inside self.training and don't return\n",
    "        token_attention_mask = torch.any(subtoken_attn > 0, dim=1).reshape(\n",
    "                batch_size, -1)\n",
    "\n",
    "        if self.training:\n",
    "            loss = compute_loss(logits, labels, token_attention_mask,\n",
    "                                self.num_labels)\n",
    "            return loss\n",
    "        else:\n",
    "            return logits, token_attention_mask\n",
    "\n",
    "    def _attn_scores(self,\n",
    "                     subtoken_embeddings: torch.FloatTensor,\n",
    "                     token_offset: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\" Calculates attention scores for each of the subtokens of a token.\n",
    "\n",
    "        Args:\n",
    "            subtoken_embedding: `n_subtokens x embedding_size` Float Tensor,\n",
    "                embeddings for each subtoken\n",
    "            token_offset: `n_tokens x 2` Long Tensor, subtoken offset of each\n",
    "                token\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: `n_tokens x n_subtokens` Float Tensor, attention\n",
    "            weights for each subtoken of a token\n",
    "        \"\"\"\n",
    "        n_subtokens, n_tokens = len(subtoken_embeddings), len(token_offset)\n",
    "        token_begin, token_end = token_offset[:,0], token_offset[:,1]\n",
    "        \n",
    "        # attn_mask: n_tokens x n_subtokens, contains -∞ for subtokens outside\n",
    "        # the token's offsets and 0 for subtokens inside the token's offsets\n",
    "        attn_mask = torch.arange(0, n_subtokens, device=self.device).expand(\n",
    "            (n_tokens, n_subtokens))\n",
    "        attn_mask = ((attn_mask >= token_begin.unsqueeze(1)) * \n",
    "                     (attn_mask <= token_end.unsqueeze(1)))\n",
    "        attn_mask = torch.log(attn_mask.to(torch.float))\n",
    "\n",
    "        # attn_scores: 1 x n_subtokens\n",
    "        attn_scores = self.subtoken(subtoken_embeddings).T\n",
    "\n",
    "        # attn_scores: n_tokens x n_subtokens\n",
    "        attn_scores = attn_scores.expand((n_tokens, n_subtokens))\n",
    "\n",
    "        # -∞ for subtokens outside the token's offsets and attn_scores for\n",
    "        # inside the token's offsets\n",
    "        attn_scores = attn_mask + attn_scores\n",
    "        del attn_mask\n",
    "\n",
    "        # subtoken_attn contains 0 for subtokens outside the token's offsets\n",
    "        subtoken_attn = torch.softmax(attn_scores, dim=1)\n",
    "        return subtoken_attn\n",
    "    \n",
    "def compute_loss(\n",
    "    logits: torch.FloatTensor, label_ids: torch.LongTensor,\n",
    "    attn_mask: torch.FloatTensor, n_labels: int) -> torch.FloatTensor:\n",
    "    \"\"\"Compute cross entropy loss\"\"\"\n",
    "    active_labels = label_ids[attn_mask == 1.]\n",
    "    active_logits = logits.flatten(0, 1)[attn_mask.flatten() == 1.]\n",
    "    label_distribution = torch.bincount(active_labels, minlength=n_labels)\n",
    "    class_weight = torch.sqrt(len(active_labels)/(1 + label_distribution))\n",
    "    cross_entrop_loss_fn = nn.CrossEntropyLoss(\n",
    "        weight=class_weight, reduction=\"mean\")\n",
    "    loss = cross_entrop_loss_fn(active_logits, active_labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64)\n",
    "for batch in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/sbaruah_usc_edu/anaconda3/envs/coreference/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = CharacterRecognition(\"roberta-base\", len(dataset.parse_tag_to_id), 32, 768, 1, 0.2, True, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_logits, batch_token_attention_mask = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.Size([64, 256, 2]), torch.bool, torch.Size([64, 256]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_logits.dtype, batch_logits.shape, batch_token_attention_mask.dtype, batch_token_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8883)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(batch_logits[:,:,1] > batch_logits[:,:,0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
