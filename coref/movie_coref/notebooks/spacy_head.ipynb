{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import tqdm\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenplay_parse_file = (\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                         \"movie_coref/parse.csv\")\n",
    "movie_and_raters_file = (\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                         \"movie_coref/movies.txt\")\n",
    "screenplays_dir = (\"/home/sbaruah_usc_edu/mica_text_coref/data/\"\n",
    "                   \"movie_coref/screenplay\")\n",
    "annotations_dir = \"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/csv\"\n",
    "spacy_model = \"en_core_web_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy model and movie data jsonlines\n",
    "spacy.require_gpu()\n",
    "nlp = spacy.load(spacy_model)\n",
    "movie_data = []\n",
    "\n",
    "# Read screenplay parse and movie names\n",
    "movies, raters = [], []\n",
    "parse_df = pd.read_csv(screenplay_parse_file, index_col=None)\n",
    "with open(movie_and_raters_file) as fr:\n",
    "    for line in fr:\n",
    "        movie, rater = line.split()\n",
    "        movies.append(movie)\n",
    "        raters.append(rater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:23<00:00,  2.59s/movie]\n"
     ]
    }
   ],
   "source": [
    "movie_data = []\n",
    "\n",
    "for movie, rater in tqdm.tqdm(zip(movies, raters), total=len(movies),\n",
    "                              unit=\"movie\"):\n",
    "    # Read movie script, movie parse, and movie coreference annotations\n",
    "    script_filepath = os.path.join(screenplays_dir, f\"{movie}.txt\")\n",
    "    annotation_filepath = os.path.join(annotations_dir, f\"{movie}.csv\")\n",
    "    with open(script_filepath, encoding=\"utf-8\") as fr:\n",
    "        script = fr.read()\n",
    "    lines = script.splitlines()\n",
    "    parsetags = parse_df[parse_df[\"movie\"] == movie][\"robust\"].tolist()\n",
    "    annotation_df = pd.read_csv(annotation_filepath, index_col=None)\n",
    "    items = []\n",
    "    for _, row in annotation_df.iterrows():\n",
    "        begin, end, character = row[\"begin\"], row[\"end\"], row[\"entityLabel\"]\n",
    "        items.append((begin, end, character))\n",
    "    items = sorted(items)\n",
    "\n",
    "    # Find non-whitespace offset of coreference annotations\n",
    "    begins, ends, characters, wsbegins, wsends = [], [], [], [], []\n",
    "    prev_begin, prev_wsbegin = 0, 0\n",
    "    for i, (begin, end, character) in enumerate(items):\n",
    "        if i == 0:\n",
    "            wsbegin = len(re.sub(\"\\s\", \"\", script[:begin]))\n",
    "        else:\n",
    "            wsbegin = prev_wsbegin + len(re.sub(\"\\s\", \"\", script[prev_begin: begin]))\n",
    "        prev_begin, prev_wsbegin = begin, wsbegin\n",
    "        wsend = wsbegin + len(re.sub(\"\\s\", \"\", script[begin: end]))\n",
    "        begins.append(begin)\n",
    "        ends.append(end)\n",
    "        characters.append(character)\n",
    "        wsbegins.append(wsbegin)\n",
    "        wsends.append(wsend)\n",
    "\n",
    "    # Find segments (blocks of adjacent lines with same movieparser tags)\n",
    "    i = 0\n",
    "    segment_texts, segment_tags = [], []\n",
    "    while i < len(lines):\n",
    "        j = i + 1\n",
    "        while j < len(lines) and parsetags[j] == parsetags[i]:\n",
    "            j += 1\n",
    "        segment = re.sub(\"\\s+\", \" \", \" \".join(lines[i: j]).strip())\n",
    "        segment = (\" \".join(nltk.wordpunct_tokenize(segment))).strip()\n",
    "        segment = re.sub(\"\\s+\", \" \", segment.strip())\n",
    "        if segment:\n",
    "            segment_texts.append(segment)\n",
    "            segment_tags.append(parsetags[i])\n",
    "        i = j\n",
    "\n",
    "    # Run each segment through spacy pipeline\n",
    "    docs = nlp.pipe(segment_texts, batch_size=10200)\n",
    "\n",
    "    # Tokenize each spacy token using nltk.wordpunct_tokenizer\n",
    "    # Find tokens, token sentence ids, and token movieparser tags\n",
    "    (tokens, token_heads, token_postags, token_nertags, token_begins,\n",
    "        token_ends, token_tags, token_sentids) = [], [], [], [], [], [], [], []\n",
    "    c, s, n = 0, 0, 0\n",
    "    for i, doc in enumerate(docs):\n",
    "        for sent in doc.sents:\n",
    "            for stoken in sent:\n",
    "                text = stoken.text\n",
    "                ascii_text = unidecode.unidecode(text, errors=\"strict\")\n",
    "                assert ascii_text != \"\", f\"token={text}, ascii_text={ascii_text}\"\n",
    "                postag = stoken.tag_\n",
    "                nertag = stoken.ent_type_\n",
    "                if not nertag:\n",
    "                    nertag = \"-\"\n",
    "                token_begin = c\n",
    "                c += len(re.sub(\"\\s+\", \"\", text))\n",
    "                token_end = c\n",
    "                token_sentid = s\n",
    "                tokens.append(ascii_text)\n",
    "                token_heads.append(n + stoken.head.i)\n",
    "                token_begins.append(token_begin)\n",
    "                token_ends.append(token_end)\n",
    "                token_sentids.append(token_sentid)\n",
    "                token_tags.append(segment_tags[i])\n",
    "                token_postags.append(postag)\n",
    "                token_nertags.append(nertag)\n",
    "            n += len(sent)\n",
    "            s += 1\n",
    "\n",
    "    # Match mentions to tokens\n",
    "    mention_begins, mention_ends = [], []\n",
    "    for wsbegin, wsend, begin, end in zip(wsbegins, wsends, begins, ends):\n",
    "        try:\n",
    "            i = token_begins.index(wsbegin)\n",
    "        except Exception:\n",
    "            i = None\n",
    "        try:\n",
    "            j = token_ends.index(wsend)\n",
    "        except Exception:\n",
    "            mention = script[begin: end].rstrip()\n",
    "            right_context = script[end:].lstrip()\n",
    "            if mention.endswith(\".\") and right_context.startswith(\"..\"):\n",
    "                wsend -= 1\n",
    "                try:\n",
    "                    j = token_ends.index(wsend)\n",
    "                except Exception:\n",
    "                    j = None\n",
    "            else:\n",
    "                j = None\n",
    "        if i is None or j is None:\n",
    "            mention = script[begin: end]\n",
    "            context = script[begin-10: end+10]\n",
    "            print(f\"mention = '{mention}'\")\n",
    "            print(f\"context = '{context}'\")\n",
    "            if i is None:\n",
    "                print(\"Could not match start of mention\")\n",
    "            if j is None:\n",
    "                print(\"Could not match end of mention\")\n",
    "            print()\n",
    "        mention_begins.append(i)\n",
    "        mention_ends.append(j)\n",
    "\n",
    "    # Create speakers array\n",
    "    speakers = np.full(len(tokens), fill_value=\"-\", dtype=object)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if token_tags[i] == \"C\":\n",
    "            j = i + 1\n",
    "            while j < len(tokens) and token_tags[j] == token_tags[i]:\n",
    "                j += 1\n",
    "            k = j\n",
    "            utterance_token_indices = []\n",
    "            while k < len(tokens) and token_tags[k] not in \"SC\":\n",
    "                if token_tags[k] in \"DE\":\n",
    "                    utterance_token_indices.append(k)\n",
    "                k += 1\n",
    "            if utterance_token_indices:\n",
    "                speaker = \" \".join(tokens[i: j])\n",
    "                cleaned_speaker = re.sub(\"\\([^\\)]+\\)\", \"\", speaker).strip()\n",
    "                speaker = cleaned_speaker if cleaned_speaker else speaker\n",
    "                for l in utterance_token_indices:\n",
    "                    speakers[l] = speaker\n",
    "            i = k\n",
    "        else:\n",
    "            i += 1\n",
    "    speakers = speakers.tolist()\n",
    "\n",
    "    # Create character to mention and head\n",
    "    clusters: dict[str, list[list[int]]] = {}\n",
    "    for character, mention_begin, mention_end in zip(characters, mention_begins,\n",
    "                                                     mention_ends):\n",
    "        if character not in clusters:\n",
    "            clusters[character] = []\n",
    "        token_indexes_with_outside_head = []\n",
    "        for i in range(mention_begin, mention_end + 1):\n",
    "            head_index = token_heads[i]\n",
    "            if (head_index == i or head_index < mention_begin or\n",
    "                head_index > mention_end):\n",
    "                token_indexes_with_outside_head.append(i)\n",
    "        mention_head = mention_end\n",
    "        if len(token_indexes_with_outside_head) == 1:\n",
    "            mention_head = token_indexes_with_outside_head[0]\n",
    "        clusters[character].append([mention_begin, mention_end, mention_head])\n",
    "    \n",
    "    # Find sentence offsets\n",
    "    sentence_offsets: list[list[int]] = []\n",
    "    i = 0\n",
    "    while i < len(token_sentids):\n",
    "        j = i + 1\n",
    "        while j < len(token_sentids) and token_sentids[i] == token_sentids[j]:\n",
    "            j += 1\n",
    "        sentence_offsets.append([i, j])\n",
    "        i = j\n",
    "\n",
    "    # Create movie json\n",
    "    movie_data.append({\n",
    "        \"movie\": movie,\n",
    "        \"rater\": rater,\n",
    "        \"token\": tokens,\n",
    "        \"pos\": token_postags,\n",
    "        \"ne\": token_nertags,\n",
    "        \"parse\": token_tags,\n",
    "        \"speaker\": speakers,\n",
    "        \"sent_offset\": sentence_offsets,\n",
    "        \"clusters\": clusters\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters(movie_data: list[dict[str, any]]) -> list[dict[str, any]]:\n",
    "    '''Removes character names preceding an utterance\n",
    "    '''\n",
    "    # Initialize new movie data\n",
    "    new_movie_data = []\n",
    "\n",
    "    # Loop over movies\n",
    "    tbar = tqdm.tqdm(movie_data, total=len(movie_data), unit=\"movie\")\n",
    "    for mdata in tbar:\n",
    "        (movie, rater, tokens, postags, nertags, parsetags, sentence_offsets,\n",
    "         speakers, clusters) = (\n",
    "            mdata[\"movie\"], mdata[\"rater\"], mdata[\"token\"], mdata[\"pos\"],\n",
    "            mdata[\"ne\"], mdata[\"parse\"], mdata[\"sent_offset\"], \n",
    "            mdata[\"speaker\"], mdata[\"clusters\"])\n",
    "        tbar.set_description(movie)\n",
    "\n",
    "        # removed[x] is the number of tokens to remove from tokens[:x]\n",
    "        # if tags[i: j] == \"C\" and is followed by some utterance, \n",
    "        # then we should remove tokens[i: j]\n",
    "        # removed[: i] remains unchanged\n",
    "        # removed[i: j] = -1\n",
    "        # removed[j:] += j - i\n",
    "        removed = np.zeros(len(tokens), dtype=int)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if parsetags[i] == \"C\":\n",
    "                j = i + 1\n",
    "                while j < len(tokens) and parsetags[j] == parsetags[i]:\n",
    "                    j += 1\n",
    "                k = j\n",
    "                utterance_token_indices = []\n",
    "                while k < len(tokens) and parsetags[k] not in \"SC\":\n",
    "                    if parsetags[k] in \"DE\":\n",
    "                        utterance_token_indices.append(k)\n",
    "                    k += 1\n",
    "                if utterance_token_indices:\n",
    "                    removed[i: j] = -1\n",
    "                    removed[j:] += j - i\n",
    "                i = k\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Find the new tokens, pos tags, ner tags, parse tags, and speakers\n",
    "        newtokens, newpostags, newnertags, newparsetags, newspeakers = (\n",
    "            [], [], [], [], [])\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if removed[i] != -1:\n",
    "                newtokens.append(tokens[i])\n",
    "                newpostags.append(postags[i])\n",
    "                newnertags.append(nertags[i])\n",
    "                newparsetags.append(parsetags[i])\n",
    "                newspeakers.append(speakers[i])\n",
    "            i += 1\n",
    "\n",
    "        # Find new sentence offsets\n",
    "        new_sentence_offsets = []\n",
    "        for i, j in sentence_offsets:\n",
    "            assert all(removed[i: j] == -1) or all(removed[i: j] != -1), (\n",
    "                \"All tokens or none of the tokens of a sentence should be \"\n",
    "                \"removed\")\n",
    "            if all(removed[i: j] != -1):\n",
    "                i = i - removed[i]\n",
    "                j = j - removed[i]\n",
    "                new_sentence_offsets.append([i, j])\n",
    "\n",
    "        # Find new clusters\n",
    "        new_clusters: dict[str, list[list[int]]] = {}\n",
    "        for character, mentions in clusters.items():\n",
    "            new_mentions = []\n",
    "            for begin, end, head in mentions:\n",
    "                assert (all(removed[begin: end + 1] == -1) or\n",
    "                        all(removed[begin: end + 1] != -1)), (\n",
    "                            \"All tokens or none of the tokens of a mention\"\n",
    "                            f\" should be removed, mention = [{begin},{end},{head}]\")\n",
    "                if all(removed[begin: end + 1] != -1):\n",
    "                    begin = begin - removed[begin]\n",
    "                    end = end - removed[end]\n",
    "                    head = head - removed[head]\n",
    "                    new_mentions.append([begin, end, head])\n",
    "            if new_mentions:\n",
    "                new_clusters[character] = new_mentions\n",
    "\n",
    "        # Create movie json\n",
    "        new_movie_data.append({\n",
    "            \"movie\": movie,\n",
    "            \"rater\": rater,\n",
    "            \"token\": newtokens,\n",
    "            \"pos\": newpostags,\n",
    "            \"ne\": newnertags,\n",
    "            \"parse\": newparsetags,\n",
    "            \"speaker\": newspeakers,\n",
    "            \"sent_offset\": new_sentence_offsets,\n",
    "            \"clusters\": new_clusters\n",
    "        })\n",
    "\n",
    "    return new_movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "basterds: 100%|██████████| 9/9 [00:00<00:00, 15.42movie/s]          \n"
     ]
    }
   ],
   "source": [
    "movie_nocharacters_data = remove_characters(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_says(movie_data: list[dict[str, any]]) -> list[dict[str, any]]:\n",
    "    '''\n",
    "    Inserts 'says' between character name and utterance block. Give the token\n",
    "    'says' a unique tag `A`.\n",
    "    '''\n",
    "    # Initialize new movie data\n",
    "    new_movie_data = []\n",
    "\n",
    "    # Loop over each movie\n",
    "    tbar = tqdm.tqdm(movie_data, total=len(movie_data), unit=\"movie\")\n",
    "    for mdata in tbar:\n",
    "        (movie, rater, tokens, postags, nertags, parsetags, sentence_offsets,\n",
    "         speakers, clusters) = (\n",
    "            mdata[\"movie\"], mdata[\"rater\"], mdata[\"token\"], mdata[\"pos\"],\n",
    "            mdata[\"ne\"], mdata[\"parse\"], mdata[\"sent_offset\"], \n",
    "            mdata[\"speaker\"], mdata[\"clusters\"])\n",
    "        tbar.set_description(movie)\n",
    "\n",
    "        # added[x] is the number of 'says' added in tokens[:x]\n",
    "        added = np.zeros(len(tokens), dtype=int)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if parsetags[i] == \"C\":\n",
    "                j = i + 1\n",
    "                while j < len(tokens) and parsetags[j] == parsetags[i]:\n",
    "                    j += 1\n",
    "                k = j\n",
    "                utterance_token_indices = []\n",
    "                while k < len(tokens) and parsetags[k] not in \"SC\":\n",
    "                    if parsetags[k] in \"DE\":\n",
    "                        utterance_token_indices.append(k)\n",
    "                    k += 1\n",
    "                if utterance_token_indices:\n",
    "                    added[j:] += 1\n",
    "                i = k\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Find new tokens, pos tags, ner tags, parse tags, and speakers\n",
    "        newtokens, newpostags, newnertags, newparsetags, newspeakers = (\n",
    "            [], [], [], [], [])\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            newtokens.append(tokens[i])\n",
    "            newpostags.append(postags[i])\n",
    "            newnertags.append(nertags[i])\n",
    "            newparsetags.append(parsetags[i])\n",
    "            newspeakers.append(speakers[i])\n",
    "            if i < len(tokens) - 1 and added[i] < added[i + 1]:\n",
    "                newtokens.append(\"says\")\n",
    "                newpostags.append(\"VBZ\")\n",
    "                newnertags.append(\"-\")\n",
    "                newparsetags.append(\"A\")\n",
    "                newspeakers.append(\"-\")\n",
    "            i += 1\n",
    "\n",
    "        # Find new sentence offsets\n",
    "        new_sentence_offsets = []\n",
    "        while k < len(sentence_offsets):\n",
    "            i, j = sentence_offsets[k]\n",
    "            if k < len(sentence_offsets) - 1 and added[j - 1] < added[j] and (\n",
    "                parsetags[j] in \"DE\"):\n",
    "                k = k + 1\n",
    "                j = sentence_offsets[k][1]\n",
    "            i = i + added[i]\n",
    "            j = j + added[j]\n",
    "            k = k + 1\n",
    "            new_sentence_offsets.append([i, j])\n",
    "\n",
    "        # Find new clusters\n",
    "        new_clusters: dict[str, list[list[int]]] = {}\n",
    "        for character, mentions in clusters.items():\n",
    "            new_mentions = []\n",
    "            for begin, end, head in mentions:\n",
    "                begin = begin + added[begin]\n",
    "                end = end + added[end]\n",
    "                head = head + added[head]\n",
    "                new_mentions.append([begin, end, head])\n",
    "            new_clusters[character] = new_mentions\n",
    "\n",
    "        # Create the new movie json\n",
    "        new_movie_data.append({\n",
    "            \"movie\": movie,\n",
    "            \"rater\": rater,\n",
    "            \"token\": newtokens,\n",
    "            \"pos\": newpostags,\n",
    "            \"ne\": newnertags,\n",
    "            \"parse\": newparsetags,\n",
    "            \"speaker\": newspeakers,\n",
    "            \"sent_offset\": new_sentence_offsets,\n",
    "            \"clusters\": new_clusters\n",
    "        })\n",
    "\n",
    "    return new_movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "basterds: 100%|██████████| 9/9 [00:00<00:00, 33.13movie/s]   \n"
     ]
    }
   ],
   "source": [
    "# Insert 'says' after tokens with movieparse tag = \"C\"\n",
    "movie_addsays_data = add_says(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(movie_data, open(\n",
    "    \"/home/sbaruah_usc_edu/mica_text_coref/data/temp/movie_data.json\", \"w\"),\n",
    "    indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_offsets_to_list(offsets: list[list[int]]) -> list[int]:\n",
    "    ids = []\n",
    "    for i, j in offsets:\n",
    "        ids.append([k for k in range(j - i)])\n",
    "    return ids\n",
    "\n",
    "def prepare_for_wlcoref(movie_data: list[dict[str, any]]) -> (\n",
    "    list[dict[str, any]]):\n",
    "    \"\"\"Convert to jsonlines format which can be used as input to the word-level\n",
    "    coreference model.\n",
    "    \"\"\"\n",
    "    new_movie_data = []\n",
    "    for mdata in movie_data:\n",
    "        new_movie_data.append({\n",
    "            \"document_id\": f\"wb/{mdata['movie']}\",\n",
    "            \"cased_words\": mdata[\"token\"],\n",
    "            \"sent_id\": convert_offsets_to_list(mdata[\"sent_offset\"]),\n",
    "            \"speaker\": mdata[\"speaker\"]\n",
    "        })\n",
    "    return new_movie_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_movie_data = prepare_for_wlcoref(movie_data)\n",
    "wl_movie_addsays_data = prepare_for_wlcoref(movie_addsays_data)\n",
    "wl_movie_nocharacters_data = prepare_for_wlcoref(movie_nocharacters_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mention:\n",
    "\n",
    "    def __init__(self, begin: int, end: int, head: int | None) -> None:\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "        self.head = head\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash((self.begin, self.end))\n",
    "\n",
    "    def __lt__(self, other: \"Mention\") -> bool:\n",
    "        return (self.begin, self.end) < (other.begin, other.end)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"({self.begin},{self.end})\"\n",
    "\n",
    "class CorefDocument:\n",
    "\n",
    "    def __init__(self, json: dict[str, any]) -> None:\n",
    "        self.movie: str = json[\"movie\"]\n",
    "        self.rater: str = json[\"rater\"]\n",
    "        self.token: list[str] = json[\"token\"]\n",
    "        self.parse: list[str] = json[\"parse\"]\n",
    "        self.pos: list[str] = json[\"pos\"]\n",
    "        self.ner: list[str] = json[\"ner\"]\n",
    "        self.speaker: list[str] = json[\"speaker\"]\n",
    "        self.sentence_offsets: list[tuple[int, int]] = json[\"sent_offset\"]\n",
    "        self.clusters: dict[str, set[Mention]] = {}\n",
    "        for character, mentions in json[\"clusters\"].items():\n",
    "            mentions = set([Mention(*x) for x in mentions])\n",
    "            self.clusters[character] = mentions\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        desc = \"Script\\n=====\\n\\n\"\n",
    "        for i, j in self.sentence_offsets:\n",
    "            sentence = self.token[i: j]\n",
    "            desc += f\"{sentence}\\n\"\n",
    "        desc += \"\\n\\nClusters\\n========\\n\\n\"\n",
    "        for character, mentions in self.clusters.items():\n",
    "            desc += f\"{character}\\n\"\n",
    "            sorted_mentions = sorted(mentions)\n",
    "            mention_texts = []\n",
    "            for mention in sorted_mentions:\n",
    "                mention_text = \" \".join(\n",
    "                    self.token[mention.begin: mention.end + 1])\n",
    "                mention_head = self.token[mention.head]\n",
    "                mention_texts.append(f\"{mention_text} ({mention_head})\")\n",
    "            n_rows = math.ceil(len(mention_texts)/3)\n",
    "            for i in range(n_rows):\n",
    "                row_mention_texts = mention_texts[i * 3: (i + 1) * 3]\n",
    "                row_desc = \"     \".join(\n",
    "                    [f\"{mention_text:25s}\" for mention_text in row_mention_texts])\n",
    "                desc += row_desc + \"\\n\"\n",
    "            desc += \"\\n\"\n",
    "        return desc\n",
    "\n",
    "class CorefCorpus:\n",
    "\n",
    "    def __init__(self, file: str | None = None) -> None:\n",
    "        self.documents: list[CorefDocument] = []\n",
    "        if file is not None:\n",
    "            with jsonlines.open(file) as reader:\n",
    "                for json in reader:\n",
    "                    self.documents.append(CorefDocument(json))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.documents)\n",
    "\n",
    "    def __getitem__(self, i) -> CorefDocument:\n",
    "        return self.documents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avengers_endgame\n",
      "dead_poets_society\n",
      "john_wick\n",
      "prestige\n",
      "quiet_place\n",
      "zootopia\n",
      "shawshank\n",
      "bourne\n",
      "basterds\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/regular/movie.jsonlines\") as reader:\n",
    "    for obj in reader:\n",
    "        print(obj[\"movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = CorefCorpus(\"/home/sbaruah_usc_edu/mica_text_coref/data/movie_coref/results/regular/movie.jsonlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('coreference')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e9e5767629d26198a734ee01c9558510355f25ffdcffebbd890d86f684e7226"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
